{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1597366b",
   "metadata": {},
   "source": [
    "# Getting Data in Form of List of Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044f9c5",
   "metadata": {},
   "source": [
    "DONT RERUN THIS UNLESS ABSOLUTELY NESSESARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "297540ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchsummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2701fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0029\n",
      "0001\n",
      "0094\n",
      "0067\n",
      "0064\n",
      "0065\n",
      "0083\n",
      "0070\n",
      "0087\n",
      "0018\n",
      "0004\n",
      "0069\n",
      "0011\n",
      "0077\n",
      "0038\n",
      "0051\n",
      "0071\n",
      "0016\n",
      "0035\n",
      "0062\n",
      "0033\n",
      "0080\n",
      "0054\n",
      "0025\n",
      "0034\n",
      "0015\n",
      "0005\n",
      "0002\n",
      "0051\n",
      "0016\n",
      "0033\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0025\n",
      "0015\n",
      "0094\n",
      "0012\n",
      "0079\n",
      "0085\n",
      "0067\n",
      "0064\n",
      "0049\n",
      "0093\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0004\n",
      "0087\n",
      "0019\n",
      "0089\n",
      "0069\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0075\n",
      "0046\n",
      "0044\n",
      "0052\n",
      "0098\n",
      "0041\n",
      "0036\n",
      "0022\n",
      "0030\n",
      "0066\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0056\n",
      "0045\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0057\n",
      "0037\n",
      "0095\n",
      "0027\n",
      "0090\n",
      "0091\n",
      "0008\n",
      "0092\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0053\n",
      "0007\n",
      "0013\n",
      "0074\n",
      "0097\n",
      "0081\n",
      "0009\n",
      "0068\n",
      "0099\n",
      "0086\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0008\n",
      "0090\n",
      "0092\n",
      "0053\n",
      "0078\n",
      "0061\n",
      "0013\n",
      "0060\n",
      "0072\n",
      "0074\n",
      "0059\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0046\n",
      "0006\n",
      "0096\n",
      "0026\n",
      "0036\n",
      "0052\n",
      "0043\n",
      "0098\n",
      "0063\n",
      "0066\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0030\n",
      "0045\n",
      "0023\n",
      "0056\n",
      "0023\n",
      "0063\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0026\n",
      "0098\n",
      "0043\n",
      "0052\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0096\n",
      "0006\n",
      "0013\n",
      "0059\n",
      "0072\n",
      "0060\n",
      "0078\n",
      "0053\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0090\n",
      "0009\n",
      "0086\n",
      "0095\n",
      "0057\n",
      "0099\n",
      "0068\n",
      "0007\n",
      "0074\n",
      "0092\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0100\n",
      "0008\n",
      "0091\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0009\n",
      "0095\n",
      "0099\n",
      "0057\n",
      "0037\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0045\n",
      "0056\n",
      "0022\n",
      "0040\n",
      "0066\n",
      "0020\n",
      "0030\n",
      "0032\n",
      "0036\n",
      "0041\n",
      "0075\n",
      "0046\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0014\n",
      "0076\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0089\n",
      "0019\n",
      "0049\n",
      "0093\n",
      "0085\n",
      "0079\n",
      "0029\n",
      "0012\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0025\n",
      "0051\n",
      "0002\n",
      "0016\n",
      "0005\n",
      "0038\n",
      "0015\n",
      "0034\n",
      "0054\n",
      "0031\n",
      "0033\n",
      "0002\n",
      "0035\n",
      "0071\n",
      "0038\n",
      "0050\n",
      "0077\n",
      "0004\n",
      "0087\n",
      "0019\n",
      "0011\n",
      "0069\n",
      "0064\n",
      "0070\n",
      "0083\n",
      "0065\n",
      "0012\n",
      "0094\n",
      "0001\n",
      "0029\n",
      "0067\n",
      "0065\n",
      "0083\n",
      "0093\n",
      "0070\n",
      "0064\n",
      "0067\n",
      "0079\n",
      "0085\n",
      "0029\n",
      "0094\n",
      "0001\n",
      "0077\n",
      "0010\n",
      "0069\n",
      "0011\n",
      "0087\n",
      "0084\n",
      "0017\n",
      "0004\n",
      "0071\n",
      "0035\n",
      "0038\n",
      "0034\n",
      "0015\n",
      "0033\n",
      "0088\n",
      "0054\n",
      "0035\n",
      "0071\n",
      "0016\n",
      "0002\n",
      "0051\n",
      "0005\n",
      "0034\n",
      "0025\n",
      "0054\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0070\n",
      "0093\n",
      "0049\n",
      "0012\n",
      "0079\n",
      "0085\n",
      "0076\n",
      "0014\n",
      "0077\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0089\n",
      "0019\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0043\n",
      "0041\n",
      "0036\n",
      "0026\n",
      "0046\n",
      "0075\n",
      "0028\n",
      "0047\n",
      "0056\n",
      "0045\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0030\n",
      "0066\n",
      "0022\n",
      "0091\n",
      "0008\n",
      "0021\n",
      "0055\n",
      "0037\n",
      "0057\n",
      "0068\n",
      "0095\n",
      "0086\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0074\n",
      "0007\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0078\n",
      "0092\n",
      "0090\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0099\n",
      "0068\n",
      "0086\n",
      "0009\n",
      "0060\n",
      "0072\n",
      "0059\n",
      "0013\n",
      "0078\n",
      "0053\n",
      "0052\n",
      "0041\n",
      "0098\n",
      "0043\n",
      "0026\n",
      "0006\n",
      "0096\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0075\n",
      "0023\n",
      "0073\n",
      "0058\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0022\n",
      "0063\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0063\n",
      "0023\n",
      "0039\n",
      "0096\n",
      "0006\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0043\n",
      "0098\n",
      "0052\n",
      "0026\n",
      "0003\n",
      "0053\n",
      "0078\n",
      "0082\n",
      "0059\n",
      "0072\n",
      "0060\n",
      "0007\n",
      "0013\n",
      "0086\n",
      "0037\n",
      "0068\n",
      "0099\n",
      "0009\n",
      "0042\n",
      "0091\n",
      "0090\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0092\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0074\n",
      "0007\n",
      "0095\n",
      "0037\n",
      "0057\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0008\n",
      "0091\n",
      "0024\n",
      "0066\n",
      "0030\n",
      "0063\n",
      "0022\n",
      "0023\n",
      "0045\n",
      "0056\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0006\n",
      "0096\n",
      "0046\n",
      "0075\n",
      "0036\n",
      "0041\n",
      "0011\n",
      "0019\n",
      "0089\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0085\n",
      "0079\n",
      "0012\n",
      "0001\n",
      "0083\n",
      "0093\n",
      "0065\n",
      "0049\n",
      "0025\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0005\n",
      "0016\n",
      "0051\n",
      "0002\n",
      "0054\n",
      "0033\n",
      "0015\n",
      "0034\n",
      "0038\n",
      "0005\n",
      "0035\n",
      "0071\n",
      "0089\n",
      "0011\n",
      "0069\n",
      "0004\n",
      "0087\n",
      "0076\n",
      "0014\n",
      "0077\n",
      "0048\n",
      "0067\n",
      "0001\n",
      "0094\n",
      "0029\n",
      "0070\n",
      "0083\n",
      "0065\n",
      "0049\n",
      "0064\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0014\n",
      "0077\n",
      "0076\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0019\n",
      "0089\n",
      "0049\n",
      "0093\n",
      "0070\n",
      "0079\n",
      "0085\n",
      "0012\n",
      "0034\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0054\n",
      "0025\n",
      "0051\n",
      "0002\n",
      "0071\n",
      "0016\n",
      "0035\n",
      "0005\n",
      "0034\n",
      "0015\n",
      "0054\n",
      "0033\n",
      "0088\n",
      "0035\n",
      "0071\n",
      "0038\n",
      "0010\n",
      "0077\n",
      "0017\n",
      "0084\n",
      "0004\n",
      "0087\n",
      "0069\n",
      "0011\n",
      "0064\n",
      "0070\n",
      "0065\n",
      "0093\n",
      "0083\n",
      "0001\n",
      "0094\n",
      "0079\n",
      "0085\n",
      "0029\n",
      "0067\n",
      "0058\n",
      "0073\n",
      "0023\n",
      "0022\n",
      "0063\n",
      "0032\n",
      "0020\n",
      "0024\n",
      "0040\n",
      "0026\n",
      "0052\n",
      "0041\n",
      "0043\n",
      "0098\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0075\n",
      "0096\n",
      "0006\n",
      "0013\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0053\n",
      "0078\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0090\n",
      "0009\n",
      "0068\n",
      "0099\n",
      "0086\n",
      "0007\n",
      "0074\n",
      "0092\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0078\n",
      "0055\n",
      "0021\n",
      "0008\n",
      "0091\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0057\n",
      "0037\n",
      "0068\n",
      "0086\n",
      "0095\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0045\n",
      "0056\n",
      "0022\n",
      "0066\n",
      "0030\n",
      "0026\n",
      "0036\n",
      "0043\n",
      "0041\n",
      "0075\n",
      "0046\n",
      "0047\n",
      "0028\n",
      "0075\n",
      "0046\n",
      "0096\n",
      "0006\n",
      "0041\n",
      "0036\n",
      "0063\n",
      "0022\n",
      "0030\n",
      "0024\n",
      "0066\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0056\n",
      "0023\n",
      "0045\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0095\n",
      "0057\n",
      "0037\n",
      "0091\n",
      "0008\n",
      "0092\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0007\n",
      "0059\n",
      "0074\n",
      "0072\n",
      "0060\n",
      "0042\n",
      "0009\n",
      "0086\n",
      "0037\n",
      "0099\n",
      "0068\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0091\n",
      "0090\n",
      "0003\n",
      "0078\n",
      "0053\n",
      "0082\n",
      "0013\n",
      "0007\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0006\n",
      "0096\n",
      "0026\n",
      "0098\n",
      "0043\n",
      "0052\n",
      "0063\n",
      "0040\n",
      "0024\n",
      "0020\n",
      "0032\n",
      "0039\n",
      "0023\n",
      "0029\n",
      "0094\n",
      "0001\n",
      "0067\n",
      "0049\n",
      "0064\n",
      "0083\n",
      "0065\n",
      "0070\n",
      "0087\n",
      "0004\n",
      "0011\n",
      "0069\n",
      "0089\n",
      "0048\n",
      "0014\n",
      "0077\n",
      "0076\n",
      "0005\n",
      "0038\n",
      "0071\n",
      "0035\n",
      "0033\n",
      "0054\n",
      "0015\n",
      "0034\n",
      "0005\n",
      "0002\n",
      "0051\n",
      "0016\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0025\n",
      "0012\n",
      "0001\n",
      "0085\n",
      "0079\n",
      "0049\n",
      "0093\n",
      "0083\n",
      "0065\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0089\n",
      "0019\n",
      "0011\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0069\n",
      "0089\n",
      "0019\n",
      "0087\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0004\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0067\n",
      "0079\n",
      "0085\n",
      "0094\n",
      "0012\n",
      "0093\n",
      "0064\n",
      "0049\n",
      "0025\n",
      "0033\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0015\n",
      "0005\n",
      "0016\n",
      "0051\n",
      "0002\n",
      "0025\n",
      "0054\n",
      "0062\n",
      "0033\n",
      "0080\n",
      "0034\n",
      "0015\n",
      "0038\n",
      "0035\n",
      "0071\n",
      "0016\n",
      "0051\n",
      "0069\n",
      "0011\n",
      "0018\n",
      "0004\n",
      "0087\n",
      "0077\n",
      "0067\n",
      "0094\n",
      "0001\n",
      "0029\n",
      "0070\n",
      "0065\n",
      "0083\n",
      "0064\n",
      "0032\n",
      "0030\n",
      "0020\n",
      "0066\n",
      "0024\n",
      "0040\n",
      "0063\n",
      "0045\n",
      "0023\n",
      "0096\n",
      "0006\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0046\n",
      "0052\n",
      "0098\n",
      "0043\n",
      "0036\n",
      "0026\n",
      "0078\n",
      "0053\n",
      "0061\n",
      "0092\n",
      "0074\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0013\n",
      "0099\n",
      "0068\n",
      "0086\n",
      "0009\n",
      "0097\n",
      "0081\n",
      "0090\n",
      "0008\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0053\n",
      "0092\n",
      "0074\n",
      "0013\n",
      "0007\n",
      "0037\n",
      "0057\n",
      "0095\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0008\n",
      "0090\n",
      "0091\n",
      "0027\n",
      "0066\n",
      "0030\n",
      "0022\n",
      "0045\n",
      "0056\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0046\n",
      "0075\n",
      "0044\n",
      "0036\n",
      "0052\n",
      "0098\n",
      "0041\n",
      "0041\n",
      "0036\n",
      "0046\n",
      "0075\n",
      "0056\n",
      "0045\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0030\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0066\n",
      "0022\n",
      "0091\n",
      "0008\n",
      "0100\n",
      "0095\n",
      "0099\n",
      "0037\n",
      "0057\n",
      "0009\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0074\n",
      "0007\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0092\n",
      "0090\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0095\n",
      "0086\n",
      "0057\n",
      "0068\n",
      "0099\n",
      "0009\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0013\n",
      "0053\n",
      "0078\n",
      "0043\n",
      "0098\n",
      "0052\n",
      "0026\n",
      "0006\n",
      "0096\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0023\n",
      "0056\n",
      "0040\n",
      "0024\n",
      "0020\n",
      "0032\n",
      "0063\n",
      "0083\n",
      "0065\n",
      "0070\n",
      "0064\n",
      "0067\n",
      "0029\n",
      "0012\n",
      "0001\n",
      "0094\n",
      "0077\n",
      "0050\n",
      "0011\n",
      "0069\n",
      "0019\n",
      "0087\n",
      "0004\n",
      "0071\n",
      "0035\n",
      "0002\n",
      "0038\n",
      "0015\n",
      "0034\n",
      "0031\n",
      "0033\n",
      "0054\n",
      "0016\n",
      "0002\n",
      "0051\n",
      "0038\n",
      "0005\n",
      "0025\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0093\n",
      "0049\n",
      "0012\n",
      "0085\n",
      "0079\n",
      "0029\n",
      "0076\n",
      "0014\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0019\n",
      "0089\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0034\n",
      "0015\n",
      "0033\n",
      "0031\n",
      "0054\n",
      "0002\n",
      "0071\n",
      "0035\n",
      "0038\n",
      "0050\n",
      "0077\n",
      "0087\n",
      "0004\n",
      "0069\n",
      "0011\n",
      "0019\n",
      "0064\n",
      "0065\n",
      "0083\n",
      "0070\n",
      "0029\n",
      "0001\n",
      "0094\n",
      "0012\n",
      "0067\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0019\n",
      "0089\n",
      "0049\n",
      "0093\n",
      "0012\n",
      "0029\n",
      "0085\n",
      "0079\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0025\n",
      "0002\n",
      "0051\n",
      "0016\n",
      "0005\n",
      "0038\n",
      "0007\n",
      "0074\n",
      "0092\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0100\n",
      "0091\n",
      "0008\n",
      "0009\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0037\n",
      "0057\n",
      "0099\n",
      "0095\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0056\n",
      "0045\n",
      "0022\n",
      "0032\n",
      "0030\n",
      "0020\n",
      "0066\n",
      "0040\n",
      "0041\n",
      "0036\n",
      "0046\n",
      "0075\n",
      "0023\n",
      "0056\n",
      "0063\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0026\n",
      "0052\n",
      "0098\n",
      "0043\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0006\n",
      "0096\n",
      "0013\n",
      "0060\n",
      "0072\n",
      "0059\n",
      "0053\n",
      "0078\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0090\n",
      "0009\n",
      "0068\n",
      "0099\n",
      "0057\n",
      "0095\n",
      "0086\n",
      "0009\n",
      "0081\n",
      "0097\n",
      "0086\n",
      "0099\n",
      "0068\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0090\n",
      "0008\n",
      "0092\n",
      "0061\n",
      "0078\n",
      "0053\n",
      "0013\n",
      "0059\n",
      "0074\n",
      "0072\n",
      "0060\n",
      "0046\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0096\n",
      "0006\n",
      "0026\n",
      "0043\n",
      "0098\n",
      "0052\n",
      "0036\n",
      "0063\n",
      "0030\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0066\n",
      "0023\n",
      "0045\n",
      "0044\n",
      "0046\n",
      "0075\n",
      "0036\n",
      "0041\n",
      "0098\n",
      "0052\n",
      "0022\n",
      "0066\n",
      "0030\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0045\n",
      "0056\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0095\n",
      "0037\n",
      "0057\n",
      "0027\n",
      "0008\n",
      "0091\n",
      "0090\n",
      "0092\n",
      "0053\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0013\n",
      "0007\n",
      "0074\n",
      "0005\n",
      "0051\n",
      "0002\n",
      "0016\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0033\n",
      "0025\n",
      "0015\n",
      "0079\n",
      "0085\n",
      "0012\n",
      "0094\n",
      "0067\n",
      "0049\n",
      "0064\n",
      "0093\n",
      "0087\n",
      "0004\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0069\n",
      "0089\n",
      "0019\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0014\n",
      "0076\n",
      "0094\n",
      "0001\n",
      "0029\n",
      "0067\n",
      "0064\n",
      "0070\n",
      "0083\n",
      "0065\n",
      "0004\n",
      "0018\n",
      "0087\n",
      "0011\n",
      "0069\n",
      "0077\n",
      "0038\n",
      "0051\n",
      "0035\n",
      "0016\n",
      "0071\n",
      "0054\n",
      "0080\n",
      "0033\n",
      "0062\n",
      "0025\n",
      "0015\n",
      "0034\n",
      "0033\n",
      "0054\n",
      "0034\n",
      "0015\n",
      "0038\n",
      "0005\n",
      "0071\n",
      "0035\n",
      "0069\n",
      "0011\n",
      "0089\n",
      "0087\n",
      "0004\n",
      "0077\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0067\n",
      "0029\n",
      "0094\n",
      "0001\n",
      "0065\n",
      "0083\n",
      "0070\n",
      "0064\n",
      "0049\n",
      "0089\n",
      "0019\n",
      "0011\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0076\n",
      "0014\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0001\n",
      "0012\n",
      "0085\n",
      "0079\n",
      "0065\n",
      "0093\n",
      "0083\n",
      "0049\n",
      "0025\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0005\n",
      "0016\n",
      "0002\n",
      "0051\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0092\n",
      "0074\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0007\n",
      "0057\n",
      "0037\n",
      "0095\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0091\n",
      "0008\n",
      "0030\n",
      "0066\n",
      "0024\n",
      "0022\n",
      "0063\n",
      "0056\n",
      "0045\n",
      "0023\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0096\n",
      "0006\n",
      "0075\n",
      "0046\n",
      "0041\n",
      "0036\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0063\n",
      "0023\n",
      "0039\n",
      "0006\n",
      "0096\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0052\n",
      "0043\n",
      "0098\n",
      "0026\n",
      "0082\n",
      "0078\n",
      "0053\n",
      "0003\n",
      "0060\n",
      "0072\n",
      "0059\n",
      "0013\n",
      "0007\n",
      "0099\n",
      "0068\n",
      "0037\n",
      "0086\n",
      "0042\n",
      "0009\n",
      "0090\n",
      "0091\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0090\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0086\n",
      "0068\n",
      "0099\n",
      "0009\n",
      "0059\n",
      "0072\n",
      "0060\n",
      "0013\n",
      "0053\n",
      "0078\n",
      "0098\n",
      "0043\n",
      "0041\n",
      "0052\n",
      "0026\n",
      "0096\n",
      "0006\n",
      "0075\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0023\n",
      "0058\n",
      "0073\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0063\n",
      "0022\n",
      "0036\n",
      "0041\n",
      "0043\n",
      "0026\n",
      "0047\n",
      "0028\n",
      "0075\n",
      "0046\n",
      "0045\n",
      "0056\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0066\n",
      "0030\n",
      "0022\n",
      "0008\n",
      "0091\n",
      "0055\n",
      "0021\n",
      "0086\n",
      "0095\n",
      "0068\n",
      "0057\n",
      "0037\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0074\n",
      "0007\n",
      "0078\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0092\n",
      "0016\n",
      "0071\n",
      "0035\n",
      "0051\n",
      "0002\n",
      "0005\n",
      "0034\n",
      "0025\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0054\n",
      "0093\n",
      "0070\n",
      "0049\n",
      "0079\n",
      "0085\n",
      "0012\n",
      "0077\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0019\n",
      "0089\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0070\n",
      "0093\n",
      "0083\n",
      "0065\n",
      "0064\n",
      "0067\n",
      "0001\n",
      "0094\n",
      "0029\n",
      "0079\n",
      "0085\n",
      "0077\n",
      "0010\n",
      "0011\n",
      "0069\n",
      "0004\n",
      "0017\n",
      "0084\n",
      "0087\n",
      "0035\n",
      "0071\n",
      "0038\n",
      "0015\n",
      "0034\n",
      "0054\n",
      "0088\n",
      "0033\n",
      "0005\n",
      "0051\n",
      "0002\n",
      "0016\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0025\n",
      "0085\n",
      "0079\n",
      "0001\n",
      "0012\n",
      "0049\n",
      "0065\n",
      "0083\n",
      "0093\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0011\n",
      "0019\n",
      "0089\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0014\n",
      "0076\n",
      "0001\n",
      "0094\n",
      "0029\n",
      "0067\n",
      "0064\n",
      "0049\n",
      "0070\n",
      "0065\n",
      "0083\n",
      "0004\n",
      "0087\n",
      "0089\n",
      "0069\n",
      "0011\n",
      "0048\n",
      "0076\n",
      "0077\n",
      "0014\n",
      "0005\n",
      "0038\n",
      "0035\n",
      "0071\n",
      "0054\n",
      "0033\n",
      "0034\n",
      "0015\n",
      "0009\n",
      "0042\n",
      "0068\n",
      "0099\n",
      "0037\n",
      "0086\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0090\n",
      "0091\n",
      "0082\n",
      "0053\n",
      "0078\n",
      "0003\n",
      "0007\n",
      "0013\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0096\n",
      "0006\n",
      "0026\n",
      "0052\n",
      "0098\n",
      "0043\n",
      "0063\n",
      "0032\n",
      "0020\n",
      "0024\n",
      "0040\n",
      "0039\n",
      "0023\n",
      "0046\n",
      "0075\n",
      "0006\n",
      "0096\n",
      "0036\n",
      "0041\n",
      "0022\n",
      "0063\n",
      "0066\n",
      "0024\n",
      "0030\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0045\n",
      "0023\n",
      "0056\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0037\n",
      "0057\n",
      "0095\n",
      "0008\n",
      "0091\n",
      "0092\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0007\n",
      "0060\n",
      "0072\n",
      "0074\n",
      "0059\n",
      "0007\n",
      "0074\n",
      "0092\n",
      "0078\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0021\n",
      "0055\n",
      "0091\n",
      "0008\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0095\n",
      "0086\n",
      "0068\n",
      "0037\n",
      "0057\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0056\n",
      "0045\n",
      "0022\n",
      "0030\n",
      "0066\n",
      "0026\n",
      "0041\n",
      "0043\n",
      "0036\n",
      "0028\n",
      "0047\n",
      "0046\n",
      "0075\n",
      "0073\n",
      "0058\n",
      "0023\n",
      "0063\n",
      "0022\n",
      "0040\n",
      "0024\n",
      "0020\n",
      "0032\n",
      "0026\n",
      "0043\n",
      "0098\n",
      "0041\n",
      "0052\n",
      "0075\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0006\n",
      "0096\n",
      "0013\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0078\n",
      "0053\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0090\n",
      "0009\n",
      "0086\n",
      "0099\n",
      "0068\n",
      "0015\n",
      "0034\n",
      "0088\n",
      "0033\n",
      "0054\n",
      "0071\n",
      "0035\n",
      "0038\n",
      "0010\n",
      "0077\n",
      "0087\n",
      "0004\n",
      "0084\n",
      "0017\n",
      "0011\n",
      "0069\n",
      "0064\n",
      "0083\n",
      "0093\n",
      "0065\n",
      "0070\n",
      "0029\n",
      "0079\n",
      "0085\n",
      "0094\n",
      "0001\n",
      "0067\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0076\n",
      "0077\n",
      "0014\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0089\n",
      "0019\n",
      "0049\n",
      "0070\n",
      "0093\n",
      "0012\n",
      "0079\n",
      "0085\n",
      "0034\n",
      "0054\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0025\n",
      "0002\n",
      "0051\n",
      "0035\n",
      "0016\n",
      "0071\n",
      "0005\n",
      "0016\n",
      "0051\n",
      "0002\n",
      "0038\n",
      "0005\n",
      "0025\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0093\n",
      "0049\n",
      "0029\n",
      "0085\n",
      "0079\n",
      "0012\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0089\n",
      "0019\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0070\n",
      "0065\n",
      "0083\n",
      "0064\n",
      "0067\n",
      "0094\n",
      "0001\n",
      "0012\n",
      "0029\n",
      "0077\n",
      "0050\n",
      "0019\n",
      "0069\n",
      "0011\n",
      "0004\n",
      "0087\n",
      "0035\n",
      "0071\n",
      "0002\n",
      "0038\n",
      "0034\n",
      "0015\n",
      "0054\n",
      "0033\n",
      "0031\n",
      "0090\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0099\n",
      "0068\n",
      "0057\n",
      "0086\n",
      "0095\n",
      "0009\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0013\n",
      "0078\n",
      "0053\n",
      "0052\n",
      "0043\n",
      "0098\n",
      "0026\n",
      "0096\n",
      "0006\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0056\n",
      "0023\n",
      "0032\n",
      "0020\n",
      "0024\n",
      "0040\n",
      "0063\n",
      "0036\n",
      "0041\n",
      "0075\n",
      "0046\n",
      "0045\n",
      "0056\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0066\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0030\n",
      "0022\n",
      "0008\n",
      "0091\n",
      "0100\n",
      "0057\n",
      "0037\n",
      "0099\n",
      "0095\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0009\n",
      "0074\n",
      "0007\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0092\n",
      "0053\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0092\n",
      "0074\n",
      "0007\n",
      "0013\n",
      "0095\n",
      "0057\n",
      "0037\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0091\n",
      "0090\n",
      "0008\n",
      "0027\n",
      "0030\n",
      "0066\n",
      "0022\n",
      "0056\n",
      "0045\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0044\n",
      "0075\n",
      "0046\n",
      "0041\n",
      "0098\n",
      "0052\n",
      "0036\n",
      "0040\n",
      "0024\n",
      "0066\n",
      "0020\n",
      "0030\n",
      "0032\n",
      "0063\n",
      "0023\n",
      "0045\n",
      "0006\n",
      "0096\n",
      "0046\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0036\n",
      "0098\n",
      "0043\n",
      "0052\n",
      "0026\n",
      "0061\n",
      "0053\n",
      "0078\n",
      "0092\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0074\n",
      "0013\n",
      "0086\n",
      "0068\n",
      "0099\n",
      "0081\n",
      "0097\n",
      "0009\n",
      "0008\n",
      "0090\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0025\n",
      "0080\n",
      "0033\n",
      "0062\n",
      "0054\n",
      "0015\n",
      "0034\n",
      "0038\n",
      "0016\n",
      "0071\n",
      "0035\n",
      "0051\n",
      "0011\n",
      "0069\n",
      "0087\n",
      "0004\n",
      "0018\n",
      "0077\n",
      "0067\n",
      "0029\n",
      "0001\n",
      "0094\n",
      "0083\n",
      "0065\n",
      "0070\n",
      "0064\n",
      "0019\n",
      "0089\n",
      "0069\n",
      "0004\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0087\n",
      "0076\n",
      "0014\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0067\n",
      "0012\n",
      "0094\n",
      "0079\n",
      "0085\n",
      "0093\n",
      "0049\n",
      "0064\n",
      "0025\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0033\n",
      "0015\n",
      "0005\n",
      "0016\n",
      "0002\n",
      "0051\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0063\n",
      "0056\n",
      "0023\n",
      "0096\n",
      "0006\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0043\n",
      "0098\n",
      "0052\n",
      "0026\n",
      "0078\n",
      "0053\n",
      "0059\n",
      "0072\n",
      "0060\n",
      "0013\n",
      "0086\n",
      "0095\n",
      "0057\n",
      "0099\n",
      "0068\n",
      "0009\n",
      "0090\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0092\n",
      "0074\n",
      "0007\n",
      "0095\n",
      "0099\n",
      "0037\n",
      "0057\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0009\n",
      "0008\n",
      "0091\n",
      "0100\n",
      "0040\n",
      "0066\n",
      "0020\n",
      "0030\n",
      "0032\n",
      "0022\n",
      "0045\n",
      "0056\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0046\n",
      "0075\n",
      "0036\n",
      "0041\n",
      "0089\n",
      "0019\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0085\n",
      "0079\n",
      "0029\n",
      "0012\n",
      "0093\n",
      "0049\n",
      "0025\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0038\n",
      "0005\n",
      "0016\n",
      "0051\n",
      "0002\n",
      "0054\n",
      "0031\n",
      "0033\n",
      "0015\n",
      "0034\n",
      "0038\n",
      "0035\n",
      "0071\n",
      "0002\n",
      "0019\n",
      "0011\n",
      "0069\n",
      "0004\n",
      "0087\n",
      "0077\n",
      "0050\n",
      "0067\n",
      "0012\n",
      "0094\n",
      "0001\n",
      "0029\n",
      "0070\n",
      "0083\n",
      "0065\n",
      "0064\n",
      "0065\n",
      "0083\n",
      "0070\n",
      "0064\n",
      "0067\n",
      "0029\n",
      "0001\n",
      "0094\n",
      "0077\n",
      "0069\n",
      "0011\n",
      "0087\n",
      "0018\n",
      "0004\n",
      "0071\n",
      "0016\n",
      "0035\n",
      "0051\n",
      "0038\n",
      "0034\n",
      "0015\n",
      "0025\n",
      "0062\n",
      "0033\n",
      "0080\n",
      "0054\n",
      "0016\n",
      "0002\n",
      "0051\n",
      "0005\n",
      "0015\n",
      "0025\n",
      "0033\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0093\n",
      "0064\n",
      "0049\n",
      "0067\n",
      "0094\n",
      "0012\n",
      "0079\n",
      "0085\n",
      "0076\n",
      "0014\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0019\n",
      "0089\n",
      "0069\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0004\n",
      "0087\n",
      "0052\n",
      "0098\n",
      "0041\n",
      "0036\n",
      "0046\n",
      "0075\n",
      "0044\n",
      "0056\n",
      "0045\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0030\n",
      "0066\n",
      "0022\n",
      "0090\n",
      "0091\n",
      "0008\n",
      "0027\n",
      "0037\n",
      "0057\n",
      "0095\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0074\n",
      "0007\n",
      "0013\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0053\n",
      "0092\n",
      "0008\n",
      "0090\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0068\n",
      "0099\n",
      "0086\n",
      "0097\n",
      "0081\n",
      "0009\n",
      "0060\n",
      "0072\n",
      "0074\n",
      "0059\n",
      "0013\n",
      "0053\n",
      "0078\n",
      "0061\n",
      "0092\n",
      "0036\n",
      "0052\n",
      "0098\n",
      "0043\n",
      "0026\n",
      "0006\n",
      "0096\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0046\n",
      "0045\n",
      "0023\n",
      "0066\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0030\n",
      "0063\n",
      "0039\n",
      "0023\n",
      "0063\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0026\n",
      "0098\n",
      "0043\n",
      "0052\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0096\n",
      "0006\n",
      "0007\n",
      "0013\n",
      "0059\n",
      "0072\n",
      "0060\n",
      "0003\n",
      "0053\n",
      "0078\n",
      "0082\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0091\n",
      "0090\n",
      "0009\n",
      "0042\n",
      "0086\n",
      "0037\n",
      "0068\n",
      "0099\n",
      "0007\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0074\n",
      "0092\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0008\n",
      "0091\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0095\n",
      "0057\n",
      "0037\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0023\n",
      "0045\n",
      "0056\n",
      "0063\n",
      "0022\n",
      "0024\n",
      "0066\n",
      "0030\n",
      "0036\n",
      "0041\n",
      "0075\n",
      "0046\n",
      "0006\n",
      "0096\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0014\n",
      "0076\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0011\n",
      "0019\n",
      "0089\n",
      "0049\n",
      "0083\n",
      "0093\n",
      "0065\n",
      "0085\n",
      "0079\n",
      "0012\n",
      "0001\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0025\n",
      "0051\n",
      "0002\n",
      "0016\n",
      "0005\n",
      "0015\n",
      "0034\n",
      "0054\n",
      "0033\n",
      "0035\n",
      "0071\n",
      "0005\n",
      "0038\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0077\n",
      "0004\n",
      "0087\n",
      "0089\n",
      "0011\n",
      "0069\n",
      "0049\n",
      "0064\n",
      "0070\n",
      "0083\n",
      "0065\n",
      "0001\n",
      "0094\n",
      "0029\n",
      "0067\n",
      "0079\n",
      "0085\n",
      "0029\n",
      "0094\n",
      "0001\n",
      "0067\n",
      "0064\n",
      "0065\n",
      "0083\n",
      "0093\n",
      "0070\n",
      "0087\n",
      "0084\n",
      "0017\n",
      "0004\n",
      "0069\n",
      "0011\n",
      "0010\n",
      "0077\n",
      "0038\n",
      "0071\n",
      "0035\n",
      "0033\n",
      "0088\n",
      "0054\n",
      "0034\n",
      "0015\n",
      "0005\n",
      "0002\n",
      "0051\n",
      "0035\n",
      "0071\n",
      "0016\n",
      "0054\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0025\n",
      "0034\n",
      "0012\n",
      "0079\n",
      "0085\n",
      "0049\n",
      "0070\n",
      "0093\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0089\n",
      "0019\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0077\n",
      "0075\n",
      "0046\n",
      "0028\n",
      "0047\n",
      "0026\n",
      "0043\n",
      "0041\n",
      "0036\n",
      "0022\n",
      "0030\n",
      "0066\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0056\n",
      "0045\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0057\n",
      "0037\n",
      "0068\n",
      "0095\n",
      "0086\n",
      "0021\n",
      "0055\n",
      "0091\n",
      "0008\n",
      "0092\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0078\n",
      "0007\n",
      "0074\n",
      "0009\n",
      "0099\n",
      "0068\n",
      "0086\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0090\n",
      "0078\n",
      "0053\n",
      "0013\n",
      "0060\n",
      "0072\n",
      "0059\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0075\n",
      "0006\n",
      "0096\n",
      "0026\n",
      "0052\n",
      "0041\n",
      "0043\n",
      "0098\n",
      "0022\n",
      "0063\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0073\n",
      "0058\n",
      "0023\n",
      "0041\n",
      "0036\n",
      "0096\n",
      "0006\n",
      "0046\n",
      "0075\n",
      "0056\n",
      "0023\n",
      "0045\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0030\n",
      "0024\n",
      "0066\n",
      "0063\n",
      "0022\n",
      "0091\n",
      "0008\n",
      "0095\n",
      "0037\n",
      "0057\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0059\n",
      "0074\n",
      "0072\n",
      "0060\n",
      "0007\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0092\n",
      "0091\n",
      "0090\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0086\n",
      "0037\n",
      "0099\n",
      "0068\n",
      "0042\n",
      "0009\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0013\n",
      "0007\n",
      "0003\n",
      "0078\n",
      "0053\n",
      "0082\n",
      "0043\n",
      "0098\n",
      "0052\n",
      "0026\n",
      "0006\n",
      "0096\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0023\n",
      "0039\n",
      "0040\n",
      "0024\n",
      "0020\n",
      "0032\n",
      "0063\n",
      "0083\n",
      "0065\n",
      "0070\n",
      "0049\n",
      "0064\n",
      "0067\n",
      "0029\n",
      "0094\n",
      "0001\n",
      "0014\n",
      "0077\n",
      "0076\n",
      "0048\n",
      "0011\n",
      "0069\n",
      "0089\n",
      "0087\n",
      "0004\n",
      "0071\n",
      "0035\n",
      "0038\n",
      "0005\n",
      "0015\n",
      "0034\n",
      "0033\n",
      "0054\n",
      "0016\n",
      "0002\n",
      "0051\n",
      "0005\n",
      "0025\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0093\n",
      "0083\n",
      "0065\n",
      "0049\n",
      "0012\n",
      "0001\n",
      "0085\n",
      "0079\n",
      "0076\n",
      "0014\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0089\n",
      "0019\n",
      "0011\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0019\n",
      "0089\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0014\n",
      "0077\n",
      "0076\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0079\n",
      "0085\n",
      "0012\n",
      "0093\n",
      "0070\n",
      "0049\n",
      "0025\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0054\n",
      "0034\n",
      "0005\n",
      "0071\n",
      "0016\n",
      "0035\n",
      "0051\n",
      "0002\n",
      "0054\n",
      "0033\n",
      "0088\n",
      "0034\n",
      "0015\n",
      "0038\n",
      "0035\n",
      "0071\n",
      "0069\n",
      "0011\n",
      "0017\n",
      "0084\n",
      "0004\n",
      "0087\n",
      "0077\n",
      "0010\n",
      "0067\n",
      "0001\n",
      "0094\n",
      "0079\n",
      "0085\n",
      "0029\n",
      "0070\n",
      "0065\n",
      "0093\n",
      "0083\n",
      "0064\n",
      "0032\n",
      "0020\n",
      "0024\n",
      "0040\n",
      "0022\n",
      "0063\n",
      "0023\n",
      "0058\n",
      "0073\n",
      "0096\n",
      "0006\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0075\n",
      "0052\n",
      "0041\n",
      "0098\n",
      "0043\n",
      "0026\n",
      "0053\n",
      "0078\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0013\n",
      "0068\n",
      "0099\n",
      "0086\n",
      "0009\n",
      "0090\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0078\n",
      "0092\n",
      "0074\n",
      "0007\n",
      "0037\n",
      "0057\n",
      "0068\n",
      "0086\n",
      "0095\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0008\n",
      "0091\n",
      "0055\n",
      "0021\n",
      "0066\n",
      "0030\n",
      "0022\n",
      "0045\n",
      "0056\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0046\n",
      "0075\n",
      "0047\n",
      "0028\n",
      "0036\n",
      "0043\n",
      "0041\n",
      "0026\n",
      "0075\n",
      "0046\n",
      "0041\n",
      "0036\n",
      "0022\n",
      "0030\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0066\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0056\n",
      "0045\n",
      "0009\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0095\n",
      "0099\n",
      "0057\n",
      "0037\n",
      "0100\n",
      "0091\n",
      "0008\n",
      "0092\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0007\n",
      "0074\n",
      "0009\n",
      "0095\n",
      "0086\n",
      "0057\n",
      "0068\n",
      "0099\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0090\n",
      "0053\n",
      "0078\n",
      "0013\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0006\n",
      "0096\n",
      "0026\n",
      "0098\n",
      "0043\n",
      "0052\n",
      "0063\n",
      "0040\n",
      "0024\n",
      "0020\n",
      "0032\n",
      "0023\n",
      "0056\n",
      "0029\n",
      "0012\n",
      "0001\n",
      "0094\n",
      "0067\n",
      "0064\n",
      "0083\n",
      "0065\n",
      "0070\n",
      "0087\n",
      "0004\n",
      "0011\n",
      "0069\n",
      "0019\n",
      "0050\n",
      "0077\n",
      "0038\n",
      "0002\n",
      "0071\n",
      "0035\n",
      "0031\n",
      "0033\n",
      "0054\n",
      "0015\n",
      "0034\n",
      "0005\n",
      "0038\n",
      "0002\n",
      "0051\n",
      "0016\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0025\n",
      "0012\n",
      "0085\n",
      "0079\n",
      "0029\n",
      "0049\n",
      "0093\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0019\n",
      "0089\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0014\n",
      "0076\n",
      "0087\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0004\n",
      "0069\n",
      "0089\n",
      "0019\n",
      "0064\n",
      "0049\n",
      "0093\n",
      "0079\n",
      "0085\n",
      "0094\n",
      "0012\n",
      "0067\n",
      "0015\n",
      "0033\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0025\n",
      "0051\n",
      "0002\n",
      "0016\n",
      "0005\n",
      "0034\n",
      "0015\n",
      "0054\n",
      "0062\n",
      "0033\n",
      "0080\n",
      "0025\n",
      "0051\n",
      "0035\n",
      "0071\n",
      "0016\n",
      "0038\n",
      "0077\n",
      "0018\n",
      "0004\n",
      "0087\n",
      "0069\n",
      "0011\n",
      "0064\n",
      "0070\n",
      "0065\n",
      "0083\n",
      "0094\n",
      "0001\n",
      "0029\n",
      "0067\n",
      "0045\n",
      "0023\n",
      "0063\n",
      "0032\n",
      "0030\n",
      "0020\n",
      "0066\n",
      "0024\n",
      "0040\n",
      "0026\n",
      "0052\n",
      "0043\n",
      "0098\n",
      "0036\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0046\n",
      "0096\n",
      "0006\n",
      "0013\n",
      "0074\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0092\n",
      "0078\n",
      "0053\n",
      "0061\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0090\n",
      "0008\n",
      "0009\n",
      "0097\n",
      "0081\n",
      "0099\n",
      "0068\n",
      "0086\n",
      "0013\n",
      "0007\n",
      "0074\n",
      "0092\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0053\n",
      "0027\n",
      "0008\n",
      "0090\n",
      "0091\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0057\n",
      "0037\n",
      "0095\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0045\n",
      "0056\n",
      "0022\n",
      "0066\n",
      "0030\n",
      "0036\n",
      "0052\n",
      "0098\n",
      "0041\n",
      "0075\n",
      "0046\n",
      "0044\n",
      "0090\n",
      "0008\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0086\n",
      "0099\n",
      "0068\n",
      "0009\n",
      "0081\n",
      "0097\n",
      "0059\n",
      "0074\n",
      "0072\n",
      "0060\n",
      "0013\n",
      "0061\n",
      "0078\n",
      "0053\n",
      "0092\n",
      "0098\n",
      "0043\n",
      "0052\n",
      "0036\n",
      "0026\n",
      "0096\n",
      "0006\n",
      "0046\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0023\n",
      "0045\n",
      "0030\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0066\n",
      "0063\n",
      "0036\n",
      "0041\n",
      "0098\n",
      "0052\n",
      "0044\n",
      "0075\n",
      "0046\n",
      "0045\n",
      "0056\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0066\n",
      "0030\n",
      "0022\n",
      "0008\n",
      "0091\n",
      "0090\n",
      "0027\n",
      "0095\n",
      "0057\n",
      "0037\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0074\n",
      "0013\n",
      "0007\n",
      "0053\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0092\n",
      "0016\n",
      "0051\n",
      "0002\n",
      "0005\n",
      "0015\n",
      "0025\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0033\n",
      "0093\n",
      "0049\n",
      "0064\n",
      "0067\n",
      "0079\n",
      "0085\n",
      "0012\n",
      "0094\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0069\n",
      "0089\n",
      "0019\n",
      "0087\n",
      "0004\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0070\n",
      "0083\n",
      "0065\n",
      "0064\n",
      "0067\n",
      "0094\n",
      "0001\n",
      "0029\n",
      "0077\n",
      "0011\n",
      "0069\n",
      "0004\n",
      "0018\n",
      "0087\n",
      "0035\n",
      "0016\n",
      "0071\n",
      "0051\n",
      "0038\n",
      "0015\n",
      "0034\n",
      "0025\n",
      "0054\n",
      "0080\n",
      "0033\n",
      "0062\n",
      "0033\n",
      "0031\n",
      "0054\n",
      "0034\n",
      "0015\n",
      "0038\n",
      "0071\n",
      "0035\n",
      "0002\n",
      "0069\n",
      "0011\n",
      "0019\n",
      "0087\n",
      "0004\n",
      "0077\n",
      "0050\n",
      "0067\n",
      "0029\n",
      "0001\n",
      "0094\n",
      "0012\n",
      "0065\n",
      "0083\n",
      "0070\n",
      "0064\n",
      "0019\n",
      "0089\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0076\n",
      "0014\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0012\n",
      "0029\n",
      "0085\n",
      "0079\n",
      "0093\n",
      "0049\n",
      "0025\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0038\n",
      "0005\n",
      "0016\n",
      "0002\n",
      "0051\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0092\n",
      "0074\n",
      "0007\n",
      "0057\n",
      "0037\n",
      "0099\n",
      "0095\n",
      "0009\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0091\n",
      "0008\n",
      "0100\n",
      "0032\n",
      "0030\n",
      "0020\n",
      "0066\n",
      "0040\n",
      "0022\n",
      "0056\n",
      "0045\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0075\n",
      "0046\n",
      "0041\n",
      "0036\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0063\n",
      "0023\n",
      "0056\n",
      "0006\n",
      "0096\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0052\n",
      "0043\n",
      "0098\n",
      "0026\n",
      "0053\n",
      "0078\n",
      "0060\n",
      "0072\n",
      "0059\n",
      "0013\n",
      "0068\n",
      "0099\n",
      "0057\n",
      "0095\n",
      "0086\n",
      "0009\n",
      "0090\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0009\n",
      "0086\n",
      "0068\n",
      "0099\n",
      "0021\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0090\n",
      "0053\n",
      "0078\n",
      "0013\n",
      "0059\n",
      "0072\n",
      "0060\n",
      "0075\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0096\n",
      "0006\n",
      "0026\n",
      "0043\n",
      "0098\n",
      "0041\n",
      "0052\n",
      "0063\n",
      "0022\n",
      "0020\n",
      "0032\n",
      "0040\n",
      "0024\n",
      "0058\n",
      "0073\n",
      "0023\n",
      "0047\n",
      "0028\n",
      "0046\n",
      "0075\n",
      "0026\n",
      "0036\n",
      "0041\n",
      "0043\n",
      "0022\n",
      "0066\n",
      "0030\n",
      "0073\n",
      "0058\n",
      "0039\n",
      "0045\n",
      "0056\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0086\n",
      "0095\n",
      "0068\n",
      "0037\n",
      "0057\n",
      "0055\n",
      "0021\n",
      "0008\n",
      "0091\n",
      "0092\n",
      "0078\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0007\n",
      "0074\n",
      "0005\n",
      "0051\n",
      "0002\n",
      "0016\n",
      "0071\n",
      "0035\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0054\n",
      "0025\n",
      "0034\n",
      "0079\n",
      "0085\n",
      "0012\n",
      "0049\n",
      "0093\n",
      "0070\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0019\n",
      "0089\n",
      "0048\n",
      "0050\n",
      "0010\n",
      "0077\n",
      "0014\n",
      "0076\n",
      "0001\n",
      "0094\n",
      "0029\n",
      "0079\n",
      "0085\n",
      "0067\n",
      "0064\n",
      "0070\n",
      "0093\n",
      "0083\n",
      "0065\n",
      "0004\n",
      "0017\n",
      "0084\n",
      "0087\n",
      "0011\n",
      "0069\n",
      "0010\n",
      "0077\n",
      "0038\n",
      "0035\n",
      "0071\n",
      "0054\n",
      "0088\n",
      "0033\n",
      "0015\n",
      "0034\n",
      "0034\n",
      "0015\n",
      "0033\n",
      "0054\n",
      "0071\n",
      "0035\n",
      "0005\n",
      "0038\n",
      "0048\n",
      "0077\n",
      "0014\n",
      "0076\n",
      "0087\n",
      "0004\n",
      "0069\n",
      "0011\n",
      "0089\n",
      "0064\n",
      "0049\n",
      "0065\n",
      "0083\n",
      "0070\n",
      "0029\n",
      "0094\n",
      "0001\n",
      "0067\n",
      "0010\n",
      "0050\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0089\n",
      "0019\n",
      "0011\n",
      "0049\n",
      "0065\n",
      "0093\n",
      "0083\n",
      "0001\n",
      "0012\n",
      "0085\n",
      "0079\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0025\n",
      "0002\n",
      "0051\n",
      "0016\n",
      "0005\n",
      "0007\n",
      "0074\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0092\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0091\n",
      "0008\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0037\n",
      "0057\n",
      "0095\n",
      "0039\n",
      "0058\n",
      "0073\n",
      "0056\n",
      "0045\n",
      "0023\n",
      "0022\n",
      "0063\n",
      "0030\n",
      "0066\n",
      "0024\n",
      "0041\n",
      "0036\n",
      "0046\n",
      "0075\n",
      "0096\n",
      "0006\n",
      "0039\n",
      "0023\n",
      "0063\n",
      "0024\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0026\n",
      "0052\n",
      "0098\n",
      "0043\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0006\n",
      "0096\n",
      "0013\n",
      "0007\n",
      "0060\n",
      "0072\n",
      "0059\n",
      "0082\n",
      "0078\n",
      "0053\n",
      "0003\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0021\n",
      "0090\n",
      "0091\n",
      "0042\n",
      "0009\n",
      "0099\n",
      "0068\n",
      "0037\n",
      "0086\n",
      "0078\n",
      "0003\n",
      "0061\n",
      "0082\n",
      "0092\n",
      "0074\n",
      "0007\n",
      "0095\n",
      "0086\n",
      "0068\n",
      "0057\n",
      "0037\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0091\n",
      "0008\n",
      "0021\n",
      "0055\n",
      "0030\n",
      "0066\n",
      "0022\n",
      "0056\n",
      "0045\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0028\n",
      "0047\n",
      "0075\n",
      "0046\n",
      "0041\n",
      "0043\n",
      "0036\n",
      "0026\n",
      "0040\n",
      "0024\n",
      "0020\n",
      "0032\n",
      "0063\n",
      "0022\n",
      "0023\n",
      "0073\n",
      "0058\n",
      "0006\n",
      "0096\n",
      "0075\n",
      "0047\n",
      "0044\n",
      "0028\n",
      "0098\n",
      "0043\n",
      "0041\n",
      "0052\n",
      "0026\n",
      "0078\n",
      "0053\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0013\n",
      "0086\n",
      "0099\n",
      "0068\n",
      "0009\n",
      "0090\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0088\n",
      "0033\n",
      "0054\n",
      "0015\n",
      "0034\n",
      "0038\n",
      "0071\n",
      "0035\n",
      "0011\n",
      "0069\n",
      "0087\n",
      "0004\n",
      "0084\n",
      "0017\n",
      "0077\n",
      "0010\n",
      "0067\n",
      "0029\n",
      "0079\n",
      "0085\n",
      "0094\n",
      "0001\n",
      "0083\n",
      "0093\n",
      "0065\n",
      "0070\n",
      "0064\n",
      "0089\n",
      "0019\n",
      "0017\n",
      "0018\n",
      "0084\n",
      "0076\n",
      "0077\n",
      "0014\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0012\n",
      "0079\n",
      "0085\n",
      "0070\n",
      "0093\n",
      "0049\n",
      "0025\n",
      "0054\n",
      "0088\n",
      "0080\n",
      "0031\n",
      "0062\n",
      "0034\n",
      "0005\n",
      "0035\n",
      "0016\n",
      "0071\n",
      "0002\n",
      "0051\n",
      "0016\n",
      "0051\n",
      "0002\n",
      "0005\n",
      "0025\n",
      "0062\n",
      "0031\n",
      "0080\n",
      "0088\n",
      "0065\n",
      "0083\n",
      "0093\n",
      "0049\n",
      "0085\n",
      "0079\n",
      "0001\n",
      "0012\n",
      "0014\n",
      "0076\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0011\n",
      "0019\n",
      "0089\n",
      "0018\n",
      "0084\n",
      "0017\n",
      "0070\n",
      "0065\n",
      "0083\n",
      "0064\n",
      "0049\n",
      "0067\n",
      "0001\n",
      "0094\n",
      "0029\n",
      "0076\n",
      "0077\n",
      "0014\n",
      "0048\n",
      "0089\n",
      "0069\n",
      "0011\n",
      "0004\n",
      "0087\n",
      "0035\n",
      "0071\n",
      "0038\n",
      "0005\n",
      "0034\n",
      "0015\n",
      "0054\n",
      "0033\n",
      "0090\n",
      "0091\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0068\n",
      "0099\n",
      "0037\n",
      "0086\n",
      "0009\n",
      "0042\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0007\n",
      "0013\n",
      "0082\n",
      "0053\n",
      "0078\n",
      "0003\n",
      "0052\n",
      "0043\n",
      "0098\n",
      "0026\n",
      "0096\n",
      "0006\n",
      "0028\n",
      "0047\n",
      "0044\n",
      "0023\n",
      "0039\n",
      "0032\n",
      "0020\n",
      "0024\n",
      "0040\n",
      "0063\n",
      "0036\n",
      "0041\n",
      "0006\n",
      "0096\n",
      "0075\n",
      "0046\n",
      "0045\n",
      "0023\n",
      "0056\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0066\n",
      "0024\n",
      "0030\n",
      "0022\n",
      "0063\n",
      "0008\n",
      "0091\n",
      "0057\n",
      "0037\n",
      "0095\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0060\n",
      "0072\n",
      "0074\n",
      "0059\n",
      "0007\n",
      "0082\n",
      "0003\n",
      "0061\n",
      "0092\n",
      "0007\n",
      "0013\n",
      "0074\n",
      "0092\n",
      "0053\n",
      "0061\n",
      "0003\n",
      "0082\n",
      "0027\n",
      "0091\n",
      "0090\n",
      "0008\n",
      "0042\n",
      "0097\n",
      "0081\n",
      "0095\n",
      "0037\n",
      "0057\n",
      "0058\n",
      "0039\n",
      "0073\n",
      "0056\n",
      "0045\n",
      "0022\n",
      "0030\n",
      "0066\n",
      "0041\n",
      "0098\n",
      "0052\n",
      "0036\n",
      "0044\n",
      "0046\n",
      "0075\n",
      "0023\n",
      "0045\n",
      "0063\n",
      "0040\n",
      "0024\n",
      "0066\n",
      "0020\n",
      "0030\n",
      "0032\n",
      "0026\n",
      "0036\n",
      "0043\n",
      "0098\n",
      "0052\n",
      "0046\n",
      "0028\n",
      "0044\n",
      "0047\n",
      "0006\n",
      "0096\n",
      "0013\n",
      "0072\n",
      "0060\n",
      "0059\n",
      "0074\n",
      "0092\n",
      "0061\n",
      "0053\n",
      "0078\n",
      "0055\n",
      "0100\n",
      "0027\n",
      "0021\n",
      "0008\n",
      "0090\n",
      "0081\n",
      "0097\n",
      "0009\n",
      "0086\n",
      "0068\n",
      "0099\n",
      "0015\n",
      "0034\n",
      "0080\n",
      "0033\n",
      "0062\n",
      "0054\n",
      "0025\n",
      "0051\n",
      "0016\n",
      "0071\n",
      "0035\n",
      "0038\n",
      "0077\n",
      "0087\n",
      "0004\n",
      "0018\n",
      "0011\n",
      "0069\n",
      "0064\n",
      "0083\n",
      "0065\n",
      "0070\n",
      "0029\n",
      "0001\n",
      "0094\n",
      "0067\n",
      "0050\n",
      "0010\n",
      "0048\n",
      "0076\n",
      "0014\n",
      "0004\n",
      "0084\n",
      "0018\n",
      "0017\n",
      "0087\n",
      "0019\n",
      "0089\n",
      "0069\n",
      "0049\n",
      "0064\n",
      "0093\n",
      "0012\n",
      "0094\n",
      "0079\n",
      "0085\n",
      "0067\n",
      "0015\n",
      "0080\n",
      "0088\n",
      "0031\n",
      "0062\n",
      "0033\n",
      "0025\n",
      "0002\n",
      "0051\n",
      "0016\n",
      "0005\n",
      "0005\n",
      "0038\n",
      "0051\n",
      "0002\n",
      "0016\n",
      "0062\n",
      "0031\n",
      "0088\n",
      "0080\n",
      "0025\n",
      "0029\n",
      "0085\n",
      "0079\n",
      "0012\n",
      "0049\n",
      "0093\n",
      "0017\n",
      "0084\n",
      "0018\n",
      "0089\n",
      "0019\n",
      "0048\n",
      "0010\n",
      "0050\n",
      "0014\n",
      "0076\n",
      "0094\n",
      "0001\n",
      "0012\n",
      "0029\n",
      "0067\n",
      "0064\n",
      "0070\n",
      "0065\n",
      "0083\n",
      "0004\n",
      "0087\n",
      "0019\n",
      "0069\n",
      "0011\n",
      "0050\n",
      "0077\n",
      "0038\n",
      "0002\n",
      "0035\n",
      "0071\n",
      "0054\n",
      "0033\n",
      "0031\n",
      "0034\n",
      "0015\n",
      "0009\n",
      "0099\n",
      "0068\n",
      "0057\n",
      "0086\n",
      "0095\n",
      "0021\n",
      "0027\n",
      "0100\n",
      "0055\n",
      "0090\n",
      "0078\n",
      "0053\n",
      "0013\n",
      "0059\n",
      "0060\n",
      "0072\n",
      "0044\n",
      "0047\n",
      "0028\n",
      "0096\n",
      "0006\n",
      "0026\n",
      "0052\n",
      "0098\n",
      "0043\n",
      "0063\n",
      "0032\n",
      "0020\n",
      "0024\n",
      "0040\n",
      "0056\n",
      "0023\n",
      "0046\n",
      "0075\n",
      "0036\n",
      "0041\n",
      "0022\n",
      "0066\n",
      "0040\n",
      "0032\n",
      "0020\n",
      "0030\n",
      "0073\n",
      "0039\n",
      "0058\n",
      "0045\n",
      "0056\n",
      "0081\n",
      "0097\n",
      "0042\n",
      "0009\n",
      "0037\n",
      "0057\n",
      "0099\n",
      "0095\n",
      "0100\n",
      "0008\n",
      "0091\n",
      "0092\n",
      "0082\n",
      "0061\n",
      "0003\n",
      "0007\n",
      "0074\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img_directory = r\"/Users/neerajakulkarni/Documents/WADABA\"\n",
    "images = os.listdir(img_directory)\n",
    "data = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_name(fn):\n",
    "    print(fn[0:4])\n",
    "    ptype = int(fn[fn.find('a')+1 : fn.find('a') + 3]) \n",
    "    ptype = ptype - 3 if ptype == 5 or ptype == 6 or ptype == 7 else ptype -1\n",
    "    color = fn[fn.find('b')+1 : fn.find('b') + 3]\n",
    "    light = fn[fn.find('c')+1 : fn.find('c') + 2]\n",
    "    deform = fn[fn.find('d')+1 : fn.find('d') + 2]\n",
    "    dirt = fn[fn.find('e')+1 : fn.find('e') + 2]\n",
    "    sl = fn[fn.find('f')+1 : fn.find('f') + 2]\n",
    "    ring = fn[fn.find('g')+1 : fn.find('g') + 2]\n",
    "    rand_pos = fn[fn.find('h')+1 : fn.find('h') + 2]\n",
    "    return ptype, color, light, deform, dirt, sl, ring, rand_pos\n",
    "\n",
    "\n",
    "\n",
    "for img in images:\n",
    "    if img[0].isnumeric():\n",
    "        current_img = Image.open((os.path.join(img_directory, img)))\n",
    "        ptype, color, light, deform, dirt, sl, ring, rand_pos = parse_name(img)\n",
    "        img_arr = np.asarray(current_img.resize((224,224)))\n",
    "        img_data = {'plastic_type': ptype, 'color': color, 'light': light, 'deformation':deform,\n",
    "                    'dirtiness':dirt, \"screw cap or lid\": sl, 'ring':ring, 'random position': rand_pos, 'image': img_arr}\n",
    "        current_img.close()\n",
    "        data.append(img_data)\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289a8a6",
   "metadata": {},
   "source": [
    "# Converting Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7802c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ebfa477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "plastic_type\n",
       "0    2200\n",
       "2     640\n",
       "1     600\n",
       "3     520\n",
       "4      40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "Image.fromarray(df['image'][0])\n",
    "\n",
    "df['plastic_type'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94167058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "df = df[df['plastic_type'] != 6]\n",
    "\n",
    "PET = df[df['plastic_type'] == 0]\n",
    "PE_HD = df[df['plastic_type'] == 1]\n",
    "PP = df[df['plastic_type'] == 2]\n",
    "PS = df[df['plastic_type'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f94f5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all_data(df):\n",
    "  X = df['image']\n",
    "  y = df['plastic_type']\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "      X, y, test_size=0.2, random_state=42) #42\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16d852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PET_x_train, PET_x_test, PET_y_train, PET_y_test = split_all_data(PET)\n",
    "PE_HD_x_train, PE_HD_x_test, PE_HD_y_train, PE_HD_y_test = split_all_data(PE_HD)\n",
    "PP_x_train, PP_x_test, PP_y_train, PP_y_test = split_all_data(PP)\n",
    "PS_x_train, PS_x_test, PS_y_train, PS_y_test = split_all_data(PS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aaeeef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([PET_x_train, PE_HD_x_train, PP_x_train, PS_x_train])\n",
    "X_test = pd.concat([PET_x_test, PE_HD_x_test, PP_x_test, PS_x_test])\n",
    "\n",
    "y_train = pd.concat([PET_y_train, PE_HD_y_train, PP_y_train, PS_y_train])\n",
    "y_test = pd.concat([PET_y_test, PE_HD_y_test, PP_y_test, PS_y_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88ab0189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([PET_x_test, PE_HD_x_test, PP_x_test, PS_x_test])\n",
    "y_test = pd.concat([PET_y_test, PE_HD_y_test, PP_y_test, PS_y_test])\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "      X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4916062",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "valid_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77a96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abf9673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_arr = self.df['image'].iloc[idx]\n",
    "        image = Image.fromarray(image_arr)\n",
    "        label = self.df['plastic_type'].iloc[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ec82ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = CustomImageDataset(train_df,transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]))\n",
    "valid_data = CustomImageDataset(valid_df,transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]))\n",
    "\n",
    "test_data = CustomImageDataset(test_df,transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]))\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_data, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a03cd",
   "metadata": {},
   "source": [
    "# Starting out our Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aeec8c",
   "metadata": {},
   "source": [
    "Modifying The CNNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87bc9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alexnet\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "alexnet.classifier[6] = nn.Linear(4096, 4)#Changing final layer\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1)) #adding a layer to classify\n",
    "\n",
    "\n",
    "#resnet50 takes pretty long\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet50.fc = nn.Linear(2048, 4)\n",
    "resnet50.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "\n",
    "\n",
    "#VGG SUPER SLOW\n",
    "\n",
    "VGG = models.vgg19()\n",
    "\n",
    "for param in VGG.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "VGG.classifier[6] = nn.Linear(4096, 4)#Changing final layer\n",
    "VGG.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1)) #adding a layer to classify\n",
    "\n",
    "# inception \n",
    "inception = models.inception_v3()\n",
    "\n",
    "for param in inception.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "inception.AuxLogits.fc = nn.Linear(768, 4) \n",
    "inception.fc = nn.Linear(2048, 4)  \n",
    "inception.add_module(\"7\", nn.LogSoftmax(dim=1))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "247394b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b7ad846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "           \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        \n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagate the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "            \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "            \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/len(train_data) \n",
    "        avg_train_acc = train_acc/len(train_data) \n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/len(test_data)  \n",
    "        avg_valid_acc = valid_acc/len(test_data) \n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "                \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "        \n",
    "        # Save if the model has best accuracy till now\n",
    "        #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
    "            \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "599994a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\n",
      "Batch number: 000, Training: Loss: 1.2911, Accuracy: 0.4375\n",
      "Batch number: 001, Training: Loss: 1.1647, Accuracy: 0.5000\n",
      "Batch number: 002, Training: Loss: 1.4770, Accuracy: 0.3750\n",
      "Batch number: 003, Training: Loss: 0.9001, Accuracy: 0.6875\n",
      "Batch number: 004, Training: Loss: 1.1494, Accuracy: 0.5625\n",
      "Batch number: 005, Training: Loss: 0.9567, Accuracy: 0.5000\n",
      "Batch number: 006, Training: Loss: 0.8961, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5828, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.7447, Accuracy: 0.7500\n",
      "Batch number: 009, Training: Loss: 0.7323, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 1.0602, Accuracy: 0.6875\n",
      "Batch number: 011, Training: Loss: 0.4207, Accuracy: 0.8125\n",
      "Batch number: 012, Training: Loss: 0.5283, Accuracy: 0.7500\n",
      "Batch number: 013, Training: Loss: 1.4017, Accuracy: 0.5000\n",
      "Batch number: 014, Training: Loss: 0.9059, Accuracy: 0.6875\n",
      "Batch number: 015, Training: Loss: 0.3440, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 0.8405, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.3949, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.7363, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 0.5675, Accuracy: 0.7500\n",
      "Batch number: 020, Training: Loss: 0.7702, Accuracy: 0.6250\n",
      "Batch number: 021, Training: Loss: 0.8276, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 0.7463, Accuracy: 0.6875\n",
      "Batch number: 023, Training: Loss: 0.6811, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 0.5341, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.6545, Accuracy: 0.7500\n",
      "Batch number: 026, Training: Loss: 0.5977, Accuracy: 0.8125\n",
      "Batch number: 027, Training: Loss: 0.8901, Accuracy: 0.5625\n",
      "Batch number: 028, Training: Loss: 0.7475, Accuracy: 0.6875\n",
      "Batch number: 029, Training: Loss: 0.5131, Accuracy: 0.8125\n",
      "Batch number: 030, Training: Loss: 0.3186, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.5534, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 0.4176, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 0.6647, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.8453, Accuracy: 0.6875\n",
      "Batch number: 035, Training: Loss: 0.3802, Accuracy: 0.8125\n",
      "Batch number: 036, Training: Loss: 0.5162, Accuracy: 0.7500\n",
      "Batch number: 037, Training: Loss: 0.5025, Accuracy: 0.8125\n",
      "Batch number: 038, Training: Loss: 0.4331, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 1.0260, Accuracy: 0.5625\n",
      "Batch number: 040, Training: Loss: 1.0407, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.4365, Accuracy: 0.8125\n",
      "Batch number: 042, Training: Loss: 0.5277, Accuracy: 0.8125\n",
      "Batch number: 043, Training: Loss: 0.4567, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 0.4762, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 0.6616, Accuracy: 0.6875\n",
      "Batch number: 046, Training: Loss: 0.6179, Accuracy: 0.6875\n",
      "Batch number: 047, Training: Loss: 0.8920, Accuracy: 0.5625\n",
      "Batch number: 048, Training: Loss: 0.7664, Accuracy: 0.6250\n",
      "Batch number: 049, Training: Loss: 0.3926, Accuracy: 0.8125\n",
      "Batch number: 050, Training: Loss: 0.5173, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 0.5800, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.3186, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.3628, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.6468, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.6328, Accuracy: 0.6875\n",
      "Batch number: 056, Training: Loss: 0.1406, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.4595, Accuracy: 0.8125\n",
      "Batch number: 058, Training: Loss: 0.7648, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.6976, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 0.1282, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.5768, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 0.3902, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.5280, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.7424, Accuracy: 0.6875\n",
      "Batch number: 065, Training: Loss: 0.2560, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.5236, Accuracy: 0.8125\n",
      "Batch number: 067, Training: Loss: 0.4443, Accuracy: 0.7500\n",
      "Batch number: 068, Training: Loss: 0.1591, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.2555, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.5200, Accuracy: 0.7500\n",
      "Batch number: 071, Training: Loss: 0.5210, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 0.5755, Accuracy: 0.8125\n",
      "Batch number: 073, Training: Loss: 0.2979, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.4298, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.3698, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.6783, Accuracy: 0.8750\n",
      "Batch number: 077, Training: Loss: 0.8222, Accuracy: 0.6875\n",
      "Batch number: 078, Training: Loss: 0.7895, Accuracy: 0.7500\n",
      "Batch number: 079, Training: Loss: 0.4230, Accuracy: 0.8125\n",
      "Batch number: 080, Training: Loss: 0.4342, Accuracy: 0.8125\n",
      "Batch number: 081, Training: Loss: 0.5006, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 0.2550, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.2721, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.4685, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.4126, Accuracy: 0.8125\n",
      "Batch number: 086, Training: Loss: 0.3549, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.1112, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.3293, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.5527, Accuracy: 0.8125\n",
      "Batch number: 090, Training: Loss: 0.3512, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.5383, Accuracy: 0.6875\n",
      "Batch number: 092, Training: Loss: 0.5099, Accuracy: 0.7500\n",
      "Batch number: 093, Training: Loss: 0.3046, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 0.2918, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.4674, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.5249, Accuracy: 0.8125\n",
      "Batch number: 097, Training: Loss: 0.7434, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 0.5339, Accuracy: 0.7500\n",
      "Batch number: 099, Training: Loss: 0.4626, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.5183, Accuracy: 0.7500\n",
      "Batch number: 101, Training: Loss: 0.6892, Accuracy: 0.7500\n",
      "Batch number: 102, Training: Loss: 0.3201, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.5887, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 0.4442, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 0.5868, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.3051, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.2891, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.4107, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 0.6780, Accuracy: 0.6875\n",
      "Batch number: 110, Training: Loss: 1.0525, Accuracy: 0.6875\n",
      "Batch number: 111, Training: Loss: 0.6447, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.7453, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 0.3066, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.1867, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.5151, Accuracy: 0.7500\n",
      "Batch number: 116, Training: Loss: 0.4699, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.8865, Accuracy: 0.6250\n",
      "Batch number: 118, Training: Loss: 0.3979, Accuracy: 0.8125\n",
      "Batch number: 119, Training: Loss: 0.6051, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.4308, Accuracy: 0.8125\n",
      "Batch number: 121, Training: Loss: 0.5750, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.2363, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.3115, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.5753, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.6354, Accuracy: 0.6875\n",
      "Batch number: 126, Training: Loss: 0.5134, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 0.2220, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.3670, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 0.4029, Accuracy: 0.8125\n",
      "Batch number: 130, Training: Loss: 0.6318, Accuracy: 0.7500\n",
      "Batch number: 131, Training: Loss: 0.2044, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.1704, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.3705, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.3645, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.4906, Accuracy: 0.8125\n",
      "Batch number: 136, Training: Loss: 0.6702, Accuracy: 0.6875\n",
      "Batch number: 137, Training: Loss: 0.2057, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.4379, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 0.4546, Accuracy: 0.7500\n",
      "Batch number: 140, Training: Loss: 0.5608, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 0.4868, Accuracy: 0.7500\n",
      "Batch number: 142, Training: Loss: 0.2398, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.2707, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.4866, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5580, Accuracy: 0.7500\n",
      "Batch number: 146, Training: Loss: 0.5186, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.2610, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.5059, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 0.2274, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.3691, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.4333, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.4264, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.2282, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.3748, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.5178, Accuracy: 0.8750\n",
      "Batch number: 156, Training: Loss: 0.5488, Accuracy: 0.6875\n",
      "Batch number: 157, Training: Loss: 0.2327, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.2452, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.3526, Accuracy: 0.8125\n",
      "Batch number: 160, Training: Loss: 0.2218, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.2336, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.2151, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.6441, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.4277, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.5605, Accuracy: 0.8125\n",
      "Batch number: 166, Training: Loss: 0.4809, Accuracy: 0.8125\n",
      "Batch number: 167, Training: Loss: 0.2858, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.2982, Accuracy: 0.8750\n",
      "Batch number: 169, Training: Loss: 0.3673, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.3537, Accuracy: 0.8125\n",
      "Batch number: 171, Training: Loss: 0.6613, Accuracy: 0.6875\n",
      "Batch number: 172, Training: Loss: 0.4064, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.3413, Accuracy: 0.8750\n",
      "Batch number: 174, Training: Loss: 0.8417, Accuracy: 0.5625\n",
      "Batch number: 175, Training: Loss: 0.2059, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.4101, Accuracy: 0.8125\n",
      "Batch number: 177, Training: Loss: 0.1797, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.3857, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.1973, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.2738, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.2544, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.5979, Accuracy: 0.7500\n",
      "Batch number: 183, Training: Loss: 0.4192, Accuracy: 0.8125\n",
      "Batch number: 184, Training: Loss: 0.2906, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.3601, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.2338, Accuracy: 0.8125\n",
      "Batch number: 187, Training: Loss: 0.2261, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.3755, Accuracy: 0.8125\n",
      "Batch number: 189, Training: Loss: 0.4229, Accuracy: 0.8125\n",
      "Batch number: 190, Training: Loss: 0.5862, Accuracy: 0.7500\n",
      "Batch number: 191, Training: Loss: 0.2734, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.2918, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.6872, Accuracy: 0.6875\n",
      "Batch number: 194, Training: Loss: 0.5108, Accuracy: 0.8125\n",
      "Batch number: 195, Training: Loss: 0.3615, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.2022, Accuracy: 0.8750\n",
      "Batch number: 197, Training: Loss: 0.2539, Accuracy: 0.8750\n",
      "Epoch : 001, Training: Loss: 0.5053, Accuracy: 79.7664%, \n",
      "\t\tValidation : Loss : 0.2890, Accuracy: 87.1212%, Time: 35.7477s\n",
      "Epoch: 2/50\n",
      "Batch number: 000, Training: Loss: 0.2385, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.2568, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.3108, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.3516, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.3980, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.1680, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.1878, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.3075, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 0.3970, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.3589, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.1909, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.1485, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.4625, Accuracy: 0.6875\n",
      "Batch number: 013, Training: Loss: 0.2587, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 0.2649, Accuracy: 0.8125\n",
      "Batch number: 015, Training: Loss: 0.5085, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.2836, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.2015, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0697, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.4280, Accuracy: 0.7500\n",
      "Batch number: 020, Training: Loss: 1.2055, Accuracy: 0.5625\n",
      "Batch number: 021, Training: Loss: 0.2204, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.4539, Accuracy: 0.8125\n",
      "Batch number: 023, Training: Loss: 0.2627, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.2315, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.3448, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 0.2607, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.3437, Accuracy: 0.7500\n",
      "Batch number: 028, Training: Loss: 0.3356, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.2311, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.2555, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.3295, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 0.1357, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.3484, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.4733, Accuracy: 0.8125\n",
      "Batch number: 035, Training: Loss: 0.6869, Accuracy: 0.7500\n",
      "Batch number: 036, Training: Loss: 0.8303, Accuracy: 0.6875\n",
      "Batch number: 037, Training: Loss: 0.3327, Accuracy: 0.8125\n",
      "Batch number: 038, Training: Loss: 0.1740, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.4509, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 0.2291, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0806, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.6012, Accuracy: 0.6875\n",
      "Batch number: 043, Training: Loss: 0.2859, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 0.4272, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.3211, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.4785, Accuracy: 0.8125\n",
      "Batch number: 047, Training: Loss: 0.1040, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.4174, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.3603, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.5418, Accuracy: 0.7500\n",
      "Batch number: 051, Training: Loss: 0.2716, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.4586, Accuracy: 0.7500\n",
      "Batch number: 053, Training: Loss: 0.4461, Accuracy: 0.8125\n",
      "Batch number: 054, Training: Loss: 0.6032, Accuracy: 0.7500\n",
      "Batch number: 055, Training: Loss: 0.2147, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.2347, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.1685, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.2783, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.3032, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.3513, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.1361, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.3651, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.1626, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.5732, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.2770, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 0.2737, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.1335, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.1341, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.2280, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.4636, Accuracy: 0.6250\n",
      "Batch number: 071, Training: Loss: 0.1506, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.3320, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.2787, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.5634, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.1830, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.1455, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.4818, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.2207, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.5064, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.1663, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.2806, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.3639, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.1859, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.5090, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.0952, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.2667, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.5079, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.2779, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.3365, Accuracy: 0.8125\n",
      "Batch number: 090, Training: Loss: 0.2025, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.3519, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.2710, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.4319, Accuracy: 0.8125\n",
      "Batch number: 094, Training: Loss: 0.1754, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.4197, Accuracy: 0.8125\n",
      "Batch number: 096, Training: Loss: 0.1590, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.3550, Accuracy: 0.8125\n",
      "Batch number: 098, Training: Loss: 0.3710, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.5069, Accuracy: 0.7500\n",
      "Batch number: 100, Training: Loss: 0.3922, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.5995, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.2431, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.1214, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.8113, Accuracy: 0.7500\n",
      "Batch number: 105, Training: Loss: 0.4372, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 0.1865, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1706, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.4696, Accuracy: 0.7500\n",
      "Batch number: 109, Training: Loss: 0.5649, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.1346, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.4597, Accuracy: 0.8125\n",
      "Batch number: 112, Training: Loss: 0.3142, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.2598, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.3934, Accuracy: 0.8125\n",
      "Batch number: 115, Training: Loss: 0.3958, Accuracy: 0.7500\n",
      "Batch number: 116, Training: Loss: 0.3059, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.1547, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.1980, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1626, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.6199, Accuracy: 0.7500\n",
      "Batch number: 121, Training: Loss: 0.1105, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.3310, Accuracy: 0.8125\n",
      "Batch number: 123, Training: Loss: 0.4007, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.0891, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.3958, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.1237, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.1251, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.3216, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.0389, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.2137, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0429, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.2770, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 0.1462, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.4490, Accuracy: 0.8125\n",
      "Batch number: 135, Training: Loss: 0.2217, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.0532, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.2594, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.6278, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.2316, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0830, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.6090, Accuracy: 0.7500\n",
      "Batch number: 142, Training: Loss: 0.2432, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.3864, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.5699, Accuracy: 0.7500\n",
      "Batch number: 145, Training: Loss: 0.4203, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.6415, Accuracy: 0.7500\n",
      "Batch number: 147, Training: Loss: 0.3011, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 0.6898, Accuracy: 0.7500\n",
      "Batch number: 149, Training: Loss: 0.2960, Accuracy: 0.8125\n",
      "Batch number: 150, Training: Loss: 0.5145, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 0.0976, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.6745, Accuracy: 0.6875\n",
      "Batch number: 153, Training: Loss: 0.1953, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.1476, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.3101, Accuracy: 0.8125\n",
      "Batch number: 156, Training: Loss: 0.3495, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.2021, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.1045, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.1314, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.2618, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.1446, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.3492, Accuracy: 0.8125\n",
      "Batch number: 163, Training: Loss: 0.1231, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.4427, Accuracy: 0.8125\n",
      "Batch number: 165, Training: Loss: 0.7266, Accuracy: 0.5625\n",
      "Batch number: 166, Training: Loss: 0.5407, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.1147, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.2057, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.4839, Accuracy: 0.7500\n",
      "Batch number: 170, Training: Loss: 0.4004, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.3338, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.3360, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0684, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.2600, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.3460, Accuracy: 0.8125\n",
      "Batch number: 176, Training: Loss: 0.3142, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.3878, Accuracy: 0.8125\n",
      "Batch number: 178, Training: Loss: 0.2825, Accuracy: 0.8125\n",
      "Batch number: 179, Training: Loss: 0.5248, Accuracy: 0.8125\n",
      "Batch number: 180, Training: Loss: 0.3477, Accuracy: 0.8125\n",
      "Batch number: 181, Training: Loss: 0.2619, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.2094, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.4730, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.2430, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.4293, Accuracy: 0.7500\n",
      "Batch number: 187, Training: Loss: 0.7832, Accuracy: 0.7500\n",
      "Batch number: 188, Training: Loss: 0.4414, Accuracy: 0.7500\n",
      "Batch number: 189, Training: Loss: 0.4245, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.3390, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.1401, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.2722, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.2557, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.2736, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.1877, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.1714, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.2676, Accuracy: 0.8750\n",
      "Epoch : 002, Training: Loss: 0.3219, Accuracy: 87.5947%, \n",
      "\t\tValidation : Loss : 0.1986, Accuracy: 92.9293%, Time: 33.5117s\n",
      "Epoch: 3/50\n",
      "Batch number: 000, Training: Loss: 0.3100, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 0.2124, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.2152, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.1962, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.4522, Accuracy: 0.8125\n",
      "Batch number: 005, Training: Loss: 0.1831, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.2459, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.1348, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.1883, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.2334, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.1905, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.4687, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.1408, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.3686, Accuracy: 0.7500\n",
      "Batch number: 014, Training: Loss: 0.0491, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.3327, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.1935, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.2365, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.3762, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.3075, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.0520, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.3347, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.3478, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.3596, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.2234, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.3929, Accuracy: 0.8125\n",
      "Batch number: 026, Training: Loss: 0.3222, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.0858, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.2943, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.4668, Accuracy: 0.8125\n",
      "Batch number: 030, Training: Loss: 0.3134, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.3001, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 0.2165, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.1820, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.5338, Accuracy: 0.7500\n",
      "Batch number: 035, Training: Loss: 0.4701, Accuracy: 0.6875\n",
      "Batch number: 036, Training: Loss: 0.2363, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.1685, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.2003, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.0739, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.3545, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.4852, Accuracy: 0.8125\n",
      "Batch number: 042, Training: Loss: 0.2303, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.2309, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.1626, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.4661, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 0.2694, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.2918, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.3758, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 0.4082, Accuracy: 0.8125\n",
      "Batch number: 050, Training: Loss: 0.3566, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.5201, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.3683, Accuracy: 0.8125\n",
      "Batch number: 053, Training: Loss: 0.2797, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.2417, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.2574, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.3888, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.3787, Accuracy: 0.8125\n",
      "Batch number: 058, Training: Loss: 0.3239, Accuracy: 0.8125\n",
      "Batch number: 059, Training: Loss: 0.3856, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 0.1644, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0702, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.1631, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.6049, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.1433, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.2161, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.4340, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.2779, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.1636, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.4177, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.4152, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 0.1729, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.0762, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.2402, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 0.7296, Accuracy: 0.7500\n",
      "Batch number: 075, Training: Loss: 0.2068, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.1246, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.1469, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.2070, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.2512, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 0.1329, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.4535, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.3278, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.1817, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.1265, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.1801, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.1244, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.5702, Accuracy: 0.7500\n",
      "Batch number: 088, Training: Loss: 0.3571, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.1682, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.0999, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.1393, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.1193, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.2449, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.1041, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.3414, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.3333, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.1286, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.3635, Accuracy: 0.8125\n",
      "Batch number: 099, Training: Loss: 0.2270, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.3692, Accuracy: 0.7500\n",
      "Batch number: 101, Training: Loss: 0.8181, Accuracy: 0.7500\n",
      "Batch number: 102, Training: Loss: 0.2286, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.3150, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.3481, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.6768, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 0.1514, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1279, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.2650, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.2976, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 0.2056, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.2556, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.3203, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.2206, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.2552, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.2439, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.4851, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 0.1315, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.3869, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.2744, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 0.1512, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.2546, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.2885, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.0251, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.4779, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.3730, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.1473, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0782, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.4041, Accuracy: 0.7500\n",
      "Batch number: 129, Training: Loss: 0.2199, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.3461, Accuracy: 0.7500\n",
      "Batch number: 131, Training: Loss: 0.4413, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.0957, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0766, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1592, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.1883, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1507, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.1390, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.1164, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.3320, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.4487, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 0.0810, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1316, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.1621, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.1313, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.2470, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.4207, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.2368, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.3560, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.1972, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.2332, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 0.3750, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.1854, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.2201, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.1512, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.4481, Accuracy: 0.8750\n",
      "Batch number: 156, Training: Loss: 0.4391, Accuracy: 0.7500\n",
      "Batch number: 157, Training: Loss: 0.1597, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.1230, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.6075, Accuracy: 0.6250\n",
      "Batch number: 160, Training: Loss: 0.1242, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.2364, Accuracy: 0.8750\n",
      "Batch number: 162, Training: Loss: 0.4421, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.1432, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.1818, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.1286, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.0885, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.2916, Accuracy: 0.8750\n",
      "Batch number: 168, Training: Loss: 0.0766, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.4160, Accuracy: 0.8125\n",
      "Batch number: 170, Training: Loss: 0.1751, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.1772, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0881, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.0576, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.4869, Accuracy: 0.7500\n",
      "Batch number: 175, Training: Loss: 0.1280, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.2681, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1838, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.1767, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.0575, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2215, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0372, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.2311, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.2746, Accuracy: 0.8125\n",
      "Batch number: 184, Training: Loss: 0.2692, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.3883, Accuracy: 0.8125\n",
      "Batch number: 186, Training: Loss: 0.2041, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.2086, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.1533, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.3577, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.1562, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.3350, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.2408, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.6658, Accuracy: 0.7500\n",
      "Batch number: 195, Training: Loss: 0.3117, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.1583, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2675, Accuracy: 0.8125\n",
      "Epoch : 003, Training: Loss: 0.2625, Accuracy: 89.6149%, \n",
      "\t\tValidation : Loss : 0.1866, Accuracy: 93.4343%, Time: 34.6971s\n",
      "Epoch: 4/50\n",
      "Batch number: 000, Training: Loss: 0.1390, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.1269, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.1150, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.6531, Accuracy: 0.7500\n",
      "Batch number: 004, Training: Loss: 0.1429, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.1112, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.2949, Accuracy: 0.8125\n",
      "Batch number: 007, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.2305, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.3643, Accuracy: 0.8125\n",
      "Batch number: 010, Training: Loss: 0.1645, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.4090, Accuracy: 0.8125\n",
      "Batch number: 012, Training: Loss: 0.1147, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.4998, Accuracy: 0.7500\n",
      "Batch number: 014, Training: Loss: 0.1505, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.3217, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.2129, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.3331, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0762, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0383, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.3372, Accuracy: 0.7500\n",
      "Batch number: 022, Training: Loss: 0.1118, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0176, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0870, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2610, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.2842, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.4599, Accuracy: 0.7500\n",
      "Batch number: 028, Training: Loss: 0.2653, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.1906, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.1182, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.1212, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.2786, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 0.2348, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.1414, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.2669, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.3108, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.1391, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.2685, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.1077, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.2399, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.1447, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.5272, Accuracy: 0.8125\n",
      "Batch number: 043, Training: Loss: 0.1943, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.1365, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.3223, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.3918, Accuracy: 0.8125\n",
      "Batch number: 047, Training: Loss: 0.1193, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.1140, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.1103, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.1810, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.1719, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.1847, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1616, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0274, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.3048, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.5387, Accuracy: 0.8125\n",
      "Batch number: 057, Training: Loss: 0.2535, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.1552, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.3715, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.2452, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.1010, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.4041, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.2272, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.3995, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.4993, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 0.0656, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.2226, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.3014, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.4554, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 0.1181, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.0465, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.4184, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 0.2302, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.1110, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.2027, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.1623, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.3272, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.2506, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.4094, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0669, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.1429, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.1505, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.0818, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.2893, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.0983, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.3178, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.2152, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.3725, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.4553, Accuracy: 0.6875\n",
      "Batch number: 091, Training: Loss: 0.1130, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.1469, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0636, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.3069, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.3305, Accuracy: 0.8125\n",
      "Batch number: 096, Training: Loss: 0.2082, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.3289, Accuracy: 0.6875\n",
      "Batch number: 098, Training: Loss: 0.1866, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.4509, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.2102, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0936, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.2505, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.1897, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1097, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0650, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.3437, Accuracy: 0.6875\n",
      "Batch number: 107, Training: Loss: 0.3985, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.5356, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.2950, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.3271, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.2323, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.1407, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.2390, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.2364, Accuracy: 0.8750\n",
      "Batch number: 116, Training: Loss: 0.4368, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 0.1891, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.1914, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1792, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.2291, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.0897, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.1538, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1871, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1664, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.4965, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.2308, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.1204, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.4577, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.5496, Accuracy: 0.7500\n",
      "Batch number: 130, Training: Loss: 0.2284, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.1984, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0948, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.3139, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.2292, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0804, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.3292, Accuracy: 0.8125\n",
      "Batch number: 137, Training: Loss: 0.1593, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.1894, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.1601, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.1978, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.1386, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1367, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0696, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.3826, Accuracy: 0.8125\n",
      "Batch number: 145, Training: Loss: 0.2659, Accuracy: 0.8125\n",
      "Batch number: 146, Training: Loss: 0.4221, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.2838, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.3732, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 0.2532, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.1332, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.2859, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.0758, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.1474, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0921, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0613, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.3362, Accuracy: 0.8125\n",
      "Batch number: 157, Training: Loss: 0.2996, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.2244, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.1778, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.1707, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.4323, Accuracy: 0.7500\n",
      "Batch number: 162, Training: Loss: 0.1376, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.4394, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.3561, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1467, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.0914, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.1160, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.1383, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.7154, Accuracy: 0.7500\n",
      "Batch number: 170, Training: Loss: 0.6295, Accuracy: 0.7500\n",
      "Batch number: 171, Training: Loss: 0.1411, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.1510, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.2447, Accuracy: 0.8750\n",
      "Batch number: 174, Training: Loss: 0.3762, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.2121, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.1801, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.1090, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0401, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.2369, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.2541, Accuracy: 0.8125\n",
      "Batch number: 181, Training: Loss: 0.1880, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.7938, Accuracy: 0.6875\n",
      "Batch number: 183, Training: Loss: 0.5322, Accuracy: 0.6875\n",
      "Batch number: 184, Training: Loss: 0.1212, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.2072, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.3257, Accuracy: 0.8125\n",
      "Batch number: 187, Training: Loss: 0.2587, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.3398, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.2047, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.2007, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.2914, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.1014, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.3443, Accuracy: 0.8750\n",
      "Batch number: 194, Training: Loss: 0.0648, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.4921, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.3188, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.1578, Accuracy: 1.0000\n",
      "Epoch : 004, Training: Loss: 0.2378, Accuracy: 90.1831%, \n",
      "\t\tValidation : Loss : 0.1737, Accuracy: 94.6970%, Time: 34.8741s\n",
      "Epoch: 5/50\n",
      "Batch number: 000, Training: Loss: 0.1677, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 0.4268, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.4129, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.0442, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0696, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1587, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0522, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.1342, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0400, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1631, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.2041, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.2235, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.1373, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.0673, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0569, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.3730, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 0.1835, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.3007, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.1139, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.1407, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.2119, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.3432, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 0.0728, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0942, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0986, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.3275, Accuracy: 0.8125\n",
      "Batch number: 026, Training: Loss: 0.0493, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1113, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0497, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.1550, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.1193, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.1049, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1889, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 0.1749, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0949, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.1734, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.2870, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.2676, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.2315, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.1123, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.1392, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.3495, Accuracy: 0.7500\n",
      "Batch number: 042, Training: Loss: 0.2266, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0559, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0346, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1363, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.1630, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.2392, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.2108, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.4769, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.1764, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.2445, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1368, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.1116, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.1449, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.3340, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.2624, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.2276, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.0633, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.0850, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0629, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.1576, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.3313, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.1848, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.4144, Accuracy: 0.8125\n",
      "Batch number: 067, Training: Loss: 0.2837, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.3000, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.4262, Accuracy: 0.8125\n",
      "Batch number: 070, Training: Loss: 0.1211, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.1420, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.2884, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.1500, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.2147, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.2503, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.1931, Accuracy: 0.8750\n",
      "Batch number: 077, Training: Loss: 0.1151, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.2530, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.1792, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.1418, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 0.2758, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0959, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.1742, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.1142, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.5104, Accuracy: 0.6875\n",
      "Batch number: 086, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.1431, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.4666, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.2508, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.2166, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.2091, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.3494, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.4725, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.3680, Accuracy: 0.8125\n",
      "Batch number: 096, Training: Loss: 0.1840, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.8649, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 0.2463, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.2192, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0831, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.3670, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.1311, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.0200, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.5432, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 0.2293, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.1048, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1275, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.1389, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.8226, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.3798, Accuracy: 0.8125\n",
      "Batch number: 111, Training: Loss: 0.1331, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.1265, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.1797, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.2861, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.2350, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.3312, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.2906, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.2371, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.0420, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0610, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0728, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.2808, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0655, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.2589, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.1950, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0848, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.1088, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.2451, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.5552, Accuracy: 0.8125\n",
      "Batch number: 130, Training: Loss: 0.3014, Accuracy: 0.8125\n",
      "Batch number: 131, Training: Loss: 0.2311, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0589, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1142, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1515, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1379, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0490, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0914, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0712, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.2752, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0951, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.4882, Accuracy: 0.8125\n",
      "Batch number: 143, Training: Loss: 0.3092, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.1574, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.2209, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1374, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.5670, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 0.1011, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.1607, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0803, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.2850, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.2469, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.6779, Accuracy: 0.6875\n",
      "Batch number: 154, Training: Loss: 0.1885, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.1617, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.3086, Accuracy: 0.8125\n",
      "Batch number: 157, Training: Loss: 0.0953, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.1132, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0547, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.1106, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.1328, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0962, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.4869, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.4968, Accuracy: 0.6875\n",
      "Batch number: 165, Training: Loss: 0.1213, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1150, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0342, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.4704, Accuracy: 0.7500\n",
      "Batch number: 169, Training: Loss: 0.1118, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.3298, Accuracy: 0.7500\n",
      "Batch number: 171, Training: Loss: 0.1886, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.0928, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.1937, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.2876, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.1652, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.5227, Accuracy: 0.7500\n",
      "Batch number: 177, Training: Loss: 0.1263, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.1065, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.1029, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2570, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.1672, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.1632, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.2712, Accuracy: 0.8125\n",
      "Batch number: 184, Training: Loss: 0.2146, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.1850, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1052, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.1615, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.1680, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.4099, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.1867, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.3386, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.1921, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.2456, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.0994, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 1.0088, Accuracy: 0.6875\n",
      "Batch number: 197, Training: Loss: 0.1281, Accuracy: 0.9375\n",
      "Epoch : 005, Training: Loss: 0.2095, Accuracy: 92.0455%, \n",
      "\t\tValidation : Loss : 0.1781, Accuracy: 94.6970%, Time: 36.1238s\n",
      "Epoch: 6/50\n",
      "Batch number: 000, Training: Loss: 0.3178, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 0.4967, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.2641, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.2386, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.4343, Accuracy: 0.8125\n",
      "Batch number: 005, Training: Loss: 0.1922, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.1188, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.5040, Accuracy: 0.8125\n",
      "Batch number: 009, Training: Loss: 0.5472, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 0.2718, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.0786, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.1101, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1378, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0721, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0579, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.1327, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.2876, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.1930, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.2204, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.3752, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.1565, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.2041, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.2253, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.1422, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.1030, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.1330, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.1138, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.4256, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.2784, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.4507, Accuracy: 0.6875\n",
      "Batch number: 031, Training: Loss: 0.2380, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 0.1855, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.3578, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.1238, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.1143, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0871, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.1325, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.1333, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.2800, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.2108, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.2362, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.3287, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.1614, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.1237, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.4302, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 0.0378, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.1664, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.3072, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.1710, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.2778, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.1336, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.1504, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.2563, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0179, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1959, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.0638, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.2520, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.2658, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.0400, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.0881, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0665, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.2617, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.2221, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.2904, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.1980, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.1759, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.1725, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.1407, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0921, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.2294, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 0.1019, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.1229, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.2242, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.2834, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.1191, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.3112, Accuracy: 0.8750\n",
      "Batch number: 077, Training: Loss: 0.1422, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.1850, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.4172, Accuracy: 0.7500\n",
      "Batch number: 080, Training: Loss: 0.1102, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.1229, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.2670, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.3273, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.2584, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0950, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.2551, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.2809, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.4615, Accuracy: 0.8125\n",
      "Batch number: 089, Training: Loss: 0.1351, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.4341, Accuracy: 0.8125\n",
      "Batch number: 091, Training: Loss: 0.1504, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.1716, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.1175, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.1017, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0994, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1713, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0597, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.3561, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.4961, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 0.3203, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.5032, Accuracy: 0.8125\n",
      "Batch number: 102, Training: Loss: 0.2249, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.2311, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.2710, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.3315, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.3622, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.0233, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.1092, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.6298, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.0873, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.3910, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.0615, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.3768, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.1366, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.2239, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0483, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0583, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1784, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.5298, Accuracy: 0.7500\n",
      "Batch number: 120, Training: Loss: 0.0354, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.1971, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.2570, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1240, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0893, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.2072, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.4946, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.0959, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0901, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.1661, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1553, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.1198, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.1682, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.1417, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.1510, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.3076, Accuracy: 0.8125\n",
      "Batch number: 136, Training: Loss: 0.0518, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.2000, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.3641, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 0.0369, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0455, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.3884, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 0.3183, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.1651, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0370, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.2179, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.1332, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.2246, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.2905, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.4456, Accuracy: 0.8125\n",
      "Batch number: 150, Training: Loss: 0.1721, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.1202, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1315, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.2899, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.3033, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.6197, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.2738, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.1314, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.1298, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.3040, Accuracy: 0.8125\n",
      "Batch number: 163, Training: Loss: 0.3124, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.2396, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.2786, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1210, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0937, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.2850, Accuracy: 0.8750\n",
      "Batch number: 169, Training: Loss: 0.3388, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.2024, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.2404, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.0891, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.2183, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0617, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.2822, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.6596, Accuracy: 0.6875\n",
      "Batch number: 177, Training: Loss: 0.1442, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.1271, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.2247, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.1634, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.3632, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.2197, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.5156, Accuracy: 0.7500\n",
      "Batch number: 184, Training: Loss: 0.1125, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.1027, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0284, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1798, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.2112, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.3071, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.3081, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.2106, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.1229, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.2554, Accuracy: 0.8125\n",
      "Batch number: 194, Training: Loss: 0.0841, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.1387, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.1732, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.5598, Accuracy: 0.8125\n",
      "Epoch : 006, Training: Loss: 0.2146, Accuracy: 91.6351%, \n",
      "\t\tValidation : Loss : 0.1657, Accuracy: 93.6869%, Time: 36.8168s\n",
      "Epoch: 7/50\n",
      "Batch number: 000, Training: Loss: 0.0993, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.1073, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0749, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.2191, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.0701, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.3493, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0488, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.4572, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.1264, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.0919, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.2169, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.1096, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.1496, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.2831, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.1149, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.1354, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.0875, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.1266, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.1608, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1591, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.2417, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 0.1541, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.3831, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 0.2081, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.1331, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1115, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.1144, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.2234, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0785, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.3125, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 0.2551, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.1686, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.0325, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0519, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1158, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.1993, Accuracy: 0.8125\n",
      "Batch number: 038, Training: Loss: 0.1325, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.1682, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.1807, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.2730, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.2038, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.3122, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0470, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.2074, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0435, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.3348, Accuracy: 0.7500\n",
      "Batch number: 050, Training: Loss: 0.1168, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0663, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.1265, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.3809, Accuracy: 0.8125\n",
      "Batch number: 054, Training: Loss: 0.5311, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0596, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.2991, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.5797, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.1055, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.2307, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0486, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.2489, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.3994, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.1005, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.4931, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.0202, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.1223, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.1085, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.3866, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 0.1959, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.1040, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.4332, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 0.0220, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.4054, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 0.2334, Accuracy: 0.8750\n",
      "Batch number: 077, Training: Loss: 0.0579, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0357, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.1413, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0984, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.4217, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.1397, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.2968, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.1137, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.2094, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.2487, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.0292, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.2166, Accuracy: 0.8125\n",
      "Batch number: 091, Training: Loss: 0.1226, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.3275, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 0.0974, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0950, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1523, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0558, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0156, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1170, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0812, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.2864, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.2646, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.2869, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.1180, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0540, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0773, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.3212, Accuracy: 0.7500\n",
      "Batch number: 107, Training: Loss: 0.2854, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.1903, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.2150, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.2081, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.3156, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.2369, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.0554, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.1174, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.3983, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 0.2245, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.3429, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.1359, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.1131, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.0514, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.1605, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1857, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0464, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0389, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.1089, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.1476, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0848, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1618, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.1524, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.2145, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.1290, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0959, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.3682, Accuracy: 0.8125\n",
      "Batch number: 137, Training: Loss: 0.0873, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1390, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.3092, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.2188, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0430, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1167, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.7024, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.2748, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.1489, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.0715, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.2168, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.3811, Accuracy: 0.7500\n",
      "Batch number: 149, Training: Loss: 0.1712, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.2781, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.1559, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.0325, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.3623, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.0387, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.4434, Accuracy: 0.8125\n",
      "Batch number: 156, Training: Loss: 1.1523, Accuracy: 0.5000\n",
      "Batch number: 157, Training: Loss: 0.1884, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.1099, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.1382, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.2099, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.1312, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.2157, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.3232, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.2596, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1817, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.2235, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.1636, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.4822, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.1436, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.5324, Accuracy: 0.8125\n",
      "Batch number: 171, Training: Loss: 0.1498, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.4777, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.1360, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.1555, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.2508, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.1323, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.0318, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.1285, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.1833, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.0349, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.2761, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.0415, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0482, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.3410, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.1094, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.2464, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.2726, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.5028, Accuracy: 0.7500\n",
      "Batch number: 189, Training: Loss: 0.0935, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.4106, Accuracy: 0.8125\n",
      "Batch number: 191, Training: Loss: 0.1485, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.1396, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.1025, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1659, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.4511, Accuracy: 0.7500\n",
      "Batch number: 196, Training: Loss: 0.0930, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2482, Accuracy: 0.8750\n",
      "Epoch : 007, Training: Loss: 0.1887, Accuracy: 92.5505%, \n",
      "\t\tValidation : Loss : 0.2262, Accuracy: 90.4040%, Time: 39.7024s\n",
      "Epoch: 8/50\n",
      "Batch number: 000, Training: Loss: 0.1156, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.4533, Accuracy: 0.8125\n",
      "Batch number: 003, Training: Loss: 0.2659, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.1205, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.1618, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.2617, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.2634, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.1108, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1745, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.1259, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.2620, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.5182, Accuracy: 0.8125\n",
      "Batch number: 013, Training: Loss: 0.0806, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0794, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.2517, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.0970, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0829, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.1119, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.0639, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.2569, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.0882, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.1761, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.1291, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.1800, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.2100, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.1365, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.1640, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.2462, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.1337, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0442, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.2525, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.2315, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0483, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.1033, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1653, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.1812, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.3537, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.3182, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 0.2031, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1989, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.3823, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.0881, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0171, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1940, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.0562, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.1578, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.0730, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.1058, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.1177, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.1227, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.4546, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.0321, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.0430, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0277, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0608, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.3167, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.5817, Accuracy: 0.8125\n",
      "Batch number: 061, Training: Loss: 0.2761, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.0761, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.2049, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.0762, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.4880, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 0.2542, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.4241, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 0.0163, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.0959, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.5525, Accuracy: 0.8125\n",
      "Batch number: 071, Training: Loss: 0.0479, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0874, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.2157, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.3414, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.2318, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.0333, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0670, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.2316, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 0.0325, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.2355, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 0.1466, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0996, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.4783, Accuracy: 0.7500\n",
      "Batch number: 085, Training: Loss: 0.2661, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.3690, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.1989, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.2406, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.2304, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.1745, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.2971, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.4538, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.1484, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.2034, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.3141, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.4228, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.1956, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 0.3082, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.2598, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.3724, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.1337, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.2316, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.1109, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.3738, Accuracy: 0.8125\n",
      "Batch number: 107, Training: Loss: 0.5973, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 0.0328, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.1771, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0232, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.2757, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.4642, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 0.2016, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.7011, Accuracy: 0.7500\n",
      "Batch number: 115, Training: Loss: 0.1131, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.1866, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.1298, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.1166, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.0617, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0607, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.2979, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.1469, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.2571, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.2960, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.2557, Accuracy: 0.8125\n",
      "Batch number: 126, Training: Loss: 0.7155, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 0.0625, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.2631, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0184, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.2021, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.1480, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0528, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.1726, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.0572, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.0885, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0817, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1601, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.2926, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.3734, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 0.0188, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.3982, Accuracy: 0.8125\n",
      "Batch number: 143, Training: Loss: 0.1174, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.2123, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1698, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.0990, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.0991, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.2311, Accuracy: 0.8125\n",
      "Batch number: 150, Training: Loss: 0.0863, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.0151, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.2617, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.1927, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.1950, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.4639, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.1472, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.1357, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.1485, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0682, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.7249, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.2433, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.0877, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.2351, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.3137, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.0316, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0661, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.1012, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.3757, Accuracy: 0.8125\n",
      "Batch number: 173, Training: Loss: 0.1887, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.2114, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.2801, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.2127, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.0911, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.3522, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.1453, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.1785, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.3086, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.3859, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.0905, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.3125, Accuracy: 0.8125\n",
      "Batch number: 185, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.2291, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.0599, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0901, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.2607, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.0341, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.2423, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.1046, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.1645, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1719, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0856, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.1977, Accuracy: 0.8750\n",
      "Batch number: 197, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Epoch : 008, Training: Loss: 0.1902, Accuracy: 93.0240%, \n",
      "\t\tValidation : Loss : 0.1450, Accuracy: 94.4444%, Time: 42.7097s\n",
      "Epoch: 9/50\n",
      "Batch number: 000, Training: Loss: 0.0765, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1380, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.1982, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0769, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.2981, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.0281, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.1285, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.1846, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.3420, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0575, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0627, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.2466, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.2128, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0180, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.3199, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.1721, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0473, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1387, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.3045, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.3970, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.0864, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.2201, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.1312, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.1325, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.0379, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.2237, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.1772, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.1793, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.0800, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.4453, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.2047, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.1243, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.2104, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0259, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.2738, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.2477, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.1042, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.1302, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0957, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.1190, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.2614, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0774, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.1221, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.1041, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.2802, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.1454, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.0336, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1400, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.1003, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.1673, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.4500, Accuracy: 0.8125\n",
      "Batch number: 059, Training: Loss: 0.3455, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.1090, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.1214, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0351, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.2135, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.1541, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.3084, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.1957, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.0263, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.3387, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.4202, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.1564, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.1106, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0236, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.0907, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.2389, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.1191, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.2201, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 0.1893, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.1698, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.2439, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0710, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.4459, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 0.0173, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0267, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.0639, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.2580, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.3401, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.0459, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.7061, Accuracy: 0.8125\n",
      "Batch number: 090, Training: Loss: 0.0760, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0096, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.2449, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0456, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1370, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.2136, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.3319, Accuracy: 0.8125\n",
      "Batch number: 099, Training: Loss: 0.1368, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1886, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0605, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.1705, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1856, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.2577, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.0424, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1100, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.4413, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0287, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.1086, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.0459, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.5748, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 0.2284, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.0649, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.2628, Accuracy: 0.8125\n",
      "Batch number: 116, Training: Loss: 0.0678, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.1433, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.2216, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1361, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.1476, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.3900, Accuracy: 0.8125\n",
      "Batch number: 122, Training: Loss: 0.1626, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1714, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.3285, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.0109, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.8140, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 0.1745, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.3067, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.2500, Accuracy: 0.8125\n",
      "Batch number: 130, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.2109, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.4678, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 0.1166, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.1406, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.4368, Accuracy: 0.7500\n",
      "Batch number: 136, Training: Loss: 0.3554, Accuracy: 0.8125\n",
      "Batch number: 137, Training: Loss: 0.0701, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0633, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.3554, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.1895, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0604, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0534, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0745, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0859, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.2731, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.1838, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.3571, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.2921, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 0.1839, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.1091, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.2434, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.0694, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.3320, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.0485, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.1215, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.4389, Accuracy: 0.8125\n",
      "Batch number: 157, Training: Loss: 0.0632, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0879, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0285, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0831, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0379, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0750, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.5024, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.1227, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.1358, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1783, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0132, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.4718, Accuracy: 0.8750\n",
      "Batch number: 169, Training: Loss: 0.2917, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.4066, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.0486, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.1483, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.2523, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.1774, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.2919, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.3091, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.1613, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.3078, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.1878, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.2018, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.1461, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1279, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.1864, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.1615, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.3685, Accuracy: 0.8125\n",
      "Batch number: 187, Training: Loss: 0.1780, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0836, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.1726, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.1797, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.4052, Accuracy: 0.8125\n",
      "Batch number: 192, Training: Loss: 0.0573, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0783, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.4722, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.1250, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0978, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.0952, Accuracy: 1.0000\n",
      "Epoch : 009, Training: Loss: 0.1771, Accuracy: 93.2134%, \n",
      "\t\tValidation : Loss : 0.1319, Accuracy: 95.7071%, Time: 44.8279s\n",
      "Epoch: 10/50\n",
      "Batch number: 000, Training: Loss: 0.0133, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0377, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.1384, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.2422, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.0911, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0369, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.2518, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.2085, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.1286, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.5130, Accuracy: 0.8125\n",
      "Batch number: 010, Training: Loss: 0.1517, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0280, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.1321, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.2708, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.2545, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.1175, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.1143, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.1125, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.3141, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.2948, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0214, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0732, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0797, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0740, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2274, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.1669, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0298, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0749, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0763, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0211, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.1748, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0690, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.2494, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.4102, Accuracy: 0.8125\n",
      "Batch number: 036, Training: Loss: 0.0702, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.3295, Accuracy: 0.8125\n",
      "Batch number: 038, Training: Loss: 0.5039, Accuracy: 0.8125\n",
      "Batch number: 039, Training: Loss: 0.0868, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.2093, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0694, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1747, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.1038, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.2883, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.1731, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.0716, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.3196, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.2747, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.4084, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0365, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.2214, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.0586, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0705, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.1191, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1330, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.0592, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0684, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0504, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.2873, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.1756, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.3223, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0320, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.2165, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.3549, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0772, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.1147, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.0259, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1345, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.3693, Accuracy: 0.8125\n",
      "Batch number: 071, Training: Loss: 0.0608, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.2289, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.4681, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.0430, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0831, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0864, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0671, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.2754, Accuracy: 0.8125\n",
      "Batch number: 079, Training: Loss: 0.1425, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0674, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0525, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.3142, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.2976, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.2324, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.0257, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.1686, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1054, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1946, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.1362, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.5293, Accuracy: 0.7500\n",
      "Batch number: 092, Training: Loss: 0.2634, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.1308, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.2295, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.2773, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 0.1633, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.2100, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.1782, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0897, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.4436, Accuracy: 0.8125\n",
      "Batch number: 103, Training: Loss: 0.0916, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.1463, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0458, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0313, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.1275, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0762, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0553, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1082, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.2583, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.1741, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.0844, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.1125, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.2371, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1218, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.1363, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.1827, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.1084, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0843, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.1115, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.3017, Accuracy: 0.8125\n",
      "Batch number: 126, Training: Loss: 0.1868, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0840, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.3375, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.1126, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1866, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0484, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.1528, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.2493, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.2256, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1211, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.1523, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0346, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.3036, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 0.3422, Accuracy: 0.8125\n",
      "Batch number: 142, Training: Loss: 0.0482, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.1609, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.1870, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0877, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0540, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.1241, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.1456, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.0934, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.3103, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.0248, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.1887, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.0784, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0672, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0930, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.1804, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.2798, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.4462, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.3884, Accuracy: 0.8125\n",
      "Batch number: 161, Training: Loss: 0.0776, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0151, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1111, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.1609, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.2367, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.1131, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.2374, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.1860, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.3075, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.4225, Accuracy: 0.8125\n",
      "Batch number: 172, Training: Loss: 0.2776, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.4642, Accuracy: 0.8125\n",
      "Batch number: 174, Training: Loss: 0.1709, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.1917, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.3019, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1631, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.4977, Accuracy: 0.8125\n",
      "Batch number: 179, Training: Loss: 0.2473, Accuracy: 0.8125\n",
      "Batch number: 180, Training: Loss: 0.0799, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0870, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.1876, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0630, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0708, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.4161, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.3011, Accuracy: 0.8125\n",
      "Batch number: 187, Training: Loss: 0.0550, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0936, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.2725, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.2308, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.0555, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.2860, Accuracy: 0.8125\n",
      "Batch number: 193, Training: Loss: 0.1267, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0272, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0248, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.2420, Accuracy: 0.8750\n",
      "Batch number: 197, Training: Loss: 0.1635, Accuracy: 0.9375\n",
      "Epoch : 010, Training: Loss: 0.1633, Accuracy: 93.9710%, \n",
      "\t\tValidation : Loss : 0.1292, Accuracy: 95.4545%, Time: 46.6412s\n",
      "Epoch: 11/50\n",
      "Batch number: 000, Training: Loss: 0.0563, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.3363, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.0621, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1965, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.1129, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.1293, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0986, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.2070, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 0.2257, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0285, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.3133, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.1435, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.0957, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.1472, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.1940, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.0712, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0478, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.2226, Accuracy: 0.8125\n",
      "Batch number: 019, Training: Loss: 0.0891, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.5071, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.9005, Accuracy: 0.6875\n",
      "Batch number: 022, Training: Loss: 0.2999, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.0710, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0828, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2083, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 0.0238, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1983, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.2311, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.2381, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.2656, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.1115, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1649, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.0943, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.1450, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.3379, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.0978, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0158, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0379, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.2402, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.2577, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0696, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0865, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.1962, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.1176, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.4121, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.0460, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.2499, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0531, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0425, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.2337, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.0851, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1199, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0633, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1599, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.2900, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.1848, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.3588, Accuracy: 0.7500\n",
      "Batch number: 059, Training: Loss: 0.2551, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.1508, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.1071, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0712, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.3274, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.4208, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.5816, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 0.0529, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.2939, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.0959, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.2815, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.1357, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.3101, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.2798, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.1345, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.3537, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0578, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.1845, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0376, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0752, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.1825, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.1184, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0426, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0989, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.3147, Accuracy: 0.7500\n",
      "Batch number: 086, Training: Loss: 0.3163, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.2717, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1868, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1123, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.7349, Accuracy: 0.7500\n",
      "Batch number: 091, Training: Loss: 0.1705, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.3910, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.1263, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.1856, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.2103, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1049, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.1465, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.2889, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.2138, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.0814, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.3988, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.3208, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 0.0703, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0678, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.2573, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.1209, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.1371, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0772, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.1476, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0728, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.1788, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.4224, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.1067, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0427, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.3520, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.0758, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.1375, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0681, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.3243, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.0917, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1084, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1518, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.3538, Accuracy: 0.8125\n",
      "Batch number: 126, Training: Loss: 0.1030, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.3294, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.1509, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.1216, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1745, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.1595, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.4085, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1025, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1123, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0598, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.1868, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.1186, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.3249, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 0.0797, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1697, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0104, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.1338, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.0272, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.1408, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.0677, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0324, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.1292, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.1861, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1506, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0253, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.1567, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.1348, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1001, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0866, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0227, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0388, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.1415, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0717, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1654, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.2891, Accuracy: 0.8125\n",
      "Batch number: 166, Training: Loss: 0.1690, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.4840, Accuracy: 0.8125\n",
      "Batch number: 168, Training: Loss: 0.0926, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.5460, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.1860, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.1076, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.1673, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.1900, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0773, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.0105, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0491, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.2620, Accuracy: 0.8125\n",
      "Batch number: 179, Training: Loss: 0.2582, Accuracy: 0.7500\n",
      "Batch number: 180, Training: Loss: 0.3952, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.0389, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0787, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1495, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.0927, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.2120, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.4521, Accuracy: 0.8125\n",
      "Batch number: 187, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0908, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.2166, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.5594, Accuracy: 0.7500\n",
      "Batch number: 191, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.7397, Accuracy: 0.8125\n",
      "Batch number: 193, Training: Loss: 0.2239, Accuracy: 0.8750\n",
      "Batch number: 194, Training: Loss: 0.3771, Accuracy: 0.8125\n",
      "Batch number: 195, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0638, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0632, Accuracy: 1.0000\n",
      "Epoch : 011, Training: Loss: 0.1732, Accuracy: 93.0556%, \n",
      "\t\tValidation : Loss : 0.1314, Accuracy: 95.7071%, Time: 44.5644s\n",
      "Epoch: 12/50\n",
      "Batch number: 000, Training: Loss: 0.0591, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1331, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0572, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0833, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.2149, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.0149, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.1517, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0693, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.1177, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0658, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.2942, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.0642, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1763, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.4491, Accuracy: 0.7500\n",
      "Batch number: 015, Training: Loss: 0.1164, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.1328, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.1871, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.2130, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.3793, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 0.1891, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.0866, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.2246, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.1518, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1633, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.1373, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.3241, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0705, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0886, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.3187, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.1961, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0515, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0621, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0649, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.4757, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.1181, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0693, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.1011, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.3873, Accuracy: 0.7500\n",
      "Batch number: 040, Training: Loss: 0.2662, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0227, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.0274, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0633, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.1250, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.3076, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0522, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.2379, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.3204, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.1129, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.1676, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.3819, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.1582, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.3435, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 0.2244, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.3660, Accuracy: 0.8125\n",
      "Batch number: 057, Training: Loss: 0.1359, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0534, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.3274, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.0931, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0492, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.0851, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.4437, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.1916, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.3526, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.3453, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.1624, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.1594, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.2471, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.3208, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.2206, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 0.2394, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.0585, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1872, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0180, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.3264, Accuracy: 0.7500\n",
      "Batch number: 078, Training: Loss: 0.1627, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0385, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0197, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0328, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.2665, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0376, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0249, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.2830, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.0551, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.3064, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.1419, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.4418, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.0512, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.1133, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0544, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0879, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.0870, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0646, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.0962, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0432, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0957, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0469, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0021, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.2024, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.0547, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.3132, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.3111, Accuracy: 0.8125\n",
      "Batch number: 107, Training: Loss: 0.3804, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.4632, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0773, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.2418, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.0784, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0835, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.2512, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.1436, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.2254, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.1129, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.2983, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.1225, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0435, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.1341, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.1235, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.3051, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.0985, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.1982, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.1523, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0490, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0279, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0646, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1224, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.2212, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0141, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0503, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.3691, Accuracy: 0.8125\n",
      "Batch number: 136, Training: Loss: 0.2842, Accuracy: 0.8125\n",
      "Batch number: 137, Training: Loss: 0.2214, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.0551, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.1011, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.2997, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0738, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0590, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.2387, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.0372, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.1936, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1120, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.1924, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1077, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0589, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.1739, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.2936, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.0920, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.0064, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0259, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0884, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1018, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.1888, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.1317, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.7640, Accuracy: 0.8125\n",
      "Batch number: 163, Training: Loss: 0.1848, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.0369, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.3028, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.1038, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.1077, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.4818, Accuracy: 0.8125\n",
      "Batch number: 169, Training: Loss: 0.0776, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.1727, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.2892, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.3946, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.2106, Accuracy: 0.8750\n",
      "Batch number: 174, Training: Loss: 0.0445, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0798, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.1592, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1643, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.0144, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.1648, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.1973, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.0820, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.2274, Accuracy: 0.8750\n",
      "Batch number: 184, Training: Loss: 0.1069, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.0887, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1217, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.1864, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.1808, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.1101, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.0406, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.1369, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0640, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.5199, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.1671, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.2545, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2406, Accuracy: 0.9375\n",
      "Epoch : 012, Training: Loss: 0.1578, Accuracy: 93.9394%, \n",
      "\t\tValidation : Loss : 0.0972, Accuracy: 97.4747%, Time: 44.4435s\n",
      "Epoch: 13/50\n",
      "Batch number: 000, Training: Loss: 0.1893, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.1373, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.2424, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.2331, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.2224, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.1246, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.1415, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.1369, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.1319, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.0944, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.0355, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0899, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.1224, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.2334, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.0779, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.2279, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.2547, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.1995, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.1924, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0466, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0566, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0339, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0922, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0941, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.2590, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.1052, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.4377, Accuracy: 0.6875\n",
      "Batch number: 033, Training: Loss: 0.2918, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.0031, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.1758, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.1046, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.1234, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.2617, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.0370, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.1175, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.2253, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0976, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0756, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.2181, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.0679, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.3983, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.0569, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.4871, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0245, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.1479, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.4777, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.3444, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 0.0665, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.1294, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.2045, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0279, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.3176, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.5123, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 0.1774, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0604, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.1310, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.2063, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.1151, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.2383, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0256, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0549, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1582, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.3601, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.0493, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.0171, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0737, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.1056, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.2973, Accuracy: 0.8125\n",
      "Batch number: 079, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.1146, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.1154, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.1648, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.3708, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.2680, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.3325, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.0514, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.1045, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.1050, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.1937, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.1184, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.2879, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.1710, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0645, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1596, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.4792, Accuracy: 0.8125\n",
      "Batch number: 099, Training: Loss: 0.1364, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.3046, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0743, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.3302, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.1342, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.2111, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.2068, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.3381, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 0.4118, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 0.0289, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0505, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.2452, Accuracy: 0.8125\n",
      "Batch number: 112, Training: Loss: 0.1305, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.4110, Accuracy: 0.8125\n",
      "Batch number: 114, Training: Loss: 0.1210, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.1527, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.3774, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.2722, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.0755, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1527, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.5693, Accuracy: 0.8125\n",
      "Batch number: 121, Training: Loss: 0.5151, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.4424, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.0478, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.2069, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.4637, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0684, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1496, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0594, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0513, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0568, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.4174, Accuracy: 0.8125\n",
      "Batch number: 135, Training: Loss: 0.7614, Accuracy: 0.8125\n",
      "Batch number: 136, Training: Loss: 0.0525, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.2033, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.2827, Accuracy: 0.8125\n",
      "Batch number: 140, Training: Loss: 0.4002, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.4132, Accuracy: 0.8125\n",
      "Batch number: 142, Training: Loss: 0.3409, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1474, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0493, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0835, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1229, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.1484, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.2867, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.1263, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.1717, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.0292, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0703, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.1410, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.2500, Accuracy: 0.8750\n",
      "Batch number: 156, Training: Loss: 0.0891, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1218, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.1765, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0405, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.1466, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.3040, Accuracy: 0.8750\n",
      "Batch number: 162, Training: Loss: 0.0773, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.1754, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.1793, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0657, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.1215, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0528, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.2074, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.3001, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.3610, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.0657, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.3464, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.1204, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.2568, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.3077, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.1236, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.1266, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.0446, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.1190, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0561, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0337, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0192, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0915, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.2524, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.3037, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.3736, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.0258, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.1525, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.1701, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.0608, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.2146, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0500, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.2466, Accuracy: 0.8750\n",
      "Epoch : 013, Training: Loss: 0.1634, Accuracy: 93.5606%, \n",
      "\t\tValidation : Loss : 0.1018, Accuracy: 96.9697%, Time: 44.9091s\n",
      "Epoch: 14/50\n",
      "Batch number: 000, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.2833, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.1590, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.1246, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.3714, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 0.2184, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.1975, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.1922, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0716, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.1123, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.1285, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.1128, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.1455, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.6592, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.3179, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.3810, Accuracy: 0.8125\n",
      "Batch number: 018, Training: Loss: 0.0549, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.3258, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.1333, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.0640, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.0890, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.2354, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.6233, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 0.2033, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.1231, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0065, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.2642, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.3150, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.5431, Accuracy: 0.7500\n",
      "Batch number: 032, Training: Loss: 0.0286, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0863, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.2375, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.0534, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.2075, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.4120, Accuracy: 0.7500\n",
      "Batch number: 039, Training: Loss: 0.5136, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.2927, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.2822, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.1176, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0675, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.2406, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.3662, Accuracy: 0.7500\n",
      "Batch number: 048, Training: Loss: 0.0350, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0393, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0584, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.1467, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.1300, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1521, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.0193, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.2581, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.1736, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1573, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.2378, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.1984, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0593, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.4541, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.1964, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.0486, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0234, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0458, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0776, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1592, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.1211, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.2374, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.0577, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1967, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.2031, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.5476, Accuracy: 0.7500\n",
      "Batch number: 078, Training: Loss: 0.3718, Accuracy: 0.8125\n",
      "Batch number: 079, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0287, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.1195, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.2580, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 0.0238, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.0122, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.3610, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.0183, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.2856, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.1946, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.4671, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 0.1144, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0786, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0427, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.0614, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.2866, Accuracy: 0.7500\n",
      "Batch number: 097, Training: Loss: 0.0834, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1547, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.1222, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.3063, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.0507, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0210, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0917, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.6385, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.1248, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.2486, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 0.4325, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.1403, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.1675, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.2292, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.3302, Accuracy: 0.8125\n",
      "Batch number: 115, Training: Loss: 0.1406, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0727, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.0879, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1760, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.1778, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.3028, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.0525, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.2960, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1277, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1411, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.0770, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.3650, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0401, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0331, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0869, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1392, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.1101, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.1852, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0011, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.3762, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0228, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.1292, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.1721, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.2393, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.2163, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.2931, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 0.1444, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.2633, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0719, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.1358, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.2219, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.0412, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0933, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0415, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.3187, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 0.1958, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1593, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.4849, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.1755, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.3960, Accuracy: 0.7500\n",
      "Batch number: 156, Training: Loss: 0.2755, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.2821, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.5157, Accuracy: 0.8125\n",
      "Batch number: 159, Training: Loss: 0.1749, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.1612, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0756, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0874, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.1576, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.2850, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1009, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.2800, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.3117, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.0814, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.1908, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0454, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0388, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0907, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0120, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.1196, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.1175, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.0902, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.0712, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0292, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.2517, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.1625, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.2490, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.6207, Accuracy: 0.8125\n",
      "Batch number: 183, Training: Loss: 0.0960, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.4231, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.3030, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.0756, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.0975, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.1277, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.0119, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1631, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0803, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.1065, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.2305, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.2655, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.0550, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Epoch : 014, Training: Loss: 0.1692, Accuracy: 93.1503%, \n",
      "\t\tValidation : Loss : 0.1263, Accuracy: 95.7071%, Time: 43.6093s\n",
      "Epoch: 15/50\n",
      "Batch number: 000, Training: Loss: 0.0837, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1401, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.2653, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.0258, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0955, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.0262, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.2408, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.0574, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0667, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.1897, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.0631, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1741, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 0.0266, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.3536, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 0.1037, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.2289, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.1399, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0853, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0104, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1532, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.3157, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.0349, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1735, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.1246, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.1486, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.2349, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0257, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.4740, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 0.0586, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0677, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0636, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0590, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.1062, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.3037, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.1719, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0274, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.0821, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.1766, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.4995, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.0735, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.2354, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.1632, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.1533, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.0437, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0979, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0076, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0409, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.1453, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.2379, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.2461, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.0579, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.1909, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.0837, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.2240, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.2397, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.1404, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.4908, Accuracy: 0.8125\n",
      "Batch number: 063, Training: Loss: 0.0733, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.2559, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.0886, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.1750, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.4776, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 0.1844, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.3048, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.0768, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.2108, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.2707, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.2409, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.1683, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0953, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.2298, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.2883, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.2647, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.2792, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 0.3451, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.3353, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.0583, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.1442, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.2624, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.2342, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.0611, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.1036, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0445, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0396, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0061, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.1140, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0553, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1304, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.2056, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.2145, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.1715, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.2608, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.1364, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0191, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1318, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.2731, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 0.2028, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.2304, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.0042, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0641, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0774, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1124, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.2233, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.2331, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.5165, Accuracy: 0.8125\n",
      "Batch number: 115, Training: Loss: 0.0457, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.1130, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.3023, Accuracy: 0.8125\n",
      "Batch number: 118, Training: Loss: 0.1239, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.0782, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.3096, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.4209, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.4273, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.3030, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.2135, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.2256, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.3007, Accuracy: 0.8125\n",
      "Batch number: 130, Training: Loss: 0.0935, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1689, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.6531, Accuracy: 0.7500\n",
      "Batch number: 133, Training: Loss: 0.0966, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.2081, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1226, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.2410, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 0.2233, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0873, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.1321, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.2131, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.2537, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.1682, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.2184, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.2930, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1056, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.0237, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0544, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.0337, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.1838, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.0415, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.2546, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.1350, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0272, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0521, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0921, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0946, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0630, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1196, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.2477, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.0729, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.1583, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.3098, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.0941, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.3234, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1736, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.1203, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.1086, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.3270, Accuracy: 0.8125\n",
      "Batch number: 171, Training: Loss: 0.4561, Accuracy: 0.8125\n",
      "Batch number: 172, Training: Loss: 0.0589, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.2567, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.1280, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.0661, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0369, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0228, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0104, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2430, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0613, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.5795, Accuracy: 0.8125\n",
      "Batch number: 183, Training: Loss: 0.0139, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.1455, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0690, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.1570, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.2401, Accuracy: 0.8125\n",
      "Batch number: 189, Training: Loss: 0.0612, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0889, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.2522, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0409, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.0291, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.1304, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.0351, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0439, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.2328, Accuracy: 0.9375\n",
      "Epoch : 015, Training: Loss: 0.1541, Accuracy: 93.7816%, \n",
      "\t\tValidation : Loss : 0.1039, Accuracy: 96.4646%, Time: 45.0591s\n",
      "Epoch: 16/50\n",
      "Batch number: 000, Training: Loss: 0.1138, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.1269, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0561, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1657, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.0761, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1252, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.1718, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.1190, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.1232, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.0803, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.0738, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.0384, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.2841, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.7902, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 0.0784, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0402, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.2031, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.2394, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.0583, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.0132, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0521, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1289, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.1147, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0348, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0754, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.3607, Accuracy: 0.8125\n",
      "Batch number: 026, Training: Loss: 0.0467, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0946, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.1995, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.3277, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 0.0757, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.2028, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.7062, Accuracy: 0.8125\n",
      "Batch number: 036, Training: Loss: 0.1725, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.3092, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0616, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.1337, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0219, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.3999, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.1001, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.1533, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.3664, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0967, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0706, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.2261, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.5755, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.0609, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0321, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0930, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1170, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.2485, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.1306, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.0708, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.2471, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.1239, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0583, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.1602, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0104, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.4610, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.2402, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0208, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.1041, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0697, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0325, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1000, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0798, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0220, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.1886, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.6504, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.0099, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.1663, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 0.1343, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0809, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.2302, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.0968, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0280, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0099, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.2448, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.0719, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.2076, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.1663, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.3119, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 0.2869, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0284, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1760, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.0185, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.3170, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0365, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0744, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.3097, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0288, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1672, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0622, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0230, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0230, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.0648, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.3814, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.1276, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.0876, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.1508, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0163, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.2735, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0619, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.2244, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.3068, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 0.0418, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.3345, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.1367, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1221, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.0961, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.2747, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.0489, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.1274, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0420, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0350, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.5036, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.2049, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0355, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1230, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.0557, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0745, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.4621, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.3318, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.0437, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.2068, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0427, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.1897, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.2281, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.3174, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0643, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.2126, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.2977, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.0754, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.1841, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1707, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.2876, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.1801, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.1113, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.0234, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1515, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.1269, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.2183, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.4886, Accuracy: 0.8125\n",
      "Batch number: 161, Training: Loss: 0.1306, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.2482, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.0119, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.3264, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.0489, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0432, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0769, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.2913, Accuracy: 0.8125\n",
      "Batch number: 169, Training: Loss: 0.1286, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.3799, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.2998, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.1127, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.1475, Accuracy: 0.8750\n",
      "Batch number: 174, Training: Loss: 0.0369, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0615, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0700, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.3022, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.2683, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.1460, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.3031, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.1346, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0723, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0636, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.3691, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.0320, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0443, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0557, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.1225, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0847, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0788, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.1232, Accuracy: 0.8750\n",
      "Batch number: 194, Training: Loss: 0.3777, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.1961, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.1492, Accuracy: 0.9375\n",
      "Epoch : 016, Training: Loss: 0.1471, Accuracy: 94.6338%, \n",
      "\t\tValidation : Loss : 0.1483, Accuracy: 95.4545%, Time: 78.8938s\n",
      "Epoch: 17/50\n",
      "Batch number: 000, Training: Loss: 0.3140, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 0.0844, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.1228, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.1409, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.2504, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.1701, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0210, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.1469, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.1200, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.1433, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.0162, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0273, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0124, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1120, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.5270, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.2426, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.2329, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.4084, Accuracy: 0.8125\n",
      "Batch number: 018, Training: Loss: 0.1900, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.2682, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.5662, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 0.0739, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.1210, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0137, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0702, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.0560, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0083, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0319, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0301, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0404, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.2429, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.2193, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.0253, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.1087, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0442, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.1045, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.1293, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.1235, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1771, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.3386, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.4100, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.1197, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.3348, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.0547, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0411, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.2796, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 0.1521, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.1295, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0421, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.1033, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.0700, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.1922, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.4536, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 0.2637, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.4020, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.2827, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.1641, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.1843, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.0543, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.2823, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0978, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.0725, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0707, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0758, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.1123, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0016, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.2955, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.3473, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0281, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0181, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.1325, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0445, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.2398, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.3796, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 0.3930, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.4227, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.3218, Accuracy: 0.8125\n",
      "Batch number: 087, Training: Loss: 0.1454, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1643, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0241, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.1252, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0872, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.3512, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 0.0465, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.2830, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.0896, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0883, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.2377, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1129, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.1511, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1695, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.1860, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.1248, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.0803, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.2024, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.1960, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0618, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1725, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.0916, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0400, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.1183, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.0995, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.1127, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.2930, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.0556, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.2195, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.6893, Accuracy: 0.7500\n",
      "Batch number: 117, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.1840, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.1302, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.1855, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.3229, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.3551, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.2328, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0714, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0543, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0795, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0211, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.4145, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0598, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0632, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.2446, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.1602, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.1491, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1967, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 0.0692, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1346, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.1008, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.3787, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.4987, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 0.0552, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.2729, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.0757, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.2734, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1330, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.0202, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0211, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.3096, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.3665, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.0162, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.7032, Accuracy: 0.8125\n",
      "Batch number: 153, Training: Loss: 0.3682, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0592, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0641, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.3698, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.3918, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.2832, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.2187, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.1504, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.1198, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.0570, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0760, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.2352, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.2202, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.3641, Accuracy: 0.8125\n",
      "Batch number: 170, Training: Loss: 0.1356, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.2805, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.0956, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.1719, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.2459, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.0310, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.1552, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1801, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.1426, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.2355, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.0630, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0516, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.0173, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1524, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.2659, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.0319, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0954, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1661, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.1107, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0544, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0717, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1571, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.1962, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.2452, Accuracy: 0.8750\n",
      "Batch number: 194, Training: Loss: 0.1217, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.2143, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.1597, Accuracy: 0.8750\n",
      "Epoch : 017, Training: Loss: 0.1604, Accuracy: 94.1604%, \n",
      "\t\tValidation : Loss : 0.1371, Accuracy: 95.9596%, Time: 33.9460s\n",
      "Epoch: 18/50\n",
      "Batch number: 000, Training: Loss: 0.0534, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1284, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.1688, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0227, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0278, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.5475, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 0.1154, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.2987, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.0581, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0133, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.2359, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.0346, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1100, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.2334, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.1012, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.1307, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.1949, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.6105, Accuracy: 0.7500\n",
      "Batch number: 020, Training: Loss: 0.0984, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.2843, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.2200, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.5216, Accuracy: 0.8125\n",
      "Batch number: 024, Training: Loss: 0.0535, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.1193, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0663, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0718, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0726, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.1575, Accuracy: 0.8125\n",
      "Batch number: 030, Training: Loss: 0.1158, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0689, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1181, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.2964, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.1678, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.2915, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.0262, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.1870, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.0354, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0680, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0068, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.4795, Accuracy: 0.8125\n",
      "Batch number: 042, Training: Loss: 0.1871, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0550, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0649, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.2332, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0494, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.2096, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.1345, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.0126, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0974, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.1826, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.2051, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.0751, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.1296, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0727, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0468, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.3751, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.2413, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.2514, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0988, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.0408, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.2735, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0969, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.1044, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.2447, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0299, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0586, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.3540, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.1867, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0169, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.0746, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.1319, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.1799, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.3085, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0661, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.2524, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 0.3359, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.2943, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.5621, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.0887, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0553, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.2342, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.1139, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.2747, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.2046, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.0898, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.6434, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.1825, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.2356, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0993, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0122, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0912, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.1914, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.0448, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0644, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.1673, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.3337, Accuracy: 0.8125\n",
      "Batch number: 107, Training: Loss: 0.1753, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0732, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0209, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.1803, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.0509, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.2818, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.0625, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.2194, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1497, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.2277, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.1755, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.1634, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.1042, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0196, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.6847, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.2074, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0178, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.2355, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.1388, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0454, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0432, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1182, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0119, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.2089, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.2260, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0679, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.5074, Accuracy: 0.7500\n",
      "Batch number: 137, Training: Loss: 0.0761, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0335, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0191, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0236, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.3251, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 0.0557, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0566, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0872, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.1226, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1683, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.2265, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.0569, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1242, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0618, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.1118, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.0944, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.1101, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.2156, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0097, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.1602, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.1361, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0545, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.4657, Accuracy: 0.8125\n",
      "Batch number: 160, Training: Loss: 0.0340, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.0917, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0021, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0649, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.2035, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.4254, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.1448, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.2122, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0777, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0529, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0644, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.0781, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0346, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0399, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1057, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.0116, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.1680, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.2544, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.1959, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.3928, Accuracy: 0.8125\n",
      "Batch number: 184, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.3416, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.0715, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.2314, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.3070, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0941, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0923, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.0200, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.2678, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1650, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.1503, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.0761, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Epoch : 018, Training: Loss: 0.1432, Accuracy: 94.6970%, \n",
      "\t\tValidation : Loss : 0.1055, Accuracy: 96.7172%, Time: 935.9084s\n",
      "Epoch: 19/50\n",
      "Batch number: 000, Training: Loss: 0.2288, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0713, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0133, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1604, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 0.1594, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.1974, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0839, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.0925, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0121, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0372, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.2630, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.0936, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0705, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.2484, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.1072, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.1581, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.0390, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.1355, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.0516, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.4473, Accuracy: 0.8125\n",
      "Batch number: 024, Training: Loss: 0.0730, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.2268, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.0642, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.2759, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.4102, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.0093, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.1846, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 0.5796, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.5589, Accuracy: 0.7500\n",
      "Batch number: 034, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0118, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.2994, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.3932, Accuracy: 0.8125\n",
      "Batch number: 039, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0747, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0026, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.0309, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0318, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0638, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.2368, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.3120, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.2568, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.0632, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0852, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.1143, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.0408, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0760, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.4282, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.3373, Accuracy: 0.8125\n",
      "Batch number: 057, Training: Loss: 0.0609, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.2904, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.7080, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.1798, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.1260, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0372, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0021, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.2888, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.0683, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0171, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.1655, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.2280, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0357, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.1816, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0373, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.1261, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.9618, Accuracy: 0.7500\n",
      "Batch number: 075, Training: Loss: 0.5828, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 0.2331, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.3992, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0116, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.3052, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 0.3154, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.1568, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.8457, Accuracy: 0.7500\n",
      "Batch number: 084, Training: Loss: 0.8701, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.2124, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.4643, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.0682, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.1171, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.2634, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.2206, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.2931, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 0.1447, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0482, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.0790, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1296, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0301, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.4970, Accuracy: 0.8125\n",
      "Batch number: 099, Training: Loss: 0.0399, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0567, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0485, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.4702, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.0788, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.4620, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0081, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0118, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.2978, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.0493, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0487, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.2557, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.0630, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.1504, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.3956, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0399, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.1186, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0846, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.1416, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.1319, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.3256, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.0975, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.3187, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.3737, Accuracy: 0.8125\n",
      "Batch number: 123, Training: Loss: 0.1002, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.0598, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.3628, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.0751, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.2687, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.1346, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.0275, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0757, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0777, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0321, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0443, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.0254, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1131, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0394, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0017, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.1881, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0600, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.0426, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0663, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.1455, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.2448, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1177, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0164, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.3916, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.0478, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0261, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0283, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0815, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1993, Accuracy: 0.8750\n",
      "Batch number: 160, Training: Loss: 0.1843, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.2591, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.0626, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0582, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.2229, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.2591, Accuracy: 0.8125\n",
      "Batch number: 167, Training: Loss: 0.5243, Accuracy: 0.8125\n",
      "Batch number: 168, Training: Loss: 0.3763, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0635, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.1475, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.0448, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2049, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.6067, Accuracy: 0.7500\n",
      "Batch number: 174, Training: Loss: 0.0658, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0826, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.4035, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.2614, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.1298, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.1414, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.2465, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.1785, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.1033, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.3579, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.2896, Accuracy: 0.8125\n",
      "Batch number: 186, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.4320, Accuracy: 0.7500\n",
      "Batch number: 188, Training: Loss: 0.2778, Accuracy: 0.8125\n",
      "Batch number: 189, Training: Loss: 0.0333, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0833, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0226, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.0748, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.1529, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.4752, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.0869, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2971, Accuracy: 0.9375\n",
      "Epoch : 019, Training: Loss: 0.1650, Accuracy: 93.7184%, \n",
      "\t\tValidation : Loss : 0.1052, Accuracy: 97.2222%, Time: 32.9962s\n",
      "Epoch: 20/50\n",
      "Batch number: 000, Training: Loss: 0.1394, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.1363, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0152, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1663, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.2169, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.1084, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0426, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.2128, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.2685, Accuracy: 0.8125\n",
      "Batch number: 009, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.5959, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.0585, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.2990, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.1956, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0912, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1585, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.5608, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.1314, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0792, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.1764, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.5660, Accuracy: 0.7500\n",
      "Batch number: 022, Training: Loss: 0.3893, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.1184, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0249, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2898, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.3993, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0760, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.1085, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.2258, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.0550, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0947, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1417, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.2368, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.2525, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.1158, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0635, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.2527, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.0771, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.1394, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0170, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.0563, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0626, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0653, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.1319, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.1735, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.0994, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0354, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1157, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.1074, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.1472, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.3672, Accuracy: 0.8125\n",
      "Batch number: 053, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0430, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.3550, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.0304, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.1738, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.0669, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.2604, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.0686, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0624, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.2113, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.3203, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.1598, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.7293, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 0.1976, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.0618, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0806, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0815, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.3672, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0733, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.1493, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.0662, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.1016, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.1429, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.5260, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 0.0083, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.1441, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.1708, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0537, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.0168, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0811, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.1888, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0278, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.3072, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.0156, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.1213, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0867, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.3842, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.3411, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.2059, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.2137, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.1425, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.0674, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.3293, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.1542, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.0724, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0126, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0800, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.0685, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.2399, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.1372, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.4718, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.0567, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0698, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.1734, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.0313, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0742, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0149, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.2194, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1593, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.1422, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.0202, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.1190, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.0426, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0017, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.4354, Accuracy: 0.8125\n",
      "Batch number: 126, Training: Loss: 0.2484, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.2281, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.1096, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.1914, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.0967, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.1271, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.1238, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.2724, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.1822, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0982, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0816, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.1491, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.2000, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.4284, Accuracy: 0.8125\n",
      "Batch number: 142, Training: Loss: 0.2973, Accuracy: 0.8125\n",
      "Batch number: 143, Training: Loss: 0.0430, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.1391, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.1614, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.0840, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.1163, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.2237, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0573, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.1331, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.0198, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.0341, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.1854, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.7096, Accuracy: 0.7500\n",
      "Batch number: 158, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0741, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.0776, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.2043, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.1779, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0465, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.2930, Accuracy: 0.8125\n",
      "Batch number: 166, Training: Loss: 0.1578, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0766, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0190, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0716, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0717, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.3056, Accuracy: 0.8125\n",
      "Batch number: 173, Training: Loss: 0.4488, Accuracy: 0.8125\n",
      "Batch number: 174, Training: Loss: 0.1000, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0308, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0744, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0467, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0224, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0592, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.2210, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.3449, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.2757, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.2511, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.3751, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1419, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.0526, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.2122, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0973, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.0132, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.0774, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0479, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0744, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.1720, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.6835, Accuracy: 0.7500\n",
      "Epoch : 020, Training: Loss: 0.1477, Accuracy: 94.2551%, \n",
      "\t\tValidation : Loss : 0.0904, Accuracy: 97.7273%, Time: 33.3220s\n",
      "Epoch: 21/50\n",
      "Batch number: 000, Training: Loss: 0.2681, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 0.0106, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.1954, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0650, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0176, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.1363, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0589, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0731, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.3376, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.0370, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0144, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0809, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1526, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.1085, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.1584, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.0639, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.1149, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1371, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0147, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.2209, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0021, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0155, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0521, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1107, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.1502, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0556, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.2171, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0874, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0217, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0610, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0435, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.1499, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.2326, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.4718, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 0.0862, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.1074, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0249, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.1639, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.1145, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0700, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.1389, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0393, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0308, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.2535, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0883, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.3403, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.6173, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.3198, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.1523, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.1056, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.4786, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.1909, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.1833, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0357, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.2119, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 0.0192, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.2061, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.2647, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.0738, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0637, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.2245, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.0963, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0846, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0760, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.1552, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.2952, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.1067, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0531, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.4681, Accuracy: 0.8125\n",
      "Batch number: 079, Training: Loss: 0.2692, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.1279, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0458, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0929, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0483, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0642, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.2222, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.2774, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.2916, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.2245, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0874, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.1181, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.0660, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.1365, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1953, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0784, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.2478, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1653, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.4548, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0386, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0699, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0519, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.3344, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 0.2430, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0282, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1830, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.1288, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.1489, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.1670, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.1761, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.9471, Accuracy: 0.8125\n",
      "Batch number: 113, Training: Loss: 0.2086, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.0676, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0222, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0063, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0251, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0853, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.2109, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.0748, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.0974, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.2502, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.4026, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.0828, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0373, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0824, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.4594, Accuracy: 0.7500\n",
      "Batch number: 128, Training: Loss: 0.1428, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0959, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1620, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.1271, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.2870, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0552, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.4079, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0848, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1389, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.4301, Accuracy: 0.8125\n",
      "Batch number: 140, Training: Loss: 0.3073, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.1892, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 0.0876, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0071, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.3547, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.2323, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.1102, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.1632, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.0710, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.2522, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.1891, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.1201, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.1090, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0650, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0950, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.1784, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.7432, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.0383, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.5614, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.1233, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.3331, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.1567, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0287, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.2146, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.2820, Accuracy: 0.8750\n",
      "Batch number: 169, Training: Loss: 0.2788, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0183, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.1207, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2144, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.1422, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.2212, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0609, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.2022, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0940, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.1395, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.3341, Accuracy: 0.8125\n",
      "Batch number: 181, Training: Loss: 0.1635, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.2029, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.1737, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.2860, Accuracy: 0.8125\n",
      "Batch number: 186, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1909, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.0577, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.1240, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.2984, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.2052, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0858, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0265, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0634, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.1840, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.3328, Accuracy: 0.8750\n",
      "Epoch : 021, Training: Loss: 0.1477, Accuracy: 94.4444%, \n",
      "\t\tValidation : Loss : 0.1945, Accuracy: 93.1818%, Time: 34.8083s\n",
      "Epoch: 22/50\n",
      "Batch number: 000, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0258, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0324, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1694, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.2325, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.3758, Accuracy: 0.8125\n",
      "Batch number: 007, Training: Loss: 0.1595, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.3687, Accuracy: 0.8125\n",
      "Batch number: 009, Training: Loss: 0.3191, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.1974, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.4260, Accuracy: 0.8125\n",
      "Batch number: 012, Training: Loss: 0.0897, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.2182, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.2528, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.0896, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.2342, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.0623, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.3355, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.1789, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1003, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1065, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0401, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0597, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1537, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.3287, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.1051, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0747, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0745, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.0803, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.1982, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0508, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0259, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.2778, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.1629, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.4276, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.8464, Accuracy: 0.7500\n",
      "Batch number: 041, Training: Loss: 0.0891, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0546, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.2115, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.2248, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.1107, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.0114, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.1693, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0546, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.4082, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.2153, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.3184, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.0891, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.3097, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.1728, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.0049, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.3005, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.6394, Accuracy: 0.8125\n",
      "Batch number: 059, Training: Loss: 0.0562, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.0532, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0650, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.1516, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0965, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.1928, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.1898, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.1415, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.0918, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.0359, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.3789, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.0845, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.0914, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0761, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.2579, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.2805, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.3563, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 0.0855, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0492, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.6950, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.5192, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 0.2786, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0766, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0697, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.4735, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0998, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0119, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.4522, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.1718, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.1820, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.1263, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1097, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.0720, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1023, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.1564, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.1370, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.3039, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.0904, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0328, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0862, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.3893, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.3859, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.2505, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.1684, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.1374, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0997, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.0415, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0650, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.2714, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.2181, Accuracy: 0.8125\n",
      "Batch number: 119, Training: Loss: 0.2271, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0530, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0099, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.1592, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.2579, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1944, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0625, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.1001, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.2149, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0260, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.2551, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0911, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0890, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0576, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1426, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.0164, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.1867, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0469, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0875, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.2563, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0532, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0432, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.2042, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0462, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.0122, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.3140, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.0600, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0702, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.3663, Accuracy: 0.8750\n",
      "Batch number: 156, Training: Loss: 0.1272, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1160, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.1625, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.3350, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0875, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.0765, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0386, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0897, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.4057, Accuracy: 0.8125\n",
      "Batch number: 167, Training: Loss: 0.0242, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0491, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.2818, Accuracy: 0.8125\n",
      "Batch number: 171, Training: Loss: 0.5189, Accuracy: 0.8125\n",
      "Batch number: 172, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.0541, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.1793, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.2026, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.1739, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.1211, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.0231, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0865, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0054, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1285, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.0648, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.1279, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0980, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.3862, Accuracy: 0.8125\n",
      "Batch number: 188, Training: Loss: 0.1793, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0988, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.2668, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.2316, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.3127, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.1523, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.3482, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.1007, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.0800, Accuracy: 0.9375\n",
      "Epoch : 022, Training: Loss: 0.1493, Accuracy: 94.2866%, \n",
      "\t\tValidation : Loss : 0.0875, Accuracy: 97.4747%, Time: 35.1366s\n",
      "Epoch: 23/50\n",
      "Batch number: 000, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1212, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.0260, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0225, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0512, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1197, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.1264, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0546, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0303, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1482, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.1007, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.0180, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.2974, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.1472, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 0.1910, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.0139, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0832, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.2262, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.1058, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0395, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0208, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.1301, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1097, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0270, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.1571, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0036, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.1486, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.3002, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.0565, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0969, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0763, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.1669, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.2447, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.2349, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.2639, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.1153, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1621, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.1078, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.1420, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0471, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0996, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.1287, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.2340, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.0177, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.2145, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.0831, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0127, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.1151, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.3339, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.1810, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0549, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0031, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.4580, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.1871, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0708, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.6993, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.2812, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.2461, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 0.3389, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.0251, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.2306, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0421, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1472, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.1535, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0681, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.2696, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.1269, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.2028, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0696, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0625, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0232, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.2308, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0875, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.0282, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.0653, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.4220, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.0618, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.2297, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0236, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.0569, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.1567, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.0225, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.3603, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.0823, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0885, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0283, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0105, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.2800, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.1471, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.4262, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.2638, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0415, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.2678, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0879, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.2010, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.0861, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.2686, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.4985, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0582, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.1201, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.4617, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 0.1522, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.0135, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0933, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0032, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.1411, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0553, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0612, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.2869, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.1146, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.1294, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0929, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.2549, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0265, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.1846, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0792, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1664, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0862, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.0299, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1438, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.0215, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.3705, Accuracy: 0.8125\n",
      "Batch number: 140, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1326, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1699, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.1394, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0099, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.2082, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0476, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1121, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.2019, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.0230, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.1202, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.0222, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.1548, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1475, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.1566, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.1345, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.1861, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.3529, Accuracy: 0.7500\n",
      "Batch number: 162, Training: Loss: 0.0466, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.2038, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.3571, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1657, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1698, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.1136, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0704, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.4338, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.6073, Accuracy: 0.8125\n",
      "Batch number: 172, Training: Loss: 0.1161, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0676, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.0831, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0241, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0514, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0997, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.0884, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.1696, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0648, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.2909, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.1796, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.5687, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0429, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.2605, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.1696, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1073, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.1119, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.2175, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1769, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0703, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.4702, Accuracy: 0.8125\n",
      "Epoch : 023, Training: Loss: 0.1305, Accuracy: 94.8232%, \n",
      "\t\tValidation : Loss : 0.1398, Accuracy: 94.9495%, Time: 36.0161s\n",
      "Epoch: 24/50\n",
      "Batch number: 000, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.2548, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0439, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1779, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.2204, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.0976, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.3291, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.0632, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.2399, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.3634, Accuracy: 0.8125\n",
      "Batch number: 011, Training: Loss: 0.2372, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.1198, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.5359, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 0.0848, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.1036, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.1975, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.0032, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.1434, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.1058, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.0550, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.1272, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.1723, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.3607, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.0551, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.0467, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1104, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0427, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0508, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.1354, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0854, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.2330, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.4455, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.0921, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.3815, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.0605, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0280, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.1339, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0397, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0840, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.1085, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.2364, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.4148, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.0745, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.4321, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.3589, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 0.0161, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0895, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.6189, Accuracy: 0.7500\n",
      "Batch number: 051, Training: Loss: 0.0268, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0642, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.2068, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0309, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.0126, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0721, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.1428, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.0246, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.3961, Accuracy: 0.8125\n",
      "Batch number: 061, Training: Loss: 0.0320, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.1384, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.1637, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.0318, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.7390, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.0548, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0110, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.1559, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.1843, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.1439, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.2646, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0457, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.2915, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.1034, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0865, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0981, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.6626, Accuracy: 0.7500\n",
      "Batch number: 079, Training: Loss: 0.1545, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.2475, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0465, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.2996, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0806, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.1077, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0281, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0286, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.2007, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.2783, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.2345, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.8443, Accuracy: 0.7500\n",
      "Batch number: 091, Training: Loss: 0.1366, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.0779, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.2314, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.1222, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.4331, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.1797, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.4704, Accuracy: 0.8125\n",
      "Batch number: 098, Training: Loss: 0.1531, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0777, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.2565, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.1705, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.8000, Accuracy: 0.7500\n",
      "Batch number: 104, Training: Loss: 0.1716, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.0240, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.4402, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.3246, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0451, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.4380, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.0641, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.7682, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 0.0301, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.2401, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.0133, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0579, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.6589, Accuracy: 0.7500\n",
      "Batch number: 118, Training: Loss: 0.3302, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.0940, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.5009, Accuracy: 0.8125\n",
      "Batch number: 121, Training: Loss: 0.4332, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.1382, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.3119, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.0463, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.4489, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.2995, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 0.0645, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0173, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0158, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0534, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0114, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.2736, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.2090, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0187, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0638, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0815, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.1334, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.2654, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.0151, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.2598, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 0.5053, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.1700, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0299, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0752, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.1588, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.3308, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0384, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0777, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1777, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.2091, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0147, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.2926, Accuracy: 0.8125\n",
      "Batch number: 157, Training: Loss: 0.0118, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0372, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1873, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0137, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0539, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.3256, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.2255, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.1720, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.2530, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.2305, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.0460, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.4434, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.4763, Accuracy: 0.7500\n",
      "Batch number: 173, Training: Loss: 0.9099, Accuracy: 0.7500\n",
      "Batch number: 174, Training: Loss: 0.1650, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.2687, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.1351, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.0440, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0321, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2276, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.1842, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.0063, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1736, Accuracy: 0.8750\n",
      "Batch number: 184, Training: Loss: 0.1058, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0189, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.2060, Accuracy: 0.8125\n",
      "Batch number: 187, Training: Loss: 0.0793, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.3843, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0804, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1179, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.5258, Accuracy: 0.8125\n",
      "Batch number: 193, Training: Loss: 0.0135, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0285, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.5088, Accuracy: 0.7500\n",
      "Batch number: 197, Training: Loss: 0.0202, Accuracy: 1.0000\n",
      "Epoch : 024, Training: Loss: 0.1756, Accuracy: 93.3396%, \n",
      "\t\tValidation : Loss : 0.1340, Accuracy: 95.2020%, Time: 35.9136s\n",
      "Epoch: 25/50\n",
      "Batch number: 000, Training: Loss: 0.3309, Accuracy: 0.7500\n",
      "Batch number: 001, Training: Loss: 0.1333, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.4344, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0303, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.2417, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.2353, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.2598, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.5732, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.1297, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.1212, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1968, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0863, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0677, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.1725, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.1237, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0282, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.4252, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.0725, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.2210, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.1927, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.1648, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.2454, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.0108, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.1447, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.3681, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0777, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0499, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.1892, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0977, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0284, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.1267, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.0394, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.1137, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.1990, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0912, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0267, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1937, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.0704, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.2229, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.2714, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.5402, Accuracy: 0.8125\n",
      "Batch number: 047, Training: Loss: 0.0639, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0612, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0294, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.3425, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.1283, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.3115, Accuracy: 0.8125\n",
      "Batch number: 054, Training: Loss: 0.1245, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0779, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.3479, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.0151, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.3161, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0728, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0017, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.3129, Accuracy: 0.8125\n",
      "Batch number: 063, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.1033, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.2619, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.6091, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.0594, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.4287, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.2645, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.0994, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.2448, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.2532, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.1797, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.4286, Accuracy: 0.7500\n",
      "Batch number: 078, Training: Loss: 0.1123, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0554, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0281, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0985, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0885, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0235, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.5596, Accuracy: 0.8125\n",
      "Batch number: 085, Training: Loss: 0.0099, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.1040, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.2884, Accuracy: 0.8125\n",
      "Batch number: 089, Training: Loss: 0.2107, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0742, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.1748, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.1274, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0797, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0849, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0401, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.2558, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0188, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0288, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.3635, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.1808, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.0336, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0225, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0122, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.3777, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.1449, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.3994, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.2297, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.3253, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 0.0492, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.0580, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.1974, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.3316, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0114, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1498, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.1373, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0964, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.3447, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.2969, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.3424, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.0232, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.1293, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.8085, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 0.3338, Accuracy: 0.8125\n",
      "Batch number: 128, Training: Loss: 0.2088, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 0.0912, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.1668, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 0.3129, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.4600, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.3279, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.1552, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0015, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1354, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.5153, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.4892, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.0797, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0843, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.6357, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.2692, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.3009, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.1156, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.0253, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.1345, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.2066, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.3886, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.0753, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1283, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.2481, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.3192, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.0486, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0331, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0523, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0284, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0187, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0165, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.1909, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0930, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.2399, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.3348, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1273, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.2901, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.3181, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.1233, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0648, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.0451, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.2448, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.0737, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0288, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.1928, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0795, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.3074, Accuracy: 0.6875\n",
      "Batch number: 181, Training: Loss: 0.2070, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.0314, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1013, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.1593, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.1871, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0851, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0666, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0951, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.7270, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.3177, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.2645, Accuracy: 0.8125\n",
      "Batch number: 193, Training: Loss: 0.4309, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.2148, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.1583, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0457, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2697, Accuracy: 0.8125\n",
      "Epoch : 025, Training: Loss: 0.1669, Accuracy: 93.8131%, \n",
      "\t\tValidation : Loss : 0.1144, Accuracy: 96.2121%, Time: 36.4616s\n",
      "Epoch: 26/50\n",
      "Batch number: 000, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1949, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.2172, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.3932, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.0135, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0270, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0908, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.2057, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.3239, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.2755, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.0701, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.1701, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.0406, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0535, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.1286, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0754, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.2618, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.3701, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 0.0765, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0333, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1465, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.0308, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.1967, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.4034, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.1749, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.1222, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.3670, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 0.2081, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.1234, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.1101, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.1847, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.9219, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 0.4558, Accuracy: 0.8125\n",
      "Batch number: 038, Training: Loss: 0.1637, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.4488, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0243, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.2188, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.1336, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.0491, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.1507, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.3491, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0168, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.2996, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 0.0561, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.4262, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.1212, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.5319, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0809, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.4623, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.0266, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.6846, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.0800, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.2230, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.3210, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.1277, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.0839, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.3731, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 0.0116, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1139, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.1809, Accuracy: 0.8125\n",
      "Batch number: 071, Training: Loss: 0.0377, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0387, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.1306, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0513, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.1225, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0262, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0236, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0730, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0687, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0607, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.3936, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 0.1531, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.0966, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.2189, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.1082, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1362, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.3167, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.1451, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.3756, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 0.0584, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.1281, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0300, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.1137, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0807, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.0417, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.1981, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0596, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.1430, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.5569, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.4297, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 0.1573, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.1790, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.0864, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0074, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.3465, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 0.1852, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.0627, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.1326, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0451, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0756, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0348, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0596, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.5523, Accuracy: 0.7500\n",
      "Batch number: 121, Training: Loss: 0.0268, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0398, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.1264, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0680, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0110, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.2937, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.0166, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.1480, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0569, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0858, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0974, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.3683, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.1318, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1130, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0435, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.2605, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.1338, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0068, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0856, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0277, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.3247, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.2675, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.3816, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.5576, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.3305, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.2242, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.2096, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.2482, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1236, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.1389, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0311, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.1324, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.1391, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0834, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0874, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.1083, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0872, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.2608, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.0437, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.3034, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.2371, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.2984, Accuracy: 0.8750\n",
      "Batch number: 168, Training: Loss: 0.4014, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0056, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0788, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.1115, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.4241, Accuracy: 0.8125\n",
      "Batch number: 173, Training: Loss: 0.0903, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.1764, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.0524, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0427, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0436, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.1058, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.2303, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.1213, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.1110, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.0543, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0819, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.0420, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.3583, Accuracy: 0.8125\n",
      "Batch number: 186, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.2552, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0281, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.1197, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.0303, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.4159, Accuracy: 0.8125\n",
      "Batch number: 193, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0586, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0259, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Epoch : 026, Training: Loss: 0.1486, Accuracy: 94.1919%, \n",
      "\t\tValidation : Loss : 0.1421, Accuracy: 94.6970%, Time: 40.1063s\n",
      "Epoch: 27/50\n",
      "Batch number: 000, Training: Loss: 0.0395, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0027, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.2981, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.0577, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.1161, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.8393, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 0.2286, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0779, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0220, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0966, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.0978, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.1652, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.6549, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 0.0225, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0395, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.1247, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.2462, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.2462, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.1635, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0374, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1186, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.1405, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0200, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.4856, Accuracy: 0.7500\n",
      "Batch number: 027, Training: Loss: 0.0306, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.1981, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.2458, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.0379, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.1722, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1317, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.4556, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.4824, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.0728, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.3569, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 0.0470, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0461, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0821, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0482, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.0185, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.1027, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.1319, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.0980, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.0032, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1576, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.4522, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.3510, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.1199, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1602, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0464, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.1357, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0492, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1465, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0742, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0383, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.1005, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0685, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.2241, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.3512, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.3592, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.0622, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.1268, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.2127, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0877, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.1984, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.1967, Accuracy: 0.8750\n",
      "Batch number: 077, Training: Loss: 0.1518, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.1941, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0878, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0173, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.1388, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0280, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0140, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.6803, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.0534, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.1074, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1289, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1358, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.2109, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0945, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.2505, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.0879, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.1761, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0462, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.4657, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.1520, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0966, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0513, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0815, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.0214, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0946, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.1786, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.2804, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 0.0836, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.0110, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0240, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1661, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0777, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.1514, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.0365, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0531, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0566, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0141, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0637, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0990, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.4304, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.0459, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.1915, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.0471, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.1304, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0509, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0083, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0980, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0245, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.5609, Accuracy: 0.8125\n",
      "Batch number: 131, Training: Loss: 0.3271, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0911, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.1095, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.2448, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0594, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0513, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0512, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.1831, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.2268, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0529, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1979, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.0671, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.1501, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.1723, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.2880, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.2087, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.0528, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.1144, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.0698, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.0369, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0860, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0309, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.3415, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.0592, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0887, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.6190, Accuracy: 0.8125\n",
      "Batch number: 160, Training: Loss: 0.0646, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.3019, Accuracy: 0.8125\n",
      "Batch number: 162, Training: Loss: 0.0350, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.6020, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.2686, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.1073, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.2551, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0874, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0753, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.1105, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.0260, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.0021, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.2390, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.1725, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.1775, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.2544, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.1124, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.0300, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0421, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0478, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.3394, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0211, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0354, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.1624, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.1904, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.0644, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.3746, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.0643, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.1180, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.5469, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.1670, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.0916, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1556, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.3563, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.1343, Accuracy: 0.8750\n",
      "Batch number: 197, Training: Loss: 0.2797, Accuracy: 0.8750\n",
      "Epoch : 027, Training: Loss: 0.1404, Accuracy: 94.7917%, \n",
      "\t\tValidation : Loss : 0.1056, Accuracy: 95.9596%, Time: 43.6780s\n",
      "Epoch: 28/50\n",
      "Batch number: 000, Training: Loss: 0.0394, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.3084, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.0326, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0419, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0627, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1261, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.2014, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.1573, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.2936, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.1072, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.0016, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.1300, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.0655, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1712, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0508, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0700, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.2478, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.2186, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.1662, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0421, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.2922, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.0032, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0042, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.1304, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0026, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0400, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0346, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.2718, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.6003, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.1187, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0272, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0697, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0102, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.4077, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.3532, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.1795, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.2352, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0320, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0568, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1171, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0191, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0237, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.3668, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0687, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.0924, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.5396, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.0980, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.2091, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1335, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.1124, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0679, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.2879, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.0694, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.2511, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.2449, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0912, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.1259, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0674, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0239, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0529, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.1421, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 0.0793, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.4136, Accuracy: 0.7500\n",
      "Batch number: 071, Training: Loss: 0.0796, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.0564, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0418, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.7167, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0075, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.1672, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0249, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.1232, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.1040, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0294, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.2070, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.1995, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.3815, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.0397, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.0509, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.2230, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 0.2540, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0588, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.1323, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.2968, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.0189, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0183, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.4220, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 0.0073, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0981, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.1032, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0669, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.0477, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0812, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.2858, Accuracy: 0.7500\n",
      "Batch number: 108, Training: Loss: 0.1347, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.1626, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0702, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0655, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.3198, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.2534, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0324, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0556, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0297, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0131, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0448, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0918, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.1814, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.1456, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.0703, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.2875, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.1166, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0444, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1043, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1865, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0524, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0086, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0710, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.1397, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0490, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.4112, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.2541, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0590, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0261, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1805, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.0243, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0723, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.1590, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.2347, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.1974, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1350, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.1296, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.1970, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0324, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0017, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.2644, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.1030, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.1046, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.0551, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1384, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.3147, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0032, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.1039, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0419, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0850, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0102, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.1930, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.1592, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.6033, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.1697, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.1586, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.1298, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.4649, Accuracy: 0.8125\n",
      "Batch number: 180, Training: Loss: 0.1760, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.1251, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.0253, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.2353, Accuracy: 0.8750\n",
      "Batch number: 184, Training: Loss: 0.2007, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.1383, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0580, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1234, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.4357, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0504, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.5626, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.1699, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.2753, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.3728, Accuracy: 0.8125\n",
      "Batch number: 194, Training: Loss: 0.0567, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0677, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.1993, Accuracy: 0.8750\n",
      "Epoch : 028, Training: Loss: 0.1271, Accuracy: 95.1073%, \n",
      "\t\tValidation : Loss : 0.1583, Accuracy: 95.7071%, Time: 43.3409s\n",
      "Epoch: 29/50\n",
      "Batch number: 000, Training: Loss: 0.3059, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0316, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0561, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0655, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.3898, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.4083, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0794, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.1646, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.1914, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.6791, Accuracy: 0.8125\n",
      "Batch number: 010, Training: Loss: 0.3284, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.0898, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0620, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.1517, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0545, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0230, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.4234, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.1566, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.1835, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.4603, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.3502, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.5757, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.0646, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0680, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0214, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0239, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0877, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.2168, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.0032, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0504, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0847, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0644, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.1717, Accuracy: 0.8125\n",
      "Batch number: 036, Training: Loss: 0.0686, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.1540, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0187, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.1184, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0571, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.1522, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.3513, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.2597, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.0386, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.2876, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 0.1055, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.0615, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.1340, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.0161, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0306, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.2002, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.2602, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.2725, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.1352, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.1884, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.2148, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.1693, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.0780, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0085, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0771, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.1497, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.1931, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.0657, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0651, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0198, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1830, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.3975, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0518, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.1797, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0750, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0261, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.3192, Accuracy: 0.8750\n",
      "Batch number: 077, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0910, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.2373, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0141, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.1409, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0648, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.4006, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.0652, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.0812, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0147, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0796, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.0266, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.0014, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.0349, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0448, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.1079, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.4167, Accuracy: 0.8125\n",
      "Batch number: 094, Training: Loss: 0.0556, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1939, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0393, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.1475, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.0646, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0083, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.2326, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.0293, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.1298, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.2865, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 0.5614, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.2896, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.1152, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.2200, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.6494, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.1090, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.4116, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0680, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0364, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.2564, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.0122, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.1606, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.1224, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.0326, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.1773, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.0377, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.1373, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1268, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.0190, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0442, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0779, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0450, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.2406, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.3033, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.2466, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0870, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.1659, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.1351, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1491, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.1443, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.1250, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.1032, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.1008, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1423, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.2832, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0109, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.2466, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.1132, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0463, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.0018, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.1310, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.1008, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.2438, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.2761, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.1748, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.1119, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.6531, Accuracy: 0.8125\n",
      "Batch number: 158, Training: Loss: 0.1931, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0301, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0858, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0890, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0237, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1044, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.2284, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.0017, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0604, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0101, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.2922, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.1603, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.0158, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2104, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.2631, Accuracy: 0.8125\n",
      "Batch number: 174, Training: Loss: 0.4397, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.1141, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.6295, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.1385, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.1403, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0485, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.1812, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.1685, Accuracy: 0.8750\n",
      "Batch number: 184, Training: Loss: 0.0211, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.3394, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.4855, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.0504, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.8482, Accuracy: 0.6875\n",
      "Batch number: 189, Training: Loss: 0.0787, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0671, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.4204, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0217, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0967, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0933, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.1373, Accuracy: 0.8750\n",
      "Epoch : 029, Training: Loss: 0.1462, Accuracy: 94.4444%, \n",
      "\t\tValidation : Loss : 0.1233, Accuracy: 96.2121%, Time: 43.4894s\n",
      "Epoch: 30/50\n",
      "Batch number: 000, Training: Loss: 0.0231, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1576, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.2853, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0816, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.0349, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1365, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0137, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0566, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0807, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.0417, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.1126, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0097, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0857, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0653, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.6039, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 0.0081, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.3463, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0722, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.5299, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.0975, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.1971, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0575, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0294, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.1981, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.1218, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.4034, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.3223, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.1876, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.1061, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.2693, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.2936, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.6279, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 0.2237, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.2465, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0166, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1699, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0075, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0589, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1039, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.2318, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.0139, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.2092, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.0026, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0158, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0587, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0277, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.5703, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 0.3888, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.2968, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0185, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.0453, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.1298, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.8405, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.3286, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.0579, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0522, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.6152, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 0.1464, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.1303, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.1311, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.0587, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.1661, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.2011, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0224, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0924, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0085, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0170, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.2371, Accuracy: 0.8125\n",
      "Batch number: 081, Training: Loss: 0.0215, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.3057, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.2157, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.0402, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.1897, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.4338, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.1306, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.5767, Accuracy: 0.8125\n",
      "Batch number: 090, Training: Loss: 0.1626, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0354, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.1875, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.1276, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.1085, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0684, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.0767, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.2644, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.2800, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0950, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1313, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0765, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0194, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.3271, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.1344, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.1472, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0726, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.2492, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0226, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.5072, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.0438, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.1171, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0897, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0240, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0399, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0487, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0229, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0304, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.1614, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.1773, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0001, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.1334, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.2156, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.2062, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0968, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.1930, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.0283, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1391, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.2684, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 0.0475, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0937, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.2653, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.0259, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1908, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1770, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.0997, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.1498, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.0193, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0920, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.0709, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1003, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.2800, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.2279, Accuracy: 0.8750\n",
      "Batch number: 156, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1221, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.1062, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.2270, Accuracy: 0.8750\n",
      "Batch number: 160, Training: Loss: 0.2142, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.1540, Accuracy: 0.8750\n",
      "Batch number: 162, Training: Loss: 0.1670, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.1833, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.1374, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.0429, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0813, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.8210, Accuracy: 0.8125\n",
      "Batch number: 169, Training: Loss: 0.0229, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0518, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0445, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.1330, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0918, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0495, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.2267, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0320, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0862, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.5825, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.1179, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.1713, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0824, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.0289, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.1853, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0180, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0714, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0754, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.2387, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.0849, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.2994, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0497, Accuracy: 1.0000\n",
      "Epoch : 030, Training: Loss: 0.1319, Accuracy: 95.2020%, \n",
      "\t\tValidation : Loss : 0.1042, Accuracy: 97.2222%, Time: 42.9382s\n",
      "Epoch: 31/50\n",
      "Batch number: 000, Training: Loss: 0.0541, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0748, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1393, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.0289, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.1947, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.4246, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.1621, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.6918, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.0252, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.2263, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.1587, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.0848, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.1297, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.1992, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0402, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.2145, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.0349, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0993, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1530, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0527, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0881, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.5389, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.3248, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.5709, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.2945, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.0011, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0427, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.1625, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.0898, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.1910, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0372, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1492, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.1893, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.2052, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.0234, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.1923, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0974, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0185, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0351, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0028, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.7623, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.2485, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.1987, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.1054, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0688, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0604, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0582, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.6032, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.0585, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.1386, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0521, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0074, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0241, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1271, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.0271, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.3338, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 0.3256, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.0177, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.0279, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0908, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.1218, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.1434, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.1025, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.1005, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0371, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.6552, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.0357, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.1788, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0293, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.2170, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.3403, Accuracy: 0.8125\n",
      "Batch number: 089, Training: Loss: 0.0561, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.1863, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.2348, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0590, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.2362, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.2203, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.1577, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1092, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.1113, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0368, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.2684, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.2233, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.1654, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.3238, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 0.0972, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0772, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.3115, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 0.4582, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.2597, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.2233, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.3181, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.1679, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0469, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0193, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0774, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.5051, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.2025, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1792, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0570, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.1676, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.0278, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.0663, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0795, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.2616, Accuracy: 0.8750\n",
      "Batch number: 126, Training: Loss: 0.1776, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.1127, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.1416, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.3067, Accuracy: 0.8750\n",
      "Batch number: 130, Training: Loss: 0.2360, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0249, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0120, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.1201, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0939, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0992, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.1481, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.4278, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.1911, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.5064, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 0.2269, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1110, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.1781, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.1159, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.2384, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.0224, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.2078, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.1227, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.3067, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.1858, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0517, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0109, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0314, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.4244, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.6521, Accuracy: 0.8125\n",
      "Batch number: 160, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.2108, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0921, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.2829, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.1002, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.0642, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.4580, Accuracy: 0.8125\n",
      "Batch number: 167, Training: Loss: 0.0235, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.2502, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.4654, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.1851, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.1354, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0451, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0889, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0045, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0890, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.3745, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.4841, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1730, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.4098, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.3653, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1211, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.4796, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0717, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0318, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1518, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.2724, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.0191, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.1296, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.1181, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.2141, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2302, Accuracy: 0.8750\n",
      "Epoch : 031, Training: Loss: 0.1512, Accuracy: 94.5707%, \n",
      "\t\tValidation : Loss : 0.1216, Accuracy: 95.4545%, Time: 42.1585s\n",
      "Epoch: 32/50\n",
      "Batch number: 000, Training: Loss: 0.4106, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0272, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.3087, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.0049, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0728, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.1030, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.1257, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0821, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0336, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.4088, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.5934, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.0376, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0858, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0304, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1649, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.0670, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0194, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.0557, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1504, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0586, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0972, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.1285, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.2164, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.1974, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.1692, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0248, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0068, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.1048, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.1530, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.1462, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.0040, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0580, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0883, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1364, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.1296, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.2304, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.2164, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0119, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1725, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.4520, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.0051, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.3003, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.3432, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 0.5594, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.1938, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.3565, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.1550, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 0.0120, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.4305, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0977, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0458, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.3075, Accuracy: 0.8125\n",
      "Batch number: 058, Training: Loss: 0.0705, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.2368, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0141, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.2478, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.2020, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.4664, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.0547, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0049, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0210, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.2153, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.1652, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.3395, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.0983, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.2353, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.1218, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.1955, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.1204, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0648, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.1785, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0693, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.2017, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 0.1003, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.0379, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0679, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.0331, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.3622, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 0.0090, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.4861, Accuracy: 0.7500\n",
      "Batch number: 090, Training: Loss: 0.3003, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.0584, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.4358, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0529, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1554, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.1956, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.1039, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0594, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0589, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0940, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.1689, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.0646, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0703, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0106, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0323, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.3206, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0684, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0906, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.0841, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.0219, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.2449, Accuracy: 0.8125\n",
      "Batch number: 115, Training: Loss: 0.1552, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.5947, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.0389, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0391, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.1021, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0155, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0214, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0917, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1045, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1158, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.3131, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0599, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.2845, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0899, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0825, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0738, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1170, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.2468, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0580, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.3313, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 0.0096, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.1020, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.3196, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.2964, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.0248, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0955, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0637, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.2760, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.1464, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.0635, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.2003, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0399, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.4439, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.1307, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0361, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.3263, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.0569, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.4204, Accuracy: 0.8125\n",
      "Batch number: 161, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.2182, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.1175, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.2326, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.0289, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.2160, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.0198, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0515, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0669, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0982, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.1264, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.4316, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.2167, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.2072, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.2130, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0107, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0934, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.1447, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.0602, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.1263, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.1953, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.1139, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0109, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1857, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.1436, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0713, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.1753, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.0027, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.0274, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0460, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.2102, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.0566, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0562, Accuracy: 1.0000\n",
      "Epoch : 032, Training: Loss: 0.1318, Accuracy: 95.1073%, \n",
      "\t\tValidation : Loss : 0.1794, Accuracy: 92.9293%, Time: 43.9755s\n",
      "Epoch: 33/50\n",
      "Batch number: 000, Training: Loss: 0.0031, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0209, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.6730, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.1158, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.1638, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.0601, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.1341, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0858, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0303, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.1262, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.3548, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.2269, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.1875, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0980, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.1032, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0959, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.2534, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.0188, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0577, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.1804, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.0254, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1848, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 0.1201, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.1791, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0624, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.4119, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0393, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0668, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1202, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.3651, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.0194, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.1962, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0602, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.3465, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.2528, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0369, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0903, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.1543, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.3457, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.3115, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.0049, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.0397, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.2678, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0541, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1713, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.0749, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.3442, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.1191, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.2299, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.2068, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0374, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.1589, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1746, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.3785, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.0412, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0965, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.1494, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.2512, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.1350, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.0572, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0443, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0313, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.2955, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1351, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0229, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.2115, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.1952, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.2000, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.1404, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0661, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.3067, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.0126, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.0492, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.2033, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.0121, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.1303, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1118, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0555, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.4124, Accuracy: 0.8125\n",
      "Batch number: 091, Training: Loss: 0.0214, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0121, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0612, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0226, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.3403, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.1953, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.7975, Accuracy: 0.8125\n",
      "Batch number: 098, Training: Loss: 0.0051, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.4403, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1272, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0178, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.7030, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 0.3550, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0063, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.2447, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.1966, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.2489, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.0743, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.1382, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0795, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.3349, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.5329, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0054, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0294, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0543, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0552, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.3436, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.3786, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.2496, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0200, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.2247, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.0715, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0450, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0422, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.2656, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.1891, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 0.0166, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0984, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0456, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.1006, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1248, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0608, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.2117, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0127, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0668, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0201, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0907, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.1962, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.3959, Accuracy: 0.7500\n",
      "Batch number: 150, Training: Loss: 0.0106, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0881, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.3077, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.1579, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.1481, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.3643, Accuracy: 0.8750\n",
      "Batch number: 156, Training: Loss: 0.0435, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.2460, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.0351, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0547, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.3117, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.2150, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0233, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0608, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.0275, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.1538, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1898, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.2121, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.3911, Accuracy: 0.8750\n",
      "Batch number: 169, Training: Loss: 0.4346, Accuracy: 0.8125\n",
      "Batch number: 170, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.2134, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.0943, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0571, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.0016, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.2515, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.0587, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0176, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0597, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.6953, Accuracy: 0.8125\n",
      "Batch number: 182, Training: Loss: 0.4978, Accuracy: 0.7500\n",
      "Batch number: 183, Training: Loss: 0.0580, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.1596, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.0927, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.1748, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.3945, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.0674, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0699, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.0197, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.3074, Accuracy: 0.8125\n",
      "Batch number: 192, Training: Loss: 0.0733, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0588, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.1677, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.1270, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0147, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0611, Accuracy: 0.9375\n",
      "Epoch : 033, Training: Loss: 0.1415, Accuracy: 94.8864%, \n",
      "\t\tValidation : Loss : 0.1142, Accuracy: 96.4646%, Time: 44.7677s\n",
      "Epoch: 34/50\n",
      "Batch number: 000, Training: Loss: 0.0411, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.2527, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.2479, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0281, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0565, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.1613, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0694, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0313, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0315, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0018, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0926, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1829, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0577, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0240, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0917, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.3379, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.1659, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.8482, Accuracy: 0.7500\n",
      "Batch number: 023, Training: Loss: 0.0562, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0293, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0361, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0796, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0165, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.4422, Accuracy: 0.8125\n",
      "Batch number: 030, Training: Loss: 0.4071, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.1144, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.2398, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.2427, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.1728, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.4230, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.1699, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.3316, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.5076, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0391, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.1480, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0177, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0875, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.1460, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0170, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0931, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.0267, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.1848, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.1428, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.1958, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.5238, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.3111, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 0.0359, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.2878, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.2604, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1219, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0679, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.2921, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0011, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.2339, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.4189, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.1735, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0457, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.2864, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.1579, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.2581, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 0.1085, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.2602, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.0333, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1460, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0517, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.1131, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0367, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0825, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.2019, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0405, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.3305, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.2744, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.4215, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.0970, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.1028, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.4146, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.2803, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0847, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.1109, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0553, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0827, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0694, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1842, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.2368, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0664, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1563, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.0771, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1542, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.3168, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.4202, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0654, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.1644, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.4712, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 0.1689, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.1051, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0502, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1392, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0529, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0598, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0256, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.5108, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.1006, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.4082, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.0288, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0156, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0121, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.0950, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1985, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0840, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0339, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0090, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.4134, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.0437, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1800, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0902, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.1771, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.2413, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.4802, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.3605, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.0879, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0874, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.1067, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.1409, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.1401, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0485, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.1373, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.2063, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.2675, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.0367, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.1692, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.1476, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 0.0459, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0292, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0162, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1320, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0291, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.4283, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0926, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0540, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.2192, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0992, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.1246, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0122, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.1081, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.1047, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0778, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.2197, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.1469, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.2850, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.1540, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.0560, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0011, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0284, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.2549, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.0515, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0262, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0791, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0940, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.2364, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.0812, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.1353, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.0855, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0943, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.0023, Accuracy: 1.0000\n",
      "Epoch : 034, Training: Loss: 0.1322, Accuracy: 95.3283%, \n",
      "\t\tValidation : Loss : 0.1322, Accuracy: 96.2121%, Time: 72.1220s\n",
      "Epoch: 35/50\n",
      "Batch number: 000, Training: Loss: 0.1458, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0736, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.1342, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.0298, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0618, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.4468, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.1741, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0107, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1152, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0257, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0739, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.1874, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.8061, Accuracy: 0.7500\n",
      "Batch number: 014, Training: Loss: 0.0002, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0796, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0537, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.4850, Accuracy: 0.8125\n",
      "Batch number: 019, Training: Loss: 0.1136, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0408, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.3620, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0789, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.1087, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0724, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.2711, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0532, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.2127, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.4388, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.0722, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.2760, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.4002, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0546, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.0938, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0109, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.1798, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.1448, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0624, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.3916, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.3723, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 0.0485, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0431, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0252, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.3913, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.0283, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0529, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.1057, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.1097, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.5005, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.1553, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.1291, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.3376, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.2794, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.0781, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0519, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.0102, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0926, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.3393, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.2274, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1138, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.0198, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.0026, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.2743, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.0515, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0373, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.3520, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0275, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.2004, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.2204, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 0.1614, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.0461, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0346, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.5689, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.1032, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.2242, Accuracy: 0.8125\n",
      "Batch number: 089, Training: Loss: 0.0541, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.6335, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.0002, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0176, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0703, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.2657, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.1199, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.0720, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0486, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.3514, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.2186, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 0.0653, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.0577, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.1176, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.1962, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.4233, Accuracy: 0.6875\n",
      "Batch number: 108, Training: Loss: 0.0890, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0265, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.3441, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.1645, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.1107, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0267, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0771, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.3160, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.2611, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.1811, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.2653, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0151, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.0570, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0156, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0216, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0804, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0187, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0294, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0135, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0183, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0429, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1079, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1736, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.2949, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.1765, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0531, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0173, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0648, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0930, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.1028, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.1830, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.2003, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0942, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0736, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0711, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0236, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.3873, Accuracy: 0.8125\n",
      "Batch number: 156, Training: Loss: 0.2541, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.2715, Accuracy: 0.8125\n",
      "Batch number: 158, Training: Loss: 0.3596, Accuracy: 0.8125\n",
      "Batch number: 159, Training: Loss: 0.1105, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.0751, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.1377, Accuracy: 0.8750\n",
      "Batch number: 162, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.2102, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.3267, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.4297, Accuracy: 0.8750\n",
      "Batch number: 168, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0632, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0655, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.1220, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0425, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.2950, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.2043, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0781, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.1806, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0873, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0630, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.2668, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.2405, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0262, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.5657, Accuracy: 0.8125\n",
      "Batch number: 190, Training: Loss: 0.0408, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1825, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0141, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.0256, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.2307, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0579, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.2564, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.1353, Accuracy: 0.9375\n",
      "Epoch : 035, Training: Loss: 0.1284, Accuracy: 95.2020%, \n",
      "\t\tValidation : Loss : 0.1817, Accuracy: 93.9394%, Time: 202.5065s\n",
      "Epoch: 36/50\n",
      "Batch number: 000, Training: Loss: 0.1190, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.4956, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.0139, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0603, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.2259, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.1065, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0438, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0932, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0027, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0264, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0241, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.1599, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.0527, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0018, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.2096, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.0491, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.5446, Accuracy: 0.8125\n",
      "Batch number: 018, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.0195, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0862, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.1240, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.0001, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.1182, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.1640, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0745, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.4325, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0634, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.7983, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.0529, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0317, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.1610, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.0893, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.1769, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.2202, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0716, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0015, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0696, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0195, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.1223, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.3875, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.1222, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.1482, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.3119, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.2664, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.1979, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.5244, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.1740, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.1578, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.3260, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.2737, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.0633, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.2830, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.0354, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.6451, Accuracy: 0.8125\n",
      "Batch number: 058, Training: Loss: 0.3314, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 0.3034, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.2505, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.3930, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.2959, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.3984, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.2377, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0555, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.3288, Accuracy: 0.8125\n",
      "Batch number: 073, Training: Loss: 0.1575, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.0264, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0213, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0806, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0016, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0266, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.3881, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 0.4721, Accuracy: 0.8125\n",
      "Batch number: 081, Training: Loss: 0.1534, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 0.5564, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.6891, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.3203, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0443, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0246, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0554, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.0120, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.1866, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.5607, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.0940, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.7137, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.3017, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.3204, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.5203, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.0065, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1058, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0455, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.3660, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0306, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0474, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.1852, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.1298, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1325, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0505, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0072, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0316, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0809, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0257, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.2008, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.4796, Accuracy: 0.7500\n",
      "Batch number: 115, Training: Loss: 0.4739, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.2194, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.0746, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.3808, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.5099, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.4793, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.3111, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1424, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.0777, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0335, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.1349, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.1594, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.2082, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 0.0893, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.2437, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.2572, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.1638, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.2159, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.5264, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.1821, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0718, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0263, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.1246, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.2808, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0766, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.2635, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.0200, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0652, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.1093, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.2548, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.1748, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.2471, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0230, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.5021, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.4633, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 0.4828, Accuracy: 0.8125\n",
      "Batch number: 153, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0994, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0159, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.1550, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.2172, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.2456, Accuracy: 0.8750\n",
      "Batch number: 159, Training: Loss: 0.1915, Accuracy: 0.8750\n",
      "Batch number: 160, Training: Loss: 0.6402, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.3604, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0903, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.7366, Accuracy: 0.8125\n",
      "Batch number: 166, Training: Loss: 0.0155, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0832, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.3279, Accuracy: 0.7500\n",
      "Batch number: 169, Training: Loss: 0.0308, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2704, Accuracy: 0.8125\n",
      "Batch number: 173, Training: Loss: 0.2052, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.1069, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.4497, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.2558, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1017, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.2048, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2573, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.1262, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.6073, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.0487, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0165, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0641, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.1271, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.0718, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.6283, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.3553, Accuracy: 0.8125\n",
      "Batch number: 190, Training: Loss: 0.0692, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.4866, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.0065, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.0509, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0233, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0367, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.2950, Accuracy: 0.9375\n",
      "Epoch : 036, Training: Loss: 0.1810, Accuracy: 93.8763%, \n",
      "\t\tValidation : Loss : 0.1398, Accuracy: 93.9394%, Time: 129.2244s\n",
      "Epoch: 37/50\n",
      "Batch number: 000, Training: Loss: 0.1774, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0898, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0709, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.3198, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.3007, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.1167, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.1192, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.2093, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.3010, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.0438, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0926, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1483, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.0189, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0834, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.3670, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.1490, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0551, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.2774, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.1542, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0505, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.1073, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.2190, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0256, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0237, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.1054, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.5274, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.3314, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.0462, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.3848, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.3268, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0693, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0454, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0707, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0018, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.1391, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.1554, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.1401, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.0294, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0524, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0555, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0217, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.1584, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0056, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.2162, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.1515, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0598, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.3569, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 0.0645, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.2668, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.2088, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.2493, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.2904, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.7378, Accuracy: 0.7500\n",
      "Batch number: 069, Training: Loss: 0.1743, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.1058, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0248, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.1157, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.2900, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.1044, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0822, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.2584, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.1155, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.3556, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.5068, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 0.3833, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 0.1708, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.2349, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.1549, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1602, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0721, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.3028, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.1556, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.2110, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.3685, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.3412, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.1968, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1264, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.2371, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0187, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0031, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0300, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1486, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0796, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0953, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.2467, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.1007, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.2276, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.3030, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.1001, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0237, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.1063, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.1823, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.3344, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.2695, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 0.0265, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.5249, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.3395, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.1198, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.2218, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1213, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.2158, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.1439, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0243, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.4329, Accuracy: 0.7500\n",
      "Batch number: 128, Training: Loss: 0.1420, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0607, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0922, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0396, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0743, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0540, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.4580, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0364, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0665, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.3560, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0575, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.1313, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.1065, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.3650, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.5495, Accuracy: 0.8125\n",
      "Batch number: 146, Training: Loss: 0.1824, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.3458, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.0096, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.1558, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.4030, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 0.2807, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.4086, Accuracy: 0.8125\n",
      "Batch number: 153, Training: Loss: 0.1483, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.2445, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.0918, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.1320, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0709, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.2777, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.3394, Accuracy: 0.8125\n",
      "Batch number: 161, Training: Loss: 0.0575, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0251, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0727, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.0073, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0670, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.9833, Accuracy: 0.6875\n",
      "Batch number: 167, Training: Loss: 0.0011, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.3087, Accuracy: 0.8125\n",
      "Batch number: 169, Training: Loss: 0.3141, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.1752, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.0852, Accuracy: 0.9375\n",
      "Batch number: 172, Training: Loss: 0.0592, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0588, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0127, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0583, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.2968, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.0681, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.3219, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0966, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.1644, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.2456, Accuracy: 0.8125\n",
      "Batch number: 184, Training: Loss: 0.0933, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.5775, Accuracy: 0.8125\n",
      "Batch number: 186, Training: Loss: 0.0591, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0229, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.2064, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.1123, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.0584, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1230, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0495, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0239, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.4245, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.2305, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0119, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.1077, Accuracy: 0.9375\n",
      "Epoch : 037, Training: Loss: 0.1524, Accuracy: 94.1604%, \n",
      "\t\tValidation : Loss : 0.1134, Accuracy: 96.4646%, Time: 419.2186s\n",
      "Epoch: 38/50\n",
      "Batch number: 000, Training: Loss: 0.2494, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 0.0076, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1898, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 0.4375, Accuracy: 0.8125\n",
      "Batch number: 005, Training: Loss: 0.0198, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0801, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0590, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0667, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.1396, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.1215, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1071, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0173, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0831, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.4691, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.0106, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.0395, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.0869, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1808, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.0027, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0162, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1221, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0415, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0788, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.1196, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.1401, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.0494, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0011, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.3173, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.2381, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.3102, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.1037, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0749, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.1464, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 0.0869, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0871, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0710, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.1584, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.1037, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.1940, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.0246, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.2821, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 0.1547, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0519, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.1199, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.2388, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.1628, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0682, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.2462, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.1037, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0763, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.2303, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0489, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0027, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0356, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.2493, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 0.1535, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0131, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1570, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.2550, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.1406, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.3447, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0810, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0790, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0180, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0620, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.2224, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.1250, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.1359, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.1203, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.2576, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.1811, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.2020, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0219, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.2125, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0638, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.2294, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 0.2582, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.1236, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.0344, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0014, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.3099, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0473, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.1159, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0451, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0277, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.3849, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.1300, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0524, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0395, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.0264, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.5426, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.1119, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.0128, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.0862, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0337, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0538, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0939, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.1385, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.1470, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.7135, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.2260, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.1309, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0120, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.3418, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.0681, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0234, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0672, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.1374, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0298, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.7785, Accuracy: 0.7500\n",
      "Batch number: 133, Training: Loss: 0.2201, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0501, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.0228, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.2076, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.2299, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0853, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0716, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0531, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1214, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.2056, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.2083, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.1988, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.7876, Accuracy: 0.8125\n",
      "Batch number: 150, Training: Loss: 0.4258, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.1233, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1961, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.1067, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.2354, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.1760, Accuracy: 0.8750\n",
      "Batch number: 156, Training: Loss: 0.5455, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.1354, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.4148, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.4150, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0127, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.3156, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.0174, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.6159, Accuracy: 0.8125\n",
      "Batch number: 166, Training: Loss: 0.0197, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0746, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.1853, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.3022, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.3627, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.1153, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.2693, Accuracy: 0.8750\n",
      "Batch number: 174, Training: Loss: 0.0424, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0451, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0851, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.1071, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.3303, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.0921, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0867, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0245, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0852, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.4535, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.1550, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.1071, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.4921, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.0275, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.3624, Accuracy: 0.8125\n",
      "Batch number: 190, Training: Loss: 0.1711, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.1360, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.1374, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.3041, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0301, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.2429, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2142, Accuracy: 0.8750\n",
      "Epoch : 038, Training: Loss: 0.1372, Accuracy: 94.9495%, \n",
      "\t\tValidation : Loss : 0.1364, Accuracy: 96.4646%, Time: 4199.6959s\n",
      "Epoch: 39/50\n",
      "Batch number: 000, Training: Loss: 0.2485, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 0.2216, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.5721, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0720, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.2636, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.2554, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0065, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0991, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0014, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0568, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0842, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0209, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0333, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1528, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.2301, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0803, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.1820, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.2913, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.1975, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.1645, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1519, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0888, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.0697, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.3850, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.1316, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0671, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0859, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.1083, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.1917, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.2554, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 0.0335, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.3272, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.3260, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0656, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.1133, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.1067, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.1362, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.1079, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.1157, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0174, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0170, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.6289, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0534, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.4003, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.0487, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0451, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.3305, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.2950, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.5120, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.4253, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.3808, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.2893, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 0.0220, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.1487, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 0.0505, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0135, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0544, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0205, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0571, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0723, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.1119, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0507, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.2934, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.2410, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0040, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.5443, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 0.1908, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0814, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0749, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0015, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0193, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0709, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.1496, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.2470, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0355, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.1777, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.0640, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1644, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.1642, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.1501, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0001, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0593, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0765, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.1517, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0104, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.4302, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 0.0997, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0923, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.4755, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 0.0315, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1848, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1214, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.1670, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0650, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0326, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.1401, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.1152, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.5080, Accuracy: 0.8125\n",
      "Batch number: 115, Training: Loss: 0.0889, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.1968, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.0748, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0176, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.2445, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.4368, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.0313, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.1622, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.0228, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1288, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.1371, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0811, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.3156, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.2588, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.2016, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0439, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0137, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.1300, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0585, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0181, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0011, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.1334, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.1947, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.4110, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.0851, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.2339, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.1734, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.0229, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.2367, Accuracy: 0.8125\n",
      "Batch number: 156, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1616, Accuracy: 0.8750\n",
      "Batch number: 158, Training: Loss: 0.0214, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.6055, Accuracy: 0.8125\n",
      "Batch number: 160, Training: Loss: 0.2921, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.0237, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0234, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1926, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.0147, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.1495, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1365, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0746, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0814, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0469, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2887, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0881, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0014, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.5202, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.1990, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.1386, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.2224, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.1461, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.2473, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.3644, Accuracy: 0.8125\n",
      "Batch number: 189, Training: Loss: 0.0604, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.3721, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.1057, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.1847, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.2057, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.0614, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.1790, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.0348, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.1518, Accuracy: 0.9375\n",
      "Epoch : 039, Training: Loss: 0.1297, Accuracy: 95.3598%, \n",
      "\t\tValidation : Loss : 0.1163, Accuracy: 95.9596%, Time: 34.4788s\n",
      "Epoch: 40/50\n",
      "Batch number: 000, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0072, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.1179, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.4309, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.0271, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0264, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.2539, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.1004, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.0336, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.5740, Accuracy: 0.8125\n",
      "Batch number: 012, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.1089, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1403, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.1087, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.0510, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0688, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.1031, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.0588, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1787, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.2311, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.0176, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0211, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.1178, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.0958, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0002, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.3881, Accuracy: 0.8125\n",
      "Batch number: 029, Training: Loss: 0.1307, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0516, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0400, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.2213, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.2037, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.1038, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 0.0956, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.1893, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0558, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.2162, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0854, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.1091, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.1048, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0584, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1367, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.2856, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.0418, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.2614, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.0751, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0195, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.6935, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.0464, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.3744, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0347, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0390, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0355, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0347, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0555, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.4544, Accuracy: 0.7500\n",
      "Batch number: 069, Training: Loss: 0.1628, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.1514, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.1342, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.1424, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.0304, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.2891, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.1520, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0794, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0671, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.2680, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.1437, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0819, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.1973, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.0022, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.1113, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.1325, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.7100, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.1393, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0483, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.0758, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0284, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0577, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.1412, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.0103, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.2037, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0312, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0627, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0601, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0746, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.5941, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.1396, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.3894, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.0682, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.3447, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.2196, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 0.0158, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1813, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.1263, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0278, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0022, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.2164, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.1556, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.0647, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.3256, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.1012, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.3393, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.0558, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0264, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0946, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.1087, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 0.2908, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.0162, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1096, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.3038, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0410, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.2417, Accuracy: 0.8125\n",
      "Batch number: 133, Training: Loss: 0.2907, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.1013, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.1380, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.0504, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1946, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.1637, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0225, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1672, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.2703, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.2964, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.1089, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.4475, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.0021, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.3773, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.4721, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0377, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0781, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.2633, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.0982, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0973, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.1613, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.2682, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0293, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.1191, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.2060, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.1821, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.1236, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.2294, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.2887, Accuracy: 0.8125\n",
      "Batch number: 168, Training: Loss: 0.0132, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.5847, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.0226, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.2299, Accuracy: 0.8750\n",
      "Batch number: 172, Training: Loss: 0.2675, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.1042, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.1309, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.0022, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.1053, Accuracy: 0.9375\n",
      "Batch number: 179, Training: Loss: 0.4794, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.3000, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0556, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.1249, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.1501, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0096, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.1188, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.3626, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.1076, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.1227, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.4895, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0567, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.2474, Accuracy: 0.8750\n",
      "Epoch : 040, Training: Loss: 0.1315, Accuracy: 94.6970%, \n",
      "\t\tValidation : Loss : 0.1121, Accuracy: 97.2222%, Time: 37.0259s\n",
      "Epoch: 41/50\n",
      "Batch number: 000, Training: Loss: 0.2003, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0017, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.2878, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 0.1085, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.0196, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0681, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0314, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.0573, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.5943, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.6033, Accuracy: 0.8125\n",
      "Batch number: 011, Training: Loss: 0.0851, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0449, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.3224, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0551, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0054, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0472, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.1682, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0491, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.2886, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.1715, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.1568, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.2998, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0132, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2207, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.3357, Accuracy: 0.7500\n",
      "Batch number: 027, Training: Loss: 0.0914, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0744, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.2575, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.0607, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0330, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0980, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.1297, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0870, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.2428, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.0335, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0045, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.1901, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0210, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0528, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.3684, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.0910, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.1751, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.0653, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.1498, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0535, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0064, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.1475, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0550, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0194, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.2170, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.1027, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.1726, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.1551, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0588, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.1046, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.1583, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0239, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.1334, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.1592, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.1909, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0093, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0061, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.3585, Accuracy: 0.8125\n",
      "Batch number: 069, Training: Loss: 0.3204, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.0305, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.3594, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.2074, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0335, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.1133, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0002, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.2233, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.1265, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0614, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0309, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0826, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0926, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.2468, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.0596, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.3949, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.2092, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1506, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0444, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.3146, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.1332, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.0799, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.2092, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0517, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.1828, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.0456, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.5918, Accuracy: 0.8125\n",
      "Batch number: 098, Training: Loss: 0.1996, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.0704, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.1950, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0642, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.7645, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 0.0377, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0107, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0804, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0979, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0370, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.1739, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 0.2724, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.2841, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.0587, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0903, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0298, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0284, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0573, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0736, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.0437, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.3440, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0834, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.0286, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.5890, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 0.0736, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1198, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.6258, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.3762, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0251, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1409, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.0798, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.1802, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0450, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0388, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1874, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.5101, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.5973, Accuracy: 0.8125\n",
      "Batch number: 145, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.1291, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.0394, Accuracy: 1.0000\n",
      "Batch number: 148, Training: Loss: 0.2941, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.3090, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.0559, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.1402, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.0768, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.0556, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.1007, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0983, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.0862, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0015, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1717, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.0280, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.1021, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.2207, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.0651, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.5593, Accuracy: 0.8125\n",
      "Batch number: 169, Training: Loss: 0.2713, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.2576, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.4019, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.6369, Accuracy: 0.8125\n",
      "Batch number: 174, Training: Loss: 0.3497, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.0453, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.1942, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.3585, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0512, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.3040, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.0315, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0319, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.3494, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.0394, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0568, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0899, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0603, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.1177, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.3895, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.0106, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0028, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.1199, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.2409, Accuracy: 0.8750\n",
      "Batch number: 194, Training: Loss: 0.5282, Accuracy: 0.8125\n",
      "Batch number: 195, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.1355, Accuracy: 0.8750\n",
      "Batch number: 197, Training: Loss: 0.0580, Accuracy: 0.9375\n",
      "Epoch : 041, Training: Loss: 0.1405, Accuracy: 95.1073%, \n",
      "\t\tValidation : Loss : 0.1045, Accuracy: 96.7172%, Time: 37.9402s\n",
      "Epoch: 42/50\n",
      "Batch number: 000, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.0550, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0139, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0040, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0657, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.2015, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0303, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.3136, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.0193, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1022, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.3332, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.3540, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0504, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.5615, Accuracy: 0.7500\n",
      "Batch number: 015, Training: Loss: 0.0683, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.6906, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.1056, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.1894, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0542, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0015, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0769, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0348, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2696, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.1451, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.0076, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0109, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.1557, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0006, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0723, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0242, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.3057, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.2189, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.0851, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0049, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0557, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.1104, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1327, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.1790, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.2781, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 0.2914, Accuracy: 0.8125\n",
      "Batch number: 047, Training: Loss: 0.1404, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0565, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0541, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0518, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.2716, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0906, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.4018, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0932, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.0484, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.1382, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.0660, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.1352, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.0331, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0190, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0164, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.0308, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.2972, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.1717, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.0470, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.2317, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.0188, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.2867, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0884, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0647, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.1185, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.2120, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.1144, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.7647, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.1348, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.0291, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.2061, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1948, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.3188, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 0.1382, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.1140, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.0178, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.2828, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.2446, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.1292, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1178, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.3843, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0193, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.1136, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.1616, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.0332, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0671, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1952, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.0831, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0310, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.0133, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0543, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0701, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.1700, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.0474, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0170, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.1289, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.1589, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.0105, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.5717, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.0081, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0760, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.2759, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.2696, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.0497, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.2060, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.3443, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.5240, Accuracy: 0.8125\n",
      "Batch number: 132, Training: Loss: 0.2008, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1797, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.1226, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.2877, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 0.0650, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0252, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.1056, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0357, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0099, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0409, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.4880, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 0.0372, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.1938, Accuracy: 0.8125\n",
      "Batch number: 147, Training: Loss: 0.2387, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.1774, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.2454, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0279, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.7237, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 0.0867, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.4004, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0042, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.2228, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1701, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.5037, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.0075, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.2673, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0023, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.2921, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.1471, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.2218, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.2249, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.0784, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0488, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.1756, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.0319, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.0155, Accuracy: 1.0000\n",
      "Batch number: 173, Training: Loss: 0.0074, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.0440, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0684, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.7703, Accuracy: 0.7500\n",
      "Batch number: 180, Training: Loss: 0.2718, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.1994, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.1004, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0225, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.1811, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.4085, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.1463, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.3720, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.0560, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.5001, Accuracy: 0.8125\n",
      "Batch number: 191, Training: Loss: 0.2924, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.3198, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.0102, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.4207, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0357, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Epoch : 042, Training: Loss: 0.1344, Accuracy: 95.6755%, \n",
      "\t\tValidation : Loss : 0.1080, Accuracy: 96.7172%, Time: 34.4605s\n",
      "Epoch: 43/50\n",
      "Batch number: 000, Training: Loss: 0.0440, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.5283, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.0440, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0674, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0464, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.1144, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.1644, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0436, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.1366, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.0473, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0769, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.3637, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.0652, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.2405, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0112, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.1516, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.0301, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0001, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0340, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.3403, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.2704, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.3524, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.5062, Accuracy: 0.8125\n",
      "Batch number: 031, Training: Loss: 0.6171, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 0.1233, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0036, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0106, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.4638, Accuracy: 0.8125\n",
      "Batch number: 039, Training: Loss: 0.0501, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.1299, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1064, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.3665, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.3095, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 0.1665, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.2827, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.0452, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.1559, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.0905, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.1523, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.3110, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0589, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0709, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.4081, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 0.0257, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.1835, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.2901, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.0808, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 0.0042, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.1428, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.1117, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.1931, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0739, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0401, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.4940, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 0.1232, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.4793, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.1867, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.0075, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0072, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0171, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.1598, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.4625, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0184, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0814, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0596, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.0205, Accuracy: 1.0000\n",
      "Batch number: 086, Training: Loss: 0.0127, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0471, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.3064, Accuracy: 0.8125\n",
      "Batch number: 089, Training: Loss: 0.2951, Accuracy: 0.8125\n",
      "Batch number: 090, Training: Loss: 0.0427, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0488, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.1199, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0162, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.3020, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.0512, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.0759, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1186, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.3660, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.1052, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0686, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0120, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.2303, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0876, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0803, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 0.1505, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0558, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0917, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.5141, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.0368, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.1367, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.1434, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.0751, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.0399, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.4306, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.0960, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.0413, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0406, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.1151, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.0681, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.3217, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.1097, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0455, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0420, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.1196, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.6231, Accuracy: 0.6875\n",
      "Batch number: 131, Training: Loss: 0.0045, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0267, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.2057, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.1896, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1054, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1700, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.0404, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0825, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.3514, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.4882, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.2967, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.1167, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0137, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.2838, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 0.0249, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0232, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0497, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.1757, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.1343, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.0016, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0155, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.2281, Accuracy: 0.9375\n",
      "Batch number: 157, Training: Loss: 0.1221, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0002, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0049, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.1866, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.2361, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.1947, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0633, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.1600, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0487, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.2302, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.1417, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0254, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0566, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.1738, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.0086, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.0161, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.2393, Accuracy: 0.8750\n",
      "Batch number: 176, Training: Loss: 0.0348, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.0533, Accuracy: 1.0000\n",
      "Batch number: 178, Training: Loss: 0.2126, Accuracy: 0.8750\n",
      "Batch number: 179, Training: Loss: 0.5060, Accuracy: 0.8125\n",
      "Batch number: 180, Training: Loss: 0.1758, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.3411, Accuracy: 0.8750\n",
      "Batch number: 186, Training: Loss: 0.0608, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.6228, Accuracy: 0.8750\n",
      "Batch number: 188, Training: Loss: 0.3417, Accuracy: 0.8750\n",
      "Batch number: 189, Training: Loss: 0.0631, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.1037, Accuracy: 0.9375\n",
      "Batch number: 191, Training: Loss: 0.1875, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0590, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.4483, Accuracy: 0.7500\n",
      "Batch number: 194, Training: Loss: 0.0874, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.2307, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.0390, Accuracy: 1.0000\n",
      "Epoch : 043, Training: Loss: 0.1289, Accuracy: 95.3283%, \n",
      "\t\tValidation : Loss : 0.0711, Accuracy: 97.4747%, Time: 35.0607s\n",
      "Epoch: 44/50\n",
      "Batch number: 000, Training: Loss: 0.1000, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.1203, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0623, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0204, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0342, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0031, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.1309, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0022, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1119, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.4512, Accuracy: 0.6875\n",
      "Batch number: 011, Training: Loss: 0.2748, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.0576, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.2844, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.2836, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.1091, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 0.2213, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.2563, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0188, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.0036, Accuracy: 1.0000\n",
      "Batch number: 023, Training: Loss: 0.2091, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 0.0403, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 027, Training: Loss: 0.0552, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0526, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.1419, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.1512, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 0.0179, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0556, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.1424, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.0467, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.0395, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.4263, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 0.2991, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.1103, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0729, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.0751, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1702, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.0411, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0811, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0321, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0226, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0306, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.2019, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.2297, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.2909, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.2071, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.0514, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0237, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.2347, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0927, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0131, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.8108, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.3235, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 0.0924, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.0813, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.1279, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.3408, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 0.0389, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.5293, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.1769, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.1141, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0183, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.3461, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 0.5414, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 0.0445, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0653, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0968, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0534, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.1820, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 0.0236, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.1111, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.1608, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.4887, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 0.0747, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.1085, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0351, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.0873, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0279, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0955, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.0517, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.1883, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.1414, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.2724, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.2472, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1347, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.2382, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.3412, Accuracy: 0.8125\n",
      "Batch number: 102, Training: Loss: 0.1203, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.5210, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 0.2911, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.0319, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0702, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.0286, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0201, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0064, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.3301, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 0.1901, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.1946, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.2727, Accuracy: 0.8750\n",
      "Batch number: 116, Training: Loss: 0.0228, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0180, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0796, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.1924, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.2897, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.0336, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0263, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.0081, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.7279, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.1826, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0450, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0169, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0802, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0247, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0304, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.4828, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.1707, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.1498, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0699, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.5224, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 0.0471, Accuracy: 0.9375\n",
      "Batch number: 138, Training: Loss: 0.2862, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 0.6616, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.2172, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.1526, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0744, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0161, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.4924, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.4026, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.3254, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.0399, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.3812, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.1430, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.2003, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0181, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0317, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.1099, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0527, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0698, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0505, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.0525, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0482, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.1303, Accuracy: 0.9375\n",
      "Batch number: 166, Training: Loss: 0.0752, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.0151, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.5278, Accuracy: 0.8125\n",
      "Batch number: 169, Training: Loss: 0.3427, Accuracy: 0.8750\n",
      "Batch number: 170, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0263, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.3360, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.1016, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.2328, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.0672, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.1559, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.3090, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.3121, Accuracy: 0.8125\n",
      "Batch number: 180, Training: Loss: 0.4215, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.3627, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.2218, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0291, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.2792, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.1021, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.1009, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.1972, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.0468, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.3113, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.0510, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.2157, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1137, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0599, Accuracy: 1.0000\n",
      "Epoch : 044, Training: Loss: 0.1400, Accuracy: 94.8232%, \n",
      "\t\tValidation : Loss : 0.1294, Accuracy: 95.7071%, Time: 35.2693s\n",
      "Epoch: 45/50\n",
      "Batch number: 000, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.2971, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0697, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.2519, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 0.2861, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 0.1919, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.2617, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 0.1305, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.1262, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.0045, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.1532, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0322, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1121, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.0156, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.2775, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.0840, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0918, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.2679, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 0.0527, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.4839, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.0196, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0152, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0840, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.2532, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.1660, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0441, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0028, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.3505, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 0.1448, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1430, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0257, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.1065, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0158, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.4096, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0995, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.1104, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0101, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0107, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0291, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.3187, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.2271, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.1583, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0051, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1000, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.5838, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0558, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.2928, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.0835, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.2545, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.0718, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.1920, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.6181, Accuracy: 0.7500\n",
      "Batch number: 059, Training: Loss: 0.1627, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.0858, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0632, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.3814, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.0850, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.2211, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.3825, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.3135, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.1567, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 0.0791, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.0525, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.0371, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0028, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.0599, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.4007, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 0.1862, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0324, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.0270, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0608, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0056, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0996, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.1763, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0133, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.1648, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.1093, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0894, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.5532, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.0889, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.1508, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0383, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0001, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.2527, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.0558, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0468, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0535, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.0304, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0672, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.1165, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.0302, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0244, Accuracy: 1.0000\n",
      "Batch number: 106, Training: Loss: 0.0596, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0635, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0416, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0093, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1535, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.4058, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 0.2758, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.0392, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0882, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.2538, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.4362, Accuracy: 0.8125\n",
      "Batch number: 119, Training: Loss: 0.2316, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.2298, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.2761, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.2397, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0688, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.1499, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 0.0026, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0489, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0772, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.0329, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1507, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.0296, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0529, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.1680, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0256, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.5742, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.0023, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.0516, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0014, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0056, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0028, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0340, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0324, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.1113, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0684, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.2368, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.0685, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.2203, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.1424, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.2050, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.4778, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 0.4599, Accuracy: 0.8125\n",
      "Batch number: 153, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.3162, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.1262, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.3124, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0830, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.4448, Accuracy: 0.8125\n",
      "Batch number: 159, Training: Loss: 0.0027, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.1559, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 162, Training: Loss: 0.1970, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.4709, Accuracy: 0.8125\n",
      "Batch number: 164, Training: Loss: 0.0231, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0336, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0041, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0382, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.1308, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.3634, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.2355, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.2112, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1572, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.0071, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.2702, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.4028, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.2835, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.0615, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.1532, Accuracy: 0.9375\n",
      "Batch number: 185, Training: Loss: 0.0641, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.0959, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.1350, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0595, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0462, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1612, Accuracy: 0.8750\n",
      "Batch number: 192, Training: Loss: 0.2151, Accuracy: 0.8750\n",
      "Batch number: 193, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 194, Training: Loss: 0.2242, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0074, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.1440, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.0580, Accuracy: 1.0000\n",
      "Epoch : 045, Training: Loss: 0.1285, Accuracy: 95.1073%, \n",
      "\t\tValidation : Loss : 0.2202, Accuracy: 92.9293%, Time: 34.7059s\n",
      "Epoch: 46/50\n",
      "Batch number: 000, Training: Loss: 0.1830, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.2055, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0161, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0339, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0177, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0376, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 0.1092, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.3184, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.1507, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.2564, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1008, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.2347, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0494, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.1007, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0522, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0590, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0584, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0181, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.1754, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.0844, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.1829, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.2742, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0036, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.1198, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.1927, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.3001, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.1207, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0252, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0586, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.3068, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.2711, Accuracy: 0.8125\n",
      "Batch number: 036, Training: Loss: 0.0370, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.1312, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 0.3610, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 0.0032, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0080, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.0056, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.1631, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.1170, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.0002, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.2568, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.2010, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.3220, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.2622, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0074, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.2776, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.2441, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1493, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.1717, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.0394, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 0.0374, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0631, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.1922, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.2012, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 0.1683, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.1171, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.0101, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 0.3312, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0036, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0947, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1841, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.6668, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.3206, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 0.5028, Accuracy: 0.7500\n",
      "Batch number: 075, Training: Loss: 0.1038, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.1659, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.2155, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 0.1249, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.4334, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 0.0049, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0759, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 0.1203, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.2062, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.0772, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 0.1458, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0671, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 0.0347, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.0189, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.0756, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.0724, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0655, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.1279, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 0.1583, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.1244, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.1446, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 0.0256, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.1446, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.5825, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 0.0249, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.1609, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0876, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0028, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.0478, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 0.0519, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0088, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1418, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.1015, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.3017, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 0.0407, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.0306, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.1896, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0680, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.0045, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.4044, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 0.2905, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 0.0950, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.1141, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.2267, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.9235, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.2176, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.1156, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 0.1235, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.3579, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.0724, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0833, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.1088, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.1812, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.1587, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.1282, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.3781, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 0.0828, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0218, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0163, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.2647, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.2076, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.3660, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.1762, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 0.0424, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.1538, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.1311, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 0.0791, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0065, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.0746, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.2220, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.0152, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0246, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.1130, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0874, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.2233, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.2078, Accuracy: 0.8750\n",
      "Batch number: 163, Training: Loss: 0.3224, Accuracy: 0.8750\n",
      "Batch number: 164, Training: Loss: 0.1070, Accuracy: 0.9375\n",
      "Batch number: 165, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0298, Accuracy: 1.0000\n",
      "Batch number: 169, Training: Loss: 0.0075, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0802, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0495, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.6381, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.0757, Accuracy: 1.0000\n",
      "Batch number: 174, Training: Loss: 0.3969, Accuracy: 0.8750\n",
      "Batch number: 175, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.2265, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.2138, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.2647, Accuracy: 0.8125\n",
      "Batch number: 179, Training: Loss: 0.2877, Accuracy: 0.8750\n",
      "Batch number: 180, Training: Loss: 0.0248, Accuracy: 1.0000\n",
      "Batch number: 181, Training: Loss: 0.0026, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.2995, Accuracy: 0.8750\n",
      "Batch number: 183, Training: Loss: 0.0071, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0229, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0250, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0498, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.0475, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.0987, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0725, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.0270, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.1302, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0890, Accuracy: 0.9375\n",
      "Batch number: 196, Training: Loss: 0.0442, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.2245, Accuracy: 0.8125\n",
      "Epoch : 046, Training: Loss: 0.1268, Accuracy: 95.2020%, \n",
      "\t\tValidation : Loss : 0.0661, Accuracy: 98.2323%, Time: 34.6806s\n",
      "Epoch: 47/50\n",
      "Batch number: 000, Training: Loss: 0.1547, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0101, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.0224, Accuracy: 1.0000\n",
      "Batch number: 003, Training: Loss: 0.0247, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.1736, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.0246, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.2657, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 0.1663, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 0.0102, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.1864, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.0124, Accuracy: 1.0000\n",
      "Batch number: 014, Training: Loss: 0.0093, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0081, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0783, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.3123, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.1009, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0947, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 0.0393, Accuracy: 1.0000\n",
      "Batch number: 022, Training: Loss: 0.1041, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.0456, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.0063, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.2244, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.0016, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 0.0606, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.1987, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.3660, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0217, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0075, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0417, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.2047, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 037, Training: Loss: 0.0979, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0486, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0327, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.0677, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.1017, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0165, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.0456, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.1963, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 0.0266, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.1710, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 0.2638, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.3216, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0190, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.0240, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.9600, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 055, Training: Loss: 0.3607, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.2685, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0004, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.1172, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 0.6553, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0922, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0832, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.2140, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.4982, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.4686, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0301, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.3440, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.2761, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.1103, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 0.2896, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.1319, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.2347, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 0.1754, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0920, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0408, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.1992, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0366, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.2078, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 0.0423, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0259, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0950, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Batch number: 084, Training: Loss: 0.1789, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.1401, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0799, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 0.1165, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0868, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.2304, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 0.0101, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0013, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0107, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.6235, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 0.1909, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0131, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.1813, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.0884, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0341, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0233, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.2561, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.2815, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.2323, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.0582, Accuracy: 1.0000\n",
      "Batch number: 108, Training: Loss: 0.4307, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.4892, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 0.0362, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.0822, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0679, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.5257, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.4922, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.3307, Accuracy: 0.8125\n",
      "Batch number: 122, Training: Loss: 0.0693, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.0502, Accuracy: 1.0000\n",
      "Batch number: 124, Training: Loss: 0.0290, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0426, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.1433, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 0.0085, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0618, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.1016, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.1447, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 0.0073, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0764, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0273, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0050, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.1147, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 0.1564, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0099, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.5511, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0448, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.3346, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 0.1632, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.2189, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 0.0292, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.2250, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.0091, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0794, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0910, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 0.2659, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 0.0419, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.1825, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.2278, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.1434, Accuracy: 0.9375\n",
      "Batch number: 156, Training: Loss: 0.3028, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.2128, Accuracy: 0.9375\n",
      "Batch number: 158, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.1109, Accuracy: 0.9375\n",
      "Batch number: 160, Training: Loss: 0.1788, Accuracy: 0.8750\n",
      "Batch number: 161, Training: Loss: 0.2556, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0376, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0235, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.0830, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.5706, Accuracy: 0.8750\n",
      "Batch number: 166, Training: Loss: 0.1167, Accuracy: 0.9375\n",
      "Batch number: 167, Training: Loss: 0.1599, Accuracy: 0.8750\n",
      "Batch number: 168, Training: Loss: 0.0607, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0631, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0795, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.1059, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.2254, Accuracy: 0.8750\n",
      "Batch number: 174, Training: Loss: 0.0479, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.2255, Accuracy: 0.8125\n",
      "Batch number: 176, Training: Loss: 0.0832, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.3622, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.6291, Accuracy: 0.8125\n",
      "Batch number: 179, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2148, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.5876, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.0461, Accuracy: 1.0000\n",
      "Batch number: 183, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.0981, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0747, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.1735, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.0426, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0365, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.3918, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1516, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.4866, Accuracy: 0.8750\n",
      "Batch number: 196, Training: Loss: 0.4346, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.0701, Accuracy: 0.9375\n",
      "Epoch : 047, Training: Loss: 0.1339, Accuracy: 95.5808%, \n",
      "\t\tValidation : Loss : 0.1057, Accuracy: 97.2222%, Time: 35.2834s\n",
      "Epoch: 48/50\n",
      "Batch number: 000, Training: Loss: 0.5465, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 0.0175, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.2161, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.1239, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 0.0983, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 0.0916, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0530, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0529, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.5296, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.1114, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 0.1158, Accuracy: 0.9375\n",
      "Batch number: 014, Training: Loss: 0.3511, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.1610, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 0.4008, Accuracy: 0.8125\n",
      "Batch number: 019, Training: Loss: 0.0115, Accuracy: 1.0000\n",
      "Batch number: 020, Training: Loss: 0.0759, Accuracy: 1.0000\n",
      "Batch number: 021, Training: Loss: 0.5405, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 0.4543, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.1692, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.2218, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0477, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.2121, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 0.0543, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.2145, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 0.3779, Accuracy: 0.8125\n",
      "Batch number: 030, Training: Loss: 0.4811, Accuracy: 0.8125\n",
      "Batch number: 031, Training: Loss: 0.0922, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.1449, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.0795, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.0947, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.2383, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 0.0026, Accuracy: 1.0000\n",
      "Batch number: 038, Training: Loss: 0.0974, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.1127, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.0274, Accuracy: 1.0000\n",
      "Batch number: 041, Training: Loss: 0.2022, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 043, Training: Loss: 0.6358, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0470, Accuracy: 1.0000\n",
      "Batch number: 046, Training: Loss: 0.0257, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.3430, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.0077, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.1091, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 051, Training: Loss: 0.1256, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 0.0083, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 0.0134, Accuracy: 1.0000\n",
      "Batch number: 054, Training: Loss: 0.1046, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.3676, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.2593, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 0.1178, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 0.0275, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0426, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 0.0168, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.2095, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.4173, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.0986, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0094, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 0.0163, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0066, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 0.0295, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0384, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.2123, Accuracy: 0.9375\n",
      "Batch number: 072, Training: Loss: 0.1396, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 074, Training: Loss: 0.2367, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 0.0956, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.0567, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.3048, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.0458, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0772, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 0.0442, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0411, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.1232, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.6075, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.2735, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.4211, Accuracy: 0.8125\n",
      "Batch number: 087, Training: Loss: 0.0235, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.2995, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 0.3578, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 0.0555, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0395, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0191, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.2499, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.2476, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0023, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0838, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 0.1464, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.4285, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.0572, Accuracy: 1.0000\n",
      "Batch number: 101, Training: Loss: 0.0738, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0156, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0076, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.1811, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.1036, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0797, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.3339, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0846, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0756, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0730, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.1997, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 0.1611, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.1321, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 0.0697, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.1736, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 0.0001, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0258, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.1583, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 0.0022, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.1253, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 0.1127, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.0018, Accuracy: 1.0000\n",
      "Batch number: 125, Training: Loss: 0.0107, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0353, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.0697, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.0432, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 0.0563, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 0.0043, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0279, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0830, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 0.0948, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.3190, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 0.5525, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 0.0920, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0381, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.1802, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 0.1394, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0987, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.0558, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.1565, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.0310, Accuracy: 1.0000\n",
      "Batch number: 144, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 145, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.0122, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.3982, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.0420, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.2386, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.2431, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 0.0468, Accuracy: 1.0000\n",
      "Batch number: 152, Training: Loss: 0.1314, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 0.1039, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.1749, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 0.0231, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0040, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0644, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.1801, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0085, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0019, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.1851, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0630, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.0176, Accuracy: 1.0000\n",
      "Batch number: 164, Training: Loss: 0.5422, Accuracy: 0.8750\n",
      "Batch number: 165, Training: Loss: 0.0018, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0191, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0338, Accuracy: 1.0000\n",
      "Batch number: 168, Training: Loss: 0.0638, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0283, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.7316, Accuracy: 0.8750\n",
      "Batch number: 171, Training: Loss: 0.5748, Accuracy: 0.8125\n",
      "Batch number: 172, Training: Loss: 0.6071, Accuracy: 0.8125\n",
      "Batch number: 173, Training: Loss: 0.5045, Accuracy: 0.8125\n",
      "Batch number: 174, Training: Loss: 0.1064, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.1739, Accuracy: 0.9375\n",
      "Batch number: 177, Training: Loss: 0.1505, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.0465, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.2783, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.2161, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.1041, Accuracy: 0.9375\n",
      "Batch number: 182, Training: Loss: 0.2213, Accuracy: 0.8125\n",
      "Batch number: 183, Training: Loss: 0.0083, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.0153, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.0146, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.0207, Accuracy: 1.0000\n",
      "Batch number: 187, Training: Loss: 0.1218, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.0014, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.1240, Accuracy: 0.8750\n",
      "Batch number: 190, Training: Loss: 0.1122, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.1835, Accuracy: 0.9375\n",
      "Batch number: 192, Training: Loss: 0.3094, Accuracy: 0.9375\n",
      "Batch number: 193, Training: Loss: 0.0812, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.0225, Accuracy: 1.0000\n",
      "Batch number: 195, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0142, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.3154, Accuracy: 0.9375\n",
      "Epoch : 048, Training: Loss: 0.1350, Accuracy: 95.2652%, \n",
      "\t\tValidation : Loss : 0.1563, Accuracy: 94.9495%, Time: 35.4494s\n",
      "Epoch: 49/50\n",
      "Batch number: 000, Training: Loss: 0.1854, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 0.0639, Accuracy: 1.0000\n",
      "Batch number: 002, Training: Loss: 0.2384, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.7772, Accuracy: 0.8125\n",
      "Batch number: 006, Training: Loss: 0.2685, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0895, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.0796, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.2239, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 0.1029, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0354, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.4630, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 0.0162, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.1879, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.3900, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.2031, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 0.0383, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.1592, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 0.5770, Accuracy: 0.7500\n",
      "Batch number: 021, Training: Loss: 0.5137, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 0.3153, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.0721, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 0.1315, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 0.0552, Accuracy: 1.0000\n",
      "Batch number: 026, Training: Loss: 0.2625, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.3875, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0163, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.3577, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 0.0677, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 0.2095, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 0.4810, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 0.0565, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1964, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 0.1646, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 0.0350, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 0.0317, Accuracy: 1.0000\n",
      "Batch number: 040, Training: Loss: 0.2577, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.0493, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 0.3509, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.1745, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.1753, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0316, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.5067, Accuracy: 0.7500\n",
      "Batch number: 049, Training: Loss: 0.0045, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 0.1535, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0154, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.1359, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.1181, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.1146, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0715, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.0973, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 0.0147, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0040, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.3555, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.1087, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.1026, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.0003, Accuracy: 1.0000\n",
      "Batch number: 065, Training: Loss: 0.1416, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0616, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0975, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.0814, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0267, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0721, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.1558, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.0252, Accuracy: 1.0000\n",
      "Batch number: 073, Training: Loss: 0.0595, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.1444, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 0.0734, Accuracy: 0.9375\n",
      "Batch number: 076, Training: Loss: 0.0937, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 0.0161, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 0.2431, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 0.1121, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 0.0379, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0137, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0443, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.2601, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 0.0785, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0133, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0269, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.1714, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.2316, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0291, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0433, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0024, Accuracy: 1.0000\n",
      "Batch number: 094, Training: Loss: 0.0278, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 0.0127, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 0.2654, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0044, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.0203, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.0131, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.1155, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0789, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 0.0073, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.0124, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 0.2756, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 0.3067, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.2443, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0447, Accuracy: 1.0000\n",
      "Batch number: 109, Training: Loss: 0.0583, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0192, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.4873, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.2242, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.0360, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 0.1551, Accuracy: 0.9375\n",
      "Batch number: 116, Training: Loss: 0.3037, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 0.0495, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0481, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.4847, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 0.0808, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0170, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Batch number: 123, Training: Loss: 0.0482, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.2198, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.0169, Accuracy: 1.0000\n",
      "Batch number: 127, Training: Loss: 0.2183, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.0929, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.1486, Accuracy: 0.8750\n",
      "Batch number: 130, Training: Loss: 0.0884, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 0.0105, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.1676, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0581, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.1765, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0566, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0263, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.3209, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 0.2016, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 142, Training: Loss: 0.0038, Accuracy: 1.0000\n",
      "Batch number: 143, Training: Loss: 0.1044, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 0.0739, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.0361, Accuracy: 1.0000\n",
      "Batch number: 146, Training: Loss: 0.1866, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 0.2182, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 0.0057, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0537, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.1136, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 0.1098, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0152, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 0.0805, Accuracy: 1.0000\n",
      "Batch number: 155, Training: Loss: 0.0238, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.1535, Accuracy: 0.8750\n",
      "Batch number: 157, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.0375, Accuracy: 1.0000\n",
      "Batch number: 159, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0518, Accuracy: 0.9375\n",
      "Batch number: 161, Training: Loss: 0.3271, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.1076, Accuracy: 0.9375\n",
      "Batch number: 163, Training: Loss: 0.3214, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.2841, Accuracy: 0.8125\n",
      "Batch number: 165, Training: Loss: 0.0042, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 167, Training: Loss: 0.0845, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.1191, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.2339, Accuracy: 0.9375\n",
      "Batch number: 170, Training: Loss: 0.0313, Accuracy: 1.0000\n",
      "Batch number: 171, Training: Loss: 0.0075, Accuracy: 1.0000\n",
      "Batch number: 172, Training: Loss: 0.2356, Accuracy: 0.9375\n",
      "Batch number: 173, Training: Loss: 0.3062, Accuracy: 0.8750\n",
      "Batch number: 174, Training: Loss: 0.0646, Accuracy: 1.0000\n",
      "Batch number: 175, Training: Loss: 0.0045, Accuracy: 1.0000\n",
      "Batch number: 176, Training: Loss: 0.3526, Accuracy: 0.8750\n",
      "Batch number: 177, Training: Loss: 0.0843, Accuracy: 0.9375\n",
      "Batch number: 178, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.1528, Accuracy: 0.9375\n",
      "Batch number: 180, Training: Loss: 0.0895, Accuracy: 0.9375\n",
      "Batch number: 181, Training: Loss: 0.0363, Accuracy: 1.0000\n",
      "Batch number: 182, Training: Loss: 0.1058, Accuracy: 0.9375\n",
      "Batch number: 183, Training: Loss: 0.0645, Accuracy: 0.9375\n",
      "Batch number: 184, Training: Loss: 0.0805, Accuracy: 1.0000\n",
      "Batch number: 185, Training: Loss: 0.1652, Accuracy: 0.9375\n",
      "Batch number: 186, Training: Loss: 0.2305, Accuracy: 0.9375\n",
      "Batch number: 187, Training: Loss: 0.1285, Accuracy: 0.9375\n",
      "Batch number: 188, Training: Loss: 0.1976, Accuracy: 0.9375\n",
      "Batch number: 189, Training: Loss: 0.0580, Accuracy: 1.0000\n",
      "Batch number: 190, Training: Loss: 0.5042, Accuracy: 0.8750\n",
      "Batch number: 191, Training: Loss: 0.0418, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.0212, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.1767, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.1793, Accuracy: 0.8750\n",
      "Batch number: 195, Training: Loss: 0.0358, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.0199, Accuracy: 1.0000\n",
      "Batch number: 197, Training: Loss: 0.0853, Accuracy: 1.0000\n",
      "Epoch : 049, Training: Loss: 0.1234, Accuracy: 95.3914%, \n",
      "\t\tValidation : Loss : 0.0972, Accuracy: 97.2222%, Time: 43.8893s\n",
      "Epoch: 50/50\n",
      "Batch number: 000, Training: Loss: 0.0222, Accuracy: 1.0000\n",
      "Batch number: 001, Training: Loss: 0.1835, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.0844, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 0.0327, Accuracy: 1.0000\n",
      "Batch number: 004, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 005, Training: Loss: 0.0719, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.1397, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 0.0511, Accuracy: 1.0000\n",
      "Batch number: 008, Training: Loss: 0.0546, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 0.0095, Accuracy: 1.0000\n",
      "Batch number: 010, Training: Loss: 0.0196, Accuracy: 1.0000\n",
      "Batch number: 011, Training: Loss: 0.3813, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0219, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 0.1943, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 0.1172, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 017, Training: Loss: 0.2394, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.0270, Accuracy: 1.0000\n",
      "Batch number: 019, Training: Loss: 0.2408, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 0.3354, Accuracy: 0.8125\n",
      "Batch number: 021, Training: Loss: 0.6803, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 0.1707, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 0.1196, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 0.0060, Accuracy: 1.0000\n",
      "Batch number: 025, Training: Loss: 0.2128, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 0.2160, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.5358, Accuracy: 0.8125\n",
      "Batch number: 028, Training: Loss: 0.2237, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 0.2853, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 0.0901, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0027, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 033, Training: Loss: 0.0005, Accuracy: 1.0000\n",
      "Batch number: 034, Training: Loss: 0.1303, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 0.0048, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 0.1838, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.5413, Accuracy: 0.7500\n",
      "Batch number: 038, Training: Loss: 0.1472, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 0.1675, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 0.2537, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.2099, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 0.2965, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.0885, Accuracy: 1.0000\n",
      "Batch number: 044, Training: Loss: 0.0352, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 0.0767, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 0.0051, Accuracy: 1.0000\n",
      "Batch number: 047, Training: Loss: 0.0334, Accuracy: 1.0000\n",
      "Batch number: 048, Training: Loss: 0.0317, Accuracy: 1.0000\n",
      "Batch number: 049, Training: Loss: 0.0890, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 0.0916, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.1222, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 0.1138, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 0.0966, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 0.0964, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 0.0136, Accuracy: 1.0000\n",
      "Batch number: 057, Training: Loss: 0.0185, Accuracy: 1.0000\n",
      "Batch number: 058, Training: Loss: 0.0037, Accuracy: 1.0000\n",
      "Batch number: 059, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 060, Training: Loss: 0.1578, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 0.0047, Accuracy: 1.0000\n",
      "Batch number: 062, Training: Loss: 0.0020, Accuracy: 1.0000\n",
      "Batch number: 063, Training: Loss: 0.1081, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 0.2347, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 0.1802, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.1600, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0252, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 0.0300, Accuracy: 1.0000\n",
      "Batch number: 071, Training: Loss: 0.0428, Accuracy: 1.0000\n",
      "Batch number: 072, Training: Loss: 0.3053, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 0.0899, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 0.0922, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 0.0434, Accuracy: 1.0000\n",
      "Batch number: 076, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 077, Training: Loss: 0.3621, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 081, Training: Loss: 0.0676, Accuracy: 1.0000\n",
      "Batch number: 082, Training: Loss: 0.0126, Accuracy: 1.0000\n",
      "Batch number: 083, Training: Loss: 0.4350, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 0.1437, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 0.1264, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 0.0349, Accuracy: 1.0000\n",
      "Batch number: 087, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 089, Training: Loss: 0.2377, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 0.0030, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 0.0462, Accuracy: 1.0000\n",
      "Batch number: 092, Training: Loss: 0.0364, Accuracy: 1.0000\n",
      "Batch number: 093, Training: Loss: 0.0787, Accuracy: 0.9375\n",
      "Batch number: 094, Training: Loss: 0.2021, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 0.1022, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.0166, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 0.0002, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 0.6220, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.3612, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 0.4559, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.0331, Accuracy: 1.0000\n",
      "Batch number: 102, Training: Loss: 0.0029, Accuracy: 1.0000\n",
      "Batch number: 103, Training: Loss: 0.2440, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.1938, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 0.0950, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 0.1548, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 0.0736, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.1116, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0242, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 0.0069, Accuracy: 1.0000\n",
      "Batch number: 112, Training: Loss: 0.1489, Accuracy: 1.0000\n",
      "Batch number: 113, Training: Loss: 0.0065, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 0.2335, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.0307, Accuracy: 1.0000\n",
      "Batch number: 116, Training: Loss: 0.0035, Accuracy: 1.0000\n",
      "Batch number: 117, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Batch number: 118, Training: Loss: 0.0414, Accuracy: 1.0000\n",
      "Batch number: 119, Training: Loss: 0.0847, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0511, Accuracy: 1.0000\n",
      "Batch number: 121, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 122, Training: Loss: 0.2271, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.3584, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 0.2782, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 0.4760, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 128, Training: Loss: 0.5637, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.1437, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 0.1309, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.6062, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 0.0380, Accuracy: 1.0000\n",
      "Batch number: 133, Training: Loss: 0.1391, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.0490, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 0.0665, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 0.2211, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 138, Training: Loss: 0.0111, Accuracy: 1.0000\n",
      "Batch number: 139, Training: Loss: 0.0007, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Batch number: 141, Training: Loss: 0.0809, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 0.1038, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 0.2625, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.1279, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 0.7513, Accuracy: 0.8125\n",
      "Batch number: 146, Training: Loss: 0.0139, Accuracy: 1.0000\n",
      "Batch number: 147, Training: Loss: 0.1290, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 0.0526, Accuracy: 1.0000\n",
      "Batch number: 149, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 0.4508, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.0675, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 0.0278, Accuracy: 1.0000\n",
      "Batch number: 153, Training: Loss: 0.0613, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 0.3812, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 0.0194, Accuracy: 1.0000\n",
      "Batch number: 156, Training: Loss: 0.0105, Accuracy: 1.0000\n",
      "Batch number: 157, Training: Loss: 0.0277, Accuracy: 1.0000\n",
      "Batch number: 158, Training: Loss: 0.1630, Accuracy: 0.9375\n",
      "Batch number: 159, Training: Loss: 0.0504, Accuracy: 1.0000\n",
      "Batch number: 160, Training: Loss: 0.0052, Accuracy: 1.0000\n",
      "Batch number: 161, Training: Loss: 0.0739, Accuracy: 0.9375\n",
      "Batch number: 162, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 163, Training: Loss: 0.1948, Accuracy: 0.9375\n",
      "Batch number: 164, Training: Loss: 0.0143, Accuracy: 1.0000\n",
      "Batch number: 165, Training: Loss: 0.0512, Accuracy: 1.0000\n",
      "Batch number: 166, Training: Loss: 0.3148, Accuracy: 0.8750\n",
      "Batch number: 167, Training: Loss: 0.4153, Accuracy: 0.9375\n",
      "Batch number: 168, Training: Loss: 0.2399, Accuracy: 0.9375\n",
      "Batch number: 169, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 170, Training: Loss: 0.0665, Accuracy: 0.9375\n",
      "Batch number: 171, Training: Loss: 0.1496, Accuracy: 0.8125\n",
      "Batch number: 172, Training: Loss: 0.2812, Accuracy: 0.8750\n",
      "Batch number: 173, Training: Loss: 0.1548, Accuracy: 0.9375\n",
      "Batch number: 174, Training: Loss: 0.0897, Accuracy: 0.9375\n",
      "Batch number: 175, Training: Loss: 0.2406, Accuracy: 0.9375\n",
      "Batch number: 176, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 177, Training: Loss: 0.2097, Accuracy: 0.8750\n",
      "Batch number: 178, Training: Loss: 0.0411, Accuracy: 1.0000\n",
      "Batch number: 179, Training: Loss: 0.0025, Accuracy: 1.0000\n",
      "Batch number: 180, Training: Loss: 0.2150, Accuracy: 0.8750\n",
      "Batch number: 181, Training: Loss: 0.3168, Accuracy: 0.8750\n",
      "Batch number: 182, Training: Loss: 0.6808, Accuracy: 0.7500\n",
      "Batch number: 183, Training: Loss: 0.0147, Accuracy: 1.0000\n",
      "Batch number: 184, Training: Loss: 0.3404, Accuracy: 0.8750\n",
      "Batch number: 185, Training: Loss: 0.0223, Accuracy: 1.0000\n",
      "Batch number: 186, Training: Loss: 0.5844, Accuracy: 0.8750\n",
      "Batch number: 187, Training: Loss: 0.0056, Accuracy: 1.0000\n",
      "Batch number: 188, Training: Loss: 0.0110, Accuracy: 1.0000\n",
      "Batch number: 189, Training: Loss: 0.1122, Accuracy: 0.9375\n",
      "Batch number: 190, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 191, Training: Loss: 0.0680, Accuracy: 1.0000\n",
      "Batch number: 192, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 193, Training: Loss: 0.1072, Accuracy: 0.9375\n",
      "Batch number: 194, Training: Loss: 0.0797, Accuracy: 0.9375\n",
      "Batch number: 195, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 196, Training: Loss: 0.1869, Accuracy: 0.9375\n",
      "Batch number: 197, Training: Loss: 0.1271, Accuracy: 0.8750\n",
      "Epoch : 050, Training: Loss: 0.1299, Accuracy: 95.1389%, \n",
      "\t\tValidation : Loss : 0.0868, Accuracy: 97.2222%, Time: 36.5689s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File model_history.pt cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#trained_model, history = train_and_validate(alexnet, loss_func, optimizer, num_epochs)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m train_and_validate(alexnet, loss_func, optimizer, num_epochs)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_history.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFINALMODEL.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:525\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:496\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File model_history.pt cannot be opened."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 50\n",
    "#trained_model, history = train_and_validate(alexnet, loss_func, optimizer, num_epochs)\n",
    "trained_model, history = train_and_validate(alexnet, loss_func, optimizer, num_epochs)\n",
    "\n",
    "torch.save(history, 'model' +'_history.pt')\n",
    "torch.save(trained_model.state_dict(), 'model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a66c07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = np.array(history)\n",
    "plt.plot(history[:,0:2])\n",
    "plt.legend(['Tr Loss', 'Val Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "#plt.xlim(-1,num_epochs)\n",
    "plt.savefig('_loss_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d7ea38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49071163, 0.33163539, 0.80239899, 0.85353535],\n",
       "       [0.31791917, 0.30935048, 0.87247475, 0.87121212],\n",
       "       [0.25589338, 0.20259667, 0.89962121, 0.93181818]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "760f0425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHTUlEQVR4nO3deXgUVaL+8bc7S2chGwSyMJFFUBRZlCXCyIASjBsj/lyAKzsu4wjCMF4BZRGcEVRURkW8ctl0BgI4gs6geDGIC0YZgQgoAiIKahJAzL531++PJG2aJKQ7JHSn/H6epx+6T5+qPieV2K+nzqmyGIZhCAAAwCSs3m4AAABAYyLcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAU/FquPnggw80dOhQxcfHy2KxaNOmTfVus337dl1xxRWy2Wzq1KmTVq1a1eTtBAAAzYdXw01BQYF69OihJUuWuFX/6NGjuvHGG3X11VcrPT1dU6dO1V133aV33nmniVsKAACaC4uv3DjTYrFo48aNGjZsWJ11pk+frs2bN2v//v3OshEjRig7O1tbtmw5D60EAAC+zt/bDfBEWlqakpKSXMqSk5M1derUOrcpKSlRSUmJ87XD4dDp06fVqlUrWSyWpmoqAABoRIZhKC8vT/Hx8bJaz37iqVmFm8zMTMXExLiUxcTEKDc3V0VFRQoODq6xzYIFCzRv3rzz1UQAANCEjh8/rt/85jdnrdOswk1DzJw5U9OmTXO+zsnJ0QUXXKDjx48rPDzciy0DAADuys3NVUJCgsLCwuqt26zCTWxsrLKyslzKsrKyFB4eXuuojSTZbDbZbLYa5eHh4YQbAACaGXemlDSr69z069dPqampLmVbt25Vv379vNQiAADga7wabvLz85Wenq709HRJFUu909PTdezYMUkVp5TGjBnjrP+HP/xB33zzjR566CF99dVXevHFF7V+/Xr96U9/8kbzAQCAD/JquPnss890+eWX6/LLL5ckTZs2TZdffrnmzJkjScrIyHAGHUnq0KGDNm/erK1bt6pHjx56+umn9b//+79KTk72SvsBAIDv8Znr3Jwvubm5ioiIUE5ODnNuAABoJjz5/m5Wc24AAADqQ7gBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACm4u/tBgAAgGbGXibln5DyM6W8LCkvQ8qv/DcvS2rVSbruca81j3ADAAAq2MsqQ0pm5SPD9XV+5b8FpyQZde+n4MR5a3JtCDcAAJhdeUllSKlllKV6gCk85f4+rf5SixgpLFZqESuFxUhhcRVlUe2brCvuINwAANBclRX/cmqoalTlzFGWvEyp6LT7+7QGVAaWyuBS9WhxxvOQVpLVN6fuEm4AAPA1ZUVnhJQ6RlyKs93fp19g7aMsYXHVXsdKwVE+G1rcRbgBAOB8KS2oDCx1nBaqCjPFOe7v0z/INaRUH2GpPuISHCVZLE3XNx9CuAEA4FyV5FcLLLWcFqoKNCW57u/TP/gsp4WqhZmgyF9NaHEX4QYAgNoYhlSS5zrKUte8ltJ89/cbEHqW00LV5rnYwgktDUS4AQD8uhhGxQhKjZBSy7yWsgL39xsYdvbTQs7QEtZ0fYMkwg0AwCwMo2KCbX3LnfMypfIi9/drC695KsglsFSOuNhaNFnX4BnCDQDAtxmGVPRzHfNYzpjTUl7s/n6DIuoYZTnjFFFgaNP1DU2CcAMA8A7DkApPV46qnBFSnCMula/tJe7vNziqnuXOlaElILjp+gavItwAABqXwyEV/lT/KEtepuQoc3+/wS3rX+7cIkYKCGq6vpmY3WGosLRchaX2yke15yWVz8uqPa98v6jUrgKX53ZdEhumZ4b39FpfCDcAAPc47BX3FDrbReXysyoejnL39xsSXf9y5xYxkr+t6frWTBiGoZJyhzNcVIWJwtJyFZZUhI+i0nIVlNhVVGZXQYlrAKn+75khpqTc0WjtDPT37kUACTcA8GvnsEsFJ+tf7px/QjLsbu7UIoVG175aqPrr0DaSf2CTds8bqkZBqoePqudFlWGi6nlVEHEGlBoBxHU0xXGW+1U2Bj+rRSEBfgqx+Skk0F/BAX4KtfkpONBfoYF+Cg70U0ign0ID/RVc7d+QwIr6IYF+ahnq3WNKuAEAs7KXV9yduc7TQpVhpuCEZLj5f+0WqxTauv7L+Ie2lvwCmrZ/56hqFOTM0YyCWoKIS8goOeP0TC2nahpzFKQuNn+rQm0V4SMk0E8hNn+FVAsivwSUX0KH6/NqZZXbBgf6yeZvlaWZX1+HcAMAzY297Jc7POdn1rLcuSq0nJTk5v/mW6wVAaW+y/iHtpb8zu9Xh91hVIxslNQ2H6SOOSJnBJEzT9UUVYYSexMPg1gtqnVkoypMVDx3DRpVIyTV61eNkIQ4R0785Wdt3gGkKRFuAMBXlJf+MtE2v5bTQlUBpvAnuR9a/Oq5u3PVSEu0ZPVrcNMNw1Cp3VEjTNQ18bTwjAmoZztVU1x2fkZB6hrhOPPUS9VoSc0RkuqnbSq2NcMoSHNEuAGAplZe4t7NEgt/cn+f1oDKYFLPcueQaJc7PDscRsVplGrzO4pOl6sg43TF87LKYOHpqZrS8zMKEuKc51H3HJC6R0hqOzXjp+AAP/n7Ne+7YMMV4QYAGqqs2I3lzhkVF6BzlzVARlisjBYxKg+NUWlwG5XYWqswqLXy/aOVG9hKOX6tlG2EqbDMURk2KoPGqXIV/lgVOApVWHpYhaUHXILI+RgFCawcBXENG3XN+6j91EtVEKkYIWEUBJ4h3ADAmUoL61/unJchFee4vctyS6DyA1spxz9a2daWOm1tqVOK0gkjUhlGpDLKI3SsPEKZZcEqOulQeVZdoyD5lY9zY7Go8nRKzRBRFTjOHCEJOWO0pPpk1lBbZZBhFAQ+gHADwPRKyx0VS3ELclX6848qz8mQI7fiqrjW/Cz5F2YpoOikgotPKLjklILs7oeHYiNAWUaUTihSWUaUThoV/54wInVCUc7nOQqVitwZdXBdah3oZ62YzxHgGiiqh4mq51VBpHpAOXMOSFV5UACjIDAvwg0A3+ewV8xbsZdI5SUqKSnST9l5Ovlzrk7n5urnnDxl5xcoNy9PZUW5alH6k8LLf1Kk47RaOU6rtX5WG0u24izu3yyx0LDphBGpLLkGljODTK5CZLFUXBck2GXkoyJ0dAjwU9fqcz9qmffhcjqm2qhJSKCfAhgFATxGuAFQO8OoWHJcXizZSyvChfN5ccXKnsqw4f57JS4hpbbXhr1EjtJiGZWvrY5SWQ3Xq93aJMVXPup1RjYolE2nLC112tpKOX4tlRfQSgUBrVVki1ZxcBuVBbdRWUiMAoLDFWzzd56qSQjwUxebf40luaGMggA+h3AD+JozRilcA0BleKj+njNQ1BIcqoeNOrcrrSN8eHB35UZkkXS2Bcl2w6JSBahEAbJbA2W3Bkp+NinAJj//IFltobKHtqlY7hweK7/wOAVExskW1VYBEXEKsYXpAkkXnKf+ADj/CDeA5N4oRUPfqzWknOU9T+7Jcz75VYYI/8qHX6DkH1Rx6Xz/IMkvUHY/m4od/ip0+Cnf7q/8cqtyy/yUXWrRzyUW/VRsUb7dTyUKqAgoRsW/pfJ3KSuzBCg8NFSR4WGKCm+h1pERio4MU0zLCMW1DFN8ZLCiQgIYLQFQK8INvOusoxT1n8LwfHSjtI7w4Z1RinpZrK6Bwt9W+boyVNT6XlXYaIT3/H4JLuWGdCKvRBk5Rfohu1gZ2UX6MbtIP+YUKyOnSD9mF+t0Qalb3WoZGqi4iCDFRwYrvvLfuGrP24TZWHEDoMEIN79GNUYpiusJAGc7hXG2EQxzj1JUvBd0lvfOCCJne88lbFTb53m6zL1hGPq5sKwirJyqCC0ZOfn6Mae44nl2kbLySty6SFtIoF9FWIkIUtvIYMVFBCsusup5kOIighUc2PAr4QJAfQg351PVKMXZJlc2xihFjfdqmXfhi+ocpajl+bmORNQzSlH9iq5mUFBS7jriUhVaKkdcMnKK3Lq4m7/VotiIIMVHBCs+MqhitKVyxCUuIlhtI4MVHuzP6SIAXkW4aSyZ+6R//+nsIxi/plEKt0+lnP9RCrMpszuU6QwrxfqhMrRkZFc9L1ZOUZlb+4puYVPbyIqgEh9ZGWAqg0x8ZLCiW9i4WR8An8e3SWMpL5G+/4/79RmlgBscDkOnCkqUUTm68svIS8WIy4/ZRTqZXyLDjVv6hAX5K77yFJHLXJfKEZeYCJts/pwuAtD8EW4aS6sLpRFrGKWAR3KLy5RRGVIqAkvFiEtVeMnMKVapvf7TRYF+1orQ4jK/xXWuS1hQwHnoEQB4H9+wjSU4Supyo7dbAR9SXGavOF1UNa+llrku+SX1n6q0WKSYsCBneImv5bRRq9BAWTldBACSCDdAg9gdhk7mldQyv+WXCbqn8t1bFh0ZElB5aqj2uS4x4UFcgh8APEC4Ac5gGIZyisoqwkq1U0QZlaeNfswuVlZuscrdWBYdFGCtnN9Sc3Ju1fOQQP4MAaAx8V9V/OoUldprmd9SbaVRdrGKyuz17sfPalFseJDzYnQuc10qr/ESyVV0AeC8I9zAVMrtDmXllVSOsNQcccnIKdLPhe4ti24VGui8GJ3rqaKK523CglgWDQA+iHCDZsMwDP1UUOoyv+WX0ZaK8HIir1hunC1SaNVVdCPPmOsSUXFhuriIIAUFsCwaAJojwg18Rn5JuTKyi5wXnjtz5CUjp1gl5fUviw7wq34V3drnuoQHcRVdADArwg3Oi9JyR7Vl0dXDyy+jL3nF7l3BuU2YzeUmi857GFUGmehQG8uiAeBXjHCDc+ZwGDqVX1LniMuPOcU65eZVdMOD/CtHW2rOdWkbGayY8CAF+rMsGgBQN8INzsowDOUWlzsvPFd1+f/q9zDKzClWmb3+5BLob3W5M3TbM2+8GBmsFjZ+JQEA54Zvkl+54jJ7jVNEP1a/km52kQpK618WbbVIMdWWRVcPLFXXeGkZGsg8FwBAkyPcmJjdYehE3i+niVzDS8X1XH4qcO8qulEhAdVusvjLiqKquS4xYTb5cxVdAIAPINw0U4Zh6OfCMtcRl+r3MMouUlZeiexurIsODvBzriRyvWt0sPN+RsGBLIsGADQPhBsfVVBS7nKfoh+cN1785aq6xWX1L4v2t1oUE141whL0y+miasukI4K5ii4AwDy8Hm6WLFmip556SpmZmerRo4eef/559e3bt876ixcv1tKlS3Xs2DFFR0frtttu04IFCxQUFHQeW31uyuwVy6LPPEVUfa5LTpF7V9GNbmGrXE30y2hL1a0A4iOC1TrMxlV0AQC/Kl4NN+vWrdO0adP00ksvKTExUYsXL1ZycrIOHjyoNm3a1Ki/Zs0azZgxQytWrFD//v116NAhjRs3ThaLRc8884wXelCTw1FxFd2q1UXV57pUrS46kefesugwm79ztKX6XaOr7mEUE85VdAEAOJPFMNz5mm0aiYmJ6tOnj1544QVJksPhUEJCgiZPnqwZM2bUqD9p0iQdOHBAqampzrI///nP+vTTT/XRRx+59Zm5ubmKiIhQTk6OwsPDG6cjkj779rT+vOFzZWQXq9Re/+miQD+r4qpGXKqPtlSb6xIeFNBo7QMAoDnz5PvbayM3paWl2rVrl2bOnOkss1qtSkpKUlpaWq3b9O/fX3//+9+1c+dO9e3bV998843eeustjR49us7PKSkpUUlJifN1bm5u43WimqAAP333U6EkyWKpvIpu5YXnarvxYqvQQK6iCwBAE/BauDl16pTsdrtiYmJcymNiYvTVV1/Vus1//dd/6dSpU7rqqqtkGIbKy8v1hz/8QQ8//HCdn7NgwQLNmzevUdtem05tWmj9vf0UFxHEVXQBAPCiZvUNvH37dj3++ON68cUXtXv3br3++uvavHmzHnvssTq3mTlzpnJycpyP48ePN0nbggL81LdDSyW0DCHYAADgRV4buYmOjpafn5+ysrJcyrOyshQbG1vrNrNnz9bo0aN11113SZK6deumgoIC3XPPPXrkkUdktdYMFTabTTabrfE7AAAAfJLXhhgCAwPVq1cvl8nBDodDqamp6tevX63bFBYW1ggwfn4Vq4W8OC8aAAD4EK8uBZ82bZrGjh2r3r17q2/fvlq8eLEKCgo0fvx4SdKYMWPUtm1bLViwQJI0dOhQPfPMM7r88suVmJior7/+WrNnz9bQoUOdIQcAAPy6eTXcDB8+XCdPntScOXOUmZmpnj17asuWLc5JxseOHXMZqZk1a5YsFotmzZqlH374Qa1bt9bQoUP117/+1VtdAAAAPsar17nxhqa6zg0AAGg6nnx/s6wHAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYitfDzZIlS9S+fXsFBQUpMTFRO3fuPGv97Oxs3X///YqLi5PNZtNFF12kt9566zy1FgAA+Dp/b374unXrNG3aNL300ktKTEzU4sWLlZycrIMHD6pNmzY16peWlmrIkCFq06aNXnvtNbVt21bfffedIiMjz3/jAQCAT7IYhmF468MTExPVp08fvfDCC5Ikh8OhhIQETZ48WTNmzKhR/6WXXtJTTz2lr776SgEBAQ36zNzcXEVERCgnJ0fh4eHn1H4AAHB+ePL97bXTUqWlpdq1a5eSkpJ+aYzVqqSkJKWlpdW6zZtvvql+/frp/vvvV0xMjC677DI9/vjjstvtdX5OSUmJcnNzXR4AAMC8vBZuTp06JbvdrpiYGJfymJgYZWZm1rrNN998o9dee012u11vvfWWZs+eraefflp/+ctf6vycBQsWKCIiwvlISEho1H4AAADf4vUJxZ5wOBxq06aNXn75ZfXq1UvDhw/XI488opdeeqnObWbOnKmcnBzn4/jx4+exxQAA4Hzz2oTi6Oho+fn5KSsry6U8KytLsbGxtW4TFxengIAA+fn5OcsuueQSZWZmqrS0VIGBgTW2sdlsstlsjdt4AADgs7w2chMYGKhevXopNTXVWeZwOJSamqp+/frVus1vf/tbff3113I4HM6yQ4cOKS4urtZgAwAAfn28elpq2rRpWrZsmVavXq0DBw7ovvvuU0FBgcaPHy9JGjNmjGbOnOmsf9999+n06dOaMmWKDh06pM2bN+vxxx/X/fff760uAAAAH+PV69wMHz5cJ0+e1Jw5c5SZmamePXtqy5YtzknGx44dk9X6S/5KSEjQO++8oz/96U/q3r272rZtqylTpmj69One6gIAAPAxXr3OjTdwnRsAAJqfZnGdGwAAgKbgcbhp37695s+fr2PHjjVFewAAAM6Jx+Fm6tSpev3119WxY0cNGTJEKSkpKikpaYq2AQAAeKxB4SY9PV07d+7UJZdcosmTJysuLk6TJk3S7t27m6KNAAAAbjvnCcVlZWV68cUXNX36dJWVlalbt2564IEHNH78eFkslsZqZ6NhQjEAAM2PJ9/fDV4KXlZWpo0bN2rlypXaunWrrrzySk2cOFHff/+9Hn74Yb377rtas2ZNQ3cPAADQIB6Hm927d2vlypVau3atrFarxowZo2effVZdunRx1rnlllvUp0+fRm0oAACAOzwON3369NGQIUO0dOlSDRs2TAEBATXqdOjQQSNGjGiUBgIAAHjC43DzzTffqF27dmetExoaqpUrVza4UQAAAA3l8WqpEydO6NNPP61R/umnn+qzzz5rlEYBAAA0lMfh5v7779fx48drlP/www/cwBIAAHidx+Hmyy+/1BVXXFGj/PLLL9eXX37ZKI0CAABoKI/Djc1mU1ZWVo3yjIwM+ft79SbjAAAAnoeba6+9VjNnzlROTo6zLDs7Ww8//LCGDBnSqI0DAADwlMdDLYsWLdLvfvc7tWvXTpdffrkkKT09XTExMXr11VcbvYEAAACe8DjctG3bVnv37tU//vEPff755woODtb48eM1cuTIWq95AwAAcD41aJJMaGio7rnnnsZuCwAAwDlr8AzgL7/8UseOHVNpaalL+e9///tzbhQAAEBDNegKxbfccov27dsni8WiqpuKV90B3G63N24LAQAAPODxaqkpU6aoQ4cOOnHihEJCQvTFF1/ogw8+UO/evbV9+/YmaCIAAID7PB65SUtL07Zt2xQdHS2r1Sqr1aqrrrpKCxYs0AMPPKA9e/Y0RTsBAADc4vHIjd1uV1hYmCQpOjpaP/74oySpXbt2OnjwYOO2DgAAwEMej9xcdtll+vzzz9WhQwclJibqySefVGBgoF5++WV17NixKdoIAADgNo/DzaxZs1RQUCBJmj9/vm666SYNGDBArVq10rp16xq9gQAAAJ6wGFXLnc7B6dOnFRUV5Vwx5ctyc3MVERGhnJwchYeHe7s5AADADZ58f3s056asrEz+/v7av3+/S3nLli2bRbABAADm51G4CQgI0AUXXMC1bAAAgM/yeLXUI488oocfflinT59uivYAAACcE48nFL/wwgv6+uuvFR8fr3bt2ik0NNTl/d27dzda4wAAADzlcbgZNmxYEzQDAACgcTTKaqnmhNVSAAA0P022WgoAAMDXeXxaymq1nnXZNyupAACAN3kcbjZu3OjyuqysTHv27NHq1as1b968RmsYAABAQzTanJs1a9Zo3bp1euONNxpjd02GOTcAADQ/Xplzc+WVVyo1NbWxdgcAANAgjRJuioqK9Nxzz6lt27aNsTsAAIAG83jOzZk3yDQMQ3l5eQoJCdHf//73Rm0cAACApzwON88++6xLuLFarWrdurUSExMVFRXVqI0DAADwlMfhZty4cU3QDAAAgMbh8ZyblStXasOGDTXKN2zYoNWrVzdKowAAABrK43CzYMECRUdH1yhv06aNHn/88UZpFAAAQEN5HG6OHTumDh061Chv166djh071iiNAgAAaCiPw02bNm20d+/eGuWff/65WrVq1SiNAgAAaCiPw83IkSP1wAMP6L333pPdbpfdbte2bds0ZcoUjRgxoinaCAAA4DaPV0s99thj+vbbbzV48GD5+1ds7nA4NGbMGObcAAAAr2vwvaUOHz6s9PR0BQcHq1u3bmrXrl1jt61JcG8pAACaH0++vz0euanSuXNnde7cuaGbAwAANAmP59zceuuteuKJJ2qUP/nkk7r99tsbpVEAAAAN5XG4+eCDD3TDDTfUKL/++uv1wQcfNEqjAAAAGsrjcJOfn6/AwMAa5QEBAcrNzW2URgEAADSUx+GmW7duWrduXY3ylJQUXXrppY3SKAAAgIbyeELx7Nmz9f/+3//TkSNHdM0110iSUlNTtWbNGr322muN3kAAAABPeBxuhg4dqk2bNunxxx/Xa6+9puDgYPXo0UPbtm1Ty5Ytm6KNAAAAbmvwdW6q5Obmau3atVq+fLl27dolu93eWG1rElznBgCA5seT72+P59xU+eCDDzR27FjFx8fr6aef1jXXXKNPPvmkobsDAABoFB6dlsrMzNSqVau0fPly5ebm6o477lBJSYk2bdrEZGIAAOAT3B65GTp0qC6++GLt3btXixcv1o8//qjnn3++KdsGAADgMbdHbt5++2098MADuu+++7jtAgAA8Fluj9x89NFHysvLU69evZSYmKgXXnhBp06dasq2AQAAeMztcHPllVdq2bJlysjI0L333quUlBTFx8fL4XBo69atysvLa8p2AgAAuOWcloIfPHhQy5cv16uvvqrs7GwNGTJEb775ZmO2r9GxFBwAgObnvCwFl6SLL75YTz75pL7//nutXbv2XHYFAADQKM4p3FTx8/PTsGHDGjxqs2TJErVv315BQUFKTEzUzp073douJSVFFotFw4YNa9DnAgAA82mUcHMu1q1bp2nTpmnu3LnavXu3evTooeTkZJ04ceKs23377bd68MEHNWDAgPPUUgAA0Bx4Pdw888wzuvvuuzV+/HhdeumleumllxQSEqIVK1bUuY3dbtedd96pefPmqWPHjuextQAAwNd5NdyUlpZq165dSkpKcpZZrVYlJSUpLS2tzu3mz5+vNm3aaOLEifV+RklJiXJzc10eAADAvLwabk6dOiW73a6YmBiX8piYGGVmZta6zUcffaTly5dr2bJlbn3GggULFBER4XwkJCScc7sBAIDv8vppKU/k5eVp9OjRWrZsmaKjo93aZubMmcrJyXE+jh8/3sStBAAA3uTRjTMbW3R0tPz8/JSVleVSnpWVpdjY2Br1jxw5om+//VZDhw51ljkcDkmSv7+/Dh48qAsvvNBlG5vNJpvN1gStBwAAvsirIzeBgYHq1auXUlNTnWUOh0Opqanq169fjfpdunTRvn37lJ6e7nz8/ve/19VXX6309HROOQEAAO+O3EjStGnTNHbsWPXu3Vt9+/bV4sWLVVBQoPHjx0uSxowZo7Zt22rBggUKCgrSZZdd5rJ9ZGSkJNUoBwAAv05eDzfDhw/XyZMnNWfOHGVmZqpnz57asmWLc5LxsWPHZLU2q6lBAADAi87p3lLNEfeWAgCg+Tlv95YCAADwNYQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKj4RbpYsWaL27dsrKChIiYmJ2rlzZ511ly1bpgEDBigqKkpRUVFKSko6a30AAPDr4vVws27dOk2bNk1z587V7t271aNHDyUnJ+vEiRO11t++fbtGjhyp9957T2lpaUpISNC1116rH3744Ty3HAAA+CKLYRiGNxuQmJioPn366IUXXpAkORwOJSQkaPLkyZoxY0a929vtdkVFRemFF17QmDFj6q2fm5uriIgI5eTkKDw8/JzbDwAAmp4n399eHbkpLS3Vrl27lJSU5CyzWq1KSkpSWlqaW/soLCxUWVmZWrZsWev7JSUlys3NdXkAAADz8mq4OXXqlOx2u2JiYlzKY2JilJmZ6dY+pk+frvj4eJeAVN2CBQsUERHhfCQkJJxzuwEAgO/y+pybc7Fw4UKlpKRo48aNCgoKqrXOzJkzlZOT43wcP378PLcSAACcT/7e/PDo6Gj5+fkpKyvLpTwrK0uxsbFn3XbRokVauHCh3n33XXXv3r3OejabTTabrVHaCwAAfJ9XR24CAwPVq1cvpaamOsscDodSU1PVr1+/Ord78skn9dhjj2nLli3q3bv3+WgqAABoJrw6ciNJ06ZN09ixY9W7d2/17dtXixcvVkFBgcaPHy9JGjNmjNq2basFCxZIkp544gnNmTNHa9asUfv27Z1zc1q0aKEWLVp4rR8AAMA3eD3cDB8+XCdPntScOXOUmZmpnj17asuWLc5JxseOHZPV+ssA09KlS1VaWqrbbrvNZT9z587Vo48+ej6bDgAAfJDXr3NzvnGdGwAAmp9mc50bAACAxka4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApuLv7Qb4KrvdrrKyMm83Az4oICBAfn5+3m4GAKAOhJszGIahzMxMZWdne7sp8GGRkZGKjY2VxWLxdlMAAGcg3JyhKti0adNGISEhfHnBhWEYKiws1IkTJyRJcXFxXm4RAOBMhJtq7Ha7M9i0atXK282BjwoODpYknThxQm3atOEUFQD4GCYUV1M1xyYkJMTLLYGvq/odYV4WAPgewk0tOBWF+vA7AgC+i3ADAABMhXADAABMhXBjAhaL5ayPRx991KP93XvvvfLz89OGDRuapsEAADQhVkuZQEZGhvP5unXrNGfOHB08eNBZ1qJFC+dzwzBkt9vl71/7oS8sLFRKSooeeughrVixQrfffnvTNdwNpaWlCgwM9GobAADNCyM39TAMQ4Wl5V55GIbhVhtjY2Odj4iICFksFufrr776SmFhYXr77bfVq1cv2Ww2ffTRR3Xua8OGDbr00ks1Y8YMffDBBzp+/LjL+yUlJZo+fboSEhJks9nUqVMnLV++3Pn+F198oZtuuknh4eEKCwvTgAEDdOTIEUnSoEGDNHXqVJf9DRs2TOPGjXO+bt++vR577DGNGTNG4eHhuueeeyRJ06dP10UXXaSQkBB17NhRs2fPrrFS6V//+pf69OmjoKAgRUdH65ZbbpEkzZ8/X5dddlmNvvbs2VOzZ8+u/wcMAGhWGLmpR1GZXZfOeccrn/3l/GSFBDbOIZoxY4YWLVqkjh07Kioqqs56y5cv16hRoxQREaHrr79eq1atcgkAY8aMUVpamp577jn16NFDR48e1alTpyRJP/zwg373u99p0KBB2rZtm8LDw7Vjxw6Vl5d71NZFixZpzpw5mjt3rrMsLCxMq1atUnx8vPbt26e7775bYWFheuihhyRJmzdv1i233KJHHnlEr7zyikpLS/XWW29JkiZMmKB58+bpP//5j/r06SNJ2rNnj/bu3avXX3/do7YBAHwf4eZXYv78+RoyZMhZ6xw+fFiffPKJ8wt/1KhRmjZtmmbNmiWLxaJDhw5p/fr12rp1q5KSkiRJHTt2dG6/ZMkSRUREKCUlRQEBAZKkiy66yOO2XnPNNfrzn//sUjZr1izn8/bt2+vBBx90nj6TpL/+9a8aMWKE5s2b56zXo0cPSdJvfvMbJScna+XKlc5ws3LlSg0cONCl/QAAcyDc1CM4wE9fzk/22mc3lt69e9dbZ8WKFUpOTlZ0dLQk6YYbbtDEiRO1bds2DR48WOnp6fLz89PAgQNr3T49PV0DBgxwBpvGbOu6dev03HPP6ciRI8rPz1d5ebnCw8NdPvvuu++uc5933323JkyYoGeeeUZWq1Vr1qzRs88+e07tBAD4JsJNPSwWS6OdGvKm0NDQs75vt9u1evVqZWZmukw2ttvtWrFihQYPHuy87UBd6nvfarXWmEdU2xV+z2xrWlqa7rzzTs2bN0/JycnO0aGnn37a7c8eOnSobDabNm7cqMDAQJWVlem222476zYAgOap+X9ro1G89dZbysvL0549e1zulbR//36NHz9e2dnZ6tatmxwOh95//33naanqunfvrtWrV6usrKzW0ZvWrVu7rOyy2+3av3+/rr766rO27eOPP1a7du30yCOPOMu+++67Gp+dmpqq8ePH17oPf39/jR07VitXrlRgYKBGjBhRbyACADRPrJaCpIqJxDfeeKN69Oihyy67zPm44447FBkZqX/84x9q3769xo4dqwkTJmjTpk06evSotm/frvXr10uSJk2apNzcXI0YMUKfffaZDh8+rFdffdW5LP2aa67R5s2btXnzZn311Ve67777lJ2dXW/bOnfurGPHjiklJUVHjhzRc889p40bN7rUmTt3rtauXau5c+fqwIED2rdvn5544gmXOnfddZe2bdumLVu2aMKECY3zgwMA+BzCDZSVlaXNmzfr1ltvrfGe1WrVLbfc4lzuvXTpUt1222364x//qC5duujuu+9WQUGBJKlVq1batm2b8vPzNXDgQPXq1UvLli1zjuJMmDBBY8eO1ZgxY5yTeesbtZGk3//+9/rTn/6kSZMmqWfPnvr4449rLOEeNGiQNmzYoDfffFM9e/bUNddco507d7rU6dy5s/r3768uXbooMTGxQT8rAIDvsxjuXkzFJHJzcxUREaGcnByXCamSVFxcrKNHj6pDhw4KCgryUgvRVAzDUOfOnfXHP/5R06ZNO6d98bsCAOfX2b6/z8ScG/wqnDx5UikpKcrMzKxzXg4AwBwIN/hVaNOmjaKjo/Xyyy+f9SKGAIDmj3CDX4Vf2dlXAPhVY0IxAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINnAYNGqSpU6d6uxkAAJwTwo0JDB06VNddd12t73344YeyWCzau3dvo31eUVGRWrZsqejoaJWUlDTafgEAaAyEGxOYOHGitm7dqu+//77GeytXrlTv3r3VvXv3Rvu8f/7zn+ratau6dOmiTZs2Ndp+G8IwDJWXl3u1DQAA30K4qY9hSKUF3nm4eVXdm266Sa1bt9aqVatcyvPz87VhwwZNnDhRP/30k0aOHKm2bdsqJCRE3bp109q1axv0I1m+fLlGjRqlUaNGOe8WXt0XX3yhm266SeHh4QoLC9OAAQN05MgR5/srVqxQ165dZbPZFBcXp0mTJkmSvv32W1ksFqWnpzvrZmdny2KxaPv27ZKk7du3y2Kx6O2331avXr1ks9n00Ucf6ciRI7r55psVExOjFi1aqE+fPnr33Xdd2lVSUqLp06crISFBNptNnTp10vLly2UYhjp16qRFixa51E9PT5fFYtHXX3/doJ8TAMA7uP1CfcoKpcfjvfPZD/8oBYbWW83f319jxozRqlWr9Mgjj8hisUiSNmzYILvdrpEjRyo/P1+9evXS9OnTFR4ers2bN2v06NG68MIL1bdvX7ebdOTIEaWlpen111+XYRj605/+pO+++07t2rWTJP3www/63e9+p0GDBmnbtm0KDw/Xjh07nKMrS5cu1bRp07Rw4UJdf/31ysnJ0Y4dOzz+0cyYMUOLFi1Sx44dFRUVpePHj+uGG27QX//6V9lsNr3yyisaOnSoDh48qAsuuECSNGbMGKWlpem5555Tjx49dPToUZ06dUoWi0UTJkzQypUr9eCDDzo/Y+XKlfrd736nTp06edw+AID3EG5MYsKECXrqqaf0/vvva9CgQZIqvpxvvfVWRUREKCIiwuWLe/LkyXrnnXe0fv16j8LNihUrdP311ztvPpmcnKyVK1fq0UcflSQtWbJEERERSklJUUBAgCTpoosucm7/l7/8RX/+8581ZcoUZ1mfPn087u/8+fM1ZMgQ5+uWLVuqR48eztePPfaYNm7cqDfffFOTJk3SoUOHtH79em3dulVJSUmSpI4dOzrrjxs3TnPmzNHOnTvVt29flZWVac2aNTVGcwAAvo9wU5+AkIoRFG99tpu6dOmi/v37a8WKFRo0aJC+/vprffjhh5o/f74kyW636/HHH9f69ev1ww8/qLS0VCUlJQoJcf8z7Ha7Vq9erb/97W/OslGjRunBBx/UnDlzZLValZ6ergEDBjiDTXUnTpzQjz/+qMGDB7v9mXXp3bu3y+v8/Hw9+uij2rx5szIyMlReXq6ioiIdO3ZMUsUpJj8/Pw0cOLDW/cXHx+vGG2/UihUr1LdvX/3rX/9SSUmJbr/99nNuKwDg/CLc1MdicevUkC+YOHGiJk+erCVLlmjlypW68MILnV/mTz31lP72t79p8eLF6tatm0JDQzV16lSVlpa6vf933nlHP/zwg4YPH+5SbrfblZqaqiFDhig4OLjO7c/2niRZrRVTwKrfwbusrKzWuqGhrsfkwQcf1NatW7Vo0SJ16tRJwcHBuu2225z9q++zJemuu+7S6NGj9eyzz2rlypUaPny4R+EPAOAbmFBsInfccYesVqvWrFmjV155RRMmTHDOv9mxY4duvvlmjRo1Sj169FDHjh116NAhj/a/fPlyjRgxQunp6S6PESNGOCcWd+/eXR9++GGtoSQsLEzt27dXampqrftv3bq1JCkjI8NZVn1y8dns2LFD48aN0y233KJu3bopNjZW3377rfP9bt26yeFw6P33369zHzfccINCQ0O1dOlSbdmyRRMmTHDrswEAvoVwYyItWrTQ8OHDNXPmTGVkZGjcuHHO9zp37qytW7fq448/1oEDB3TvvfcqKyvL7X2fPHlS//rXvzR27FhddtllLo8xY8Zo06ZNOn36tCZNmqTc3FyNGDFCn332mQ4fPqxXX31VBw8elCQ9+uijevrpp/Xcc8/p8OHD2r17t55//nlJFaMrV155pRYuXKgDBw7o/fff16xZs9xqX+fOnfX6668rPT1dn3/+uf7rv/5LDofD+X779u01duxYTZgwQZs2bdLRo0e1fft2rV+/3lnHz89P48aN08yZM9W5c2f169fP7Z8PAMB3EG5MZuLEifr555+VnJys+PhfVnnNmjVLV1xxhZKTkzVo0CDFxsZq2LBhbu/3lVdeUWhoaK3zZQYPHqzg4GD9/e9/V6tWrbRt2zbl5+dr4MCB6tWrl5YtW+acgzN27FgtXrxYL774orp27aqbbrpJhw8fdu5rxYoVKi8vV69evTR16lT95S9/cat9zzzzjKKiotS/f38NHTpUycnJuuKKK1zqLF26VLfddpv++Mc/qkuXLrr77rtVUFDgUmfixIkqLS3V+PHj3f7ZAAB8i8Uw3LyYiknk5uYqIiJCOTk5Cg8Pd3mvuLhYR48eVYcOHRQUFOSlFsKbPvzwQw0ePFjHjx9XTExMnfX4XQGA8+ts399nYkIxoIoL/J08eVKPPvqobr/99rMGGwCAb+O0FCBp7dq1ateunbKzs/Xkk096uzkAgHNAuAFUcRE/u92uXbt2qW3btt5uDgDgHBBuAACAqRBuavErm2ONBuB3BAB8F+GmmqrlyoWFhV5uCXxd1e9IbbeZAAB4F6ulqvHz81NkZKROnDghSQoJCXFe4ReQKkZsCgsLdeLECUVGRsrPz8/bTQIAnIFwc4bY2FhJcgYcoDaRkZHO3xUAgG8h3JzBYrEoLi5Obdq0qfOmjfh1CwgIYMQGAHwY4aYOfn5+fIEBANAM+cSE4iVLlqh9+/YKCgpSYmKidu7cedb6GzZsUJcuXRQUFKRu3brprbfeOk8tBQAAvs7r4WbdunWaNm2a5s6dq927d6tHjx5KTk6uc87Lxx9/rJEjR2rixInas2ePhg0bpmHDhmn//v3nueUAAMAXef3GmYmJierTp49eeOEFSZLD4VBCQoImT56sGTNm1Kg/fPhwFRQU6N///rez7Morr1TPnj310ksv1ft5ntx4CwAA+IZmc+PM0tJS7dq1SzNnznSWWa1WJSUlKS0trdZt0tLSNG3aNJey5ORkbdq0qdb6JSUlKikpcb7OycmRVPFDAgAAzUPV97Y7YzJeDTenTp2S3W6vcQfmmJgYffXVV7Vuk5mZWWv9zMzMWusvWLBA8+bNq1GekJDQwFYDAABvycvLU0RExFnrmH611MyZM11GehwOh06fPq1WrVo1+gX6cnNzlZCQoOPHj5vylJfZ+yeZv4/0r/kzex/pX/PXVH00DEN5eXmKj4+vt65Xw010dLT8/PyUlZXlUp6VlVXnBdJiY2M9qm+z2WSz2VzKIiMjG95oN4SHh5v2l1Yyf/8k8/eR/jV/Zu8j/Wv+mqKP9Y3YVPHqaqnAwED16tVLqampzjKHw6HU1FT169ev1m369evnUl+Stm7dWmd9AADw6+L101LTpk3T2LFj1bt3b/Xt21eLFy9WQUGBxo8fL0kaM2aM2rZtqwULFkiSpkyZooEDB+rpp5/WjTfeqJSUFH322Wd6+eWXvdkNAADgI7weboYPH66TJ09qzpw5yszMVM+ePbVlyxbnpOFjx47Jav1lgKl///5as2aNZs2apYcfflidO3fWpk2bdNlll3mrC042m01z586tcRrMLMzeP8n8faR/zZ/Z+0j/mj9f6KPXr3MDAADQmLx+hWIAAIDGRLgBAACmQrgBAACmQrgBAACmQrg5iyVLlqh9+/YKCgpSYmKidu7cedb6GzZsUJcuXRQUFKRu3brprbfecnnfMAzNmTNHcXFxCg4OVlJSkg4fPtyUXaiXJ31ctmyZBgwYoKioKEVFRSkpKalG/XHjxslisbg8rrvuuqbuRp086d+qVatqtD0oKMiljq8dQ0/6N2jQoBr9s1gsuvHGG511fOn4ffDBBxo6dKji4+NlsVjqvH9cddu3b9cVV1whm82mTp06adWqVTXqePp33ZQ87ePrr7+uIUOGqHXr1goPD1e/fv30zjvvuNR59NFHaxzDLl26NGEv6uZp/7Zv317r7+iZt9dpzsewtr8xi8Wirl27Ouv4yjFcsGCB+vTpo7CwMLVp00bDhg3TwYMH693OF74LCTd1WLdunaZNm6a5c+dq9+7d6tGjh5KTk3XixIla63/88ccaOXKkJk6cqD179mjYsGEaNmyY9u/f76zz5JNP6rnnntNLL72kTz/9VKGhoUpOTlZxcfH56pYLT/u4fft2jRw5Uu+9957S0tKUkJCga6+9Vj/88INLveuuu04ZGRnOx9q1a89Hd2rwtH9SxRU1q7f9u+++c3nfl46hp/17/fXXXfq2f/9++fn56fbbb3ep5yvHr6CgQD169NCSJUvcqn/06FHdeOONuvrqq5Wenq6pU6fqrrvucvnyb8jvRFPytI8ffPCBhgwZorfeeku7du3S1VdfraFDh2rPnj0u9bp27epyDD/66KOmaH69PO1flYMHD7q0v02bNs73mvsx/Nvf/ubSt+PHj6tly5Y1/g594Ri+//77uv/++/XJJ59o69atKisr07XXXquCgoI6t/GZ70IDterbt69x//33O1/b7XYjPj7eWLBgQa3177jjDuPGG290KUtMTDTuvfdewzAMw+FwGLGxscZTTz3lfD87O9uw2WzG2rVrm6AH9fO0j2cqLy83wsLCjNWrVzvLxo4da9x8882N3dQG8bR/K1euNCIiIurcn68dw3M9fs8++6wRFhZm5OfnO8t86fhVJ8nYuHHjWes89NBDRteuXV3Khg8fbiQnJztfn+vPrCm508faXHrppca8efOcr+fOnWv06NGj8RrWSNzp33vvvWdIMn7++ec665jtGG7cuNGwWCzGt99+6yzz1WN44sQJQ5Lx/vvv11nHV74LGbmpRWlpqXbt2qWkpCRnmdVqVVJSktLS0mrdJi0tzaW+JCUnJzvrHz16VJmZmS51IiIilJiYWOc+m1JD+nimwsJClZWVqWXLli7l27dvV5s2bXTxxRfrvvvu008//dSobXdHQ/uXn5+vdu3aKSEhQTfffLO++OIL53u+dAwb4/gtX75cI0aMUGhoqEu5Lxy/hqjvb7Axfma+xuFwKC8vr8bf4OHDhxUfH6+OHTvqzjvv1LFjx7zUwobp2bOn4uLiNGTIEO3YscNZbsZjuHz5ciUlJaldu3Yu5b54DHNyciSpxu9bdb7yXUi4qcWpU6dkt9udV0muEhMTU+Pcb5XMzMyz1q/615N9NqWG9PFM06dPV3x8vMsv6XXXXadXXnlFqampeuKJJ/T+++/r+uuvl91ub9T216ch/bv44ou1YsUKvfHGG/r73/8uh8Oh/v376/vvv5fkW8fwXI/fzp07tX//ft11110u5b5y/Bqirr/B3NxcFRUVNcrvvK9ZtGiR8vPzdccddzjLEhMTtWrVKm3ZskVLly7V0aNHNWDAAOXl5Xmxpe6Ji4vTSy+9pH/+85/65z//qYSEBA0aNEi7d++W1Dj/3fIlP/74o95+++0af4e+eAwdDoemTp2q3/72t2e9I4CvfBd6/fYLaJ4WLlyolJQUbd++3WXS7YgRI5zPu3Xrpu7du+vCCy/U9u3bNXjwYG801W39+vVzuQFr//79dckll+h//ud/9Nhjj3mxZY1v+fLl6tatm/r27etS3pyP36/NmjVrNG/ePL3xxhsuc1Kuv/565/Pu3bsrMTFR7dq10/r16zVx4kRvNNVtF198sS6++GLn6/79++vIkSN69tln9eqrr3qxZU1j9erVioyM1LBhw1zKffEY3n///dq/f7/X5m95ipGbWkRHR8vPz09ZWVku5VlZWYqNja11m9jY2LPWr/rXk302pYb0scqiRYu0cOFC/d///Z+6d+9+1rodO3ZUdHS0vv7663NusyfOpX9VAgICdPnllzvb7kvH8Fz6V1BQoJSUFLf+I+mt49cQdf0NhoeHKzg4uFF+J3xFSkqK7rrrLq1fv77GKYAzRUZG6qKLLmoWx7A2ffv2dbbdTMfQMAytWLFCo0ePVmBg4FnrevsYTpo0Sf/+97/13nvv6Te/+c1Z6/rKdyHhphaBgYHq1auXUlNTnWUOh0Opqaku/2dfXb9+/VzqS9LWrVud9Tt06KDY2FiXOrm5ufr000/r3GdTakgfpYpZ7o899pi2bNmi3r171/s533//vX766SfFxcU1Srvd1dD+VWe327Vv3z5n233pGJ5L/zZs2KCSkhKNGjWq3s/x1vFriPr+Bhvjd8IXrF27VuPHj9fatWtdlvHXJT8/X0eOHGkWx7A26enpzrab5RhKFSuRvv76a7f+J8Nbx9AwDE2aNEkbN27Utm3b1KFDh3q38ZnvwkabmmwyKSkphs1mM1atWmV8+eWXxj333GNERkYamZmZhmEYxujRo40ZM2Y46+/YscPw9/c3Fi1aZBw4cMCYO3euERAQYOzbt89ZZ+HChUZkZKTxxhtvGHv37jVuvvlmo0OHDkZRUdF5759heN7HhQsXGoGBgcZrr71mZGRkOB95eXmGYRhGXl6e8eCDDxppaWnG0aNHjXfffde44oorjM6dOxvFxcU+37958+YZ77zzjnHkyBFj165dxogRI4ygoCDjiy++cNbxpWPoaf+qXHXVVcbw4cNrlPva8cvLyzP27Nlj7Nmzx5BkPPPMM8aePXuM7777zjAMw5gxY4YxevRoZ/1vvvnGCAkJMf77v//bOHDggLFkyRLDz8/P2LJli7NOfT+z883TPv7jH/8w/P39jSVLlrj8DWZnZzvr/PnPfza2b99uHD161NixY4eRlJRkREdHGydOnPD5/j377LPGpk2bjMOHDxv79u0zpkyZYlitVuPdd9911mnux7DKqFGjjMTExFr36SvH8L777jMiIiKM7du3u/y+FRYWOuv46nch4eYsnn/+eeOCCy4wAgMDjb59+xqffPKJ872BAwcaY8eOdam/fv1646KLLjICAwONrl27Gps3b3Z53+FwGLNnzzZiYmIMm81mDB482Dh48OD56EqdPOlju3btDEk1HnPnzjUMwzAKCwuNa6+91mjdurUREBBgtGvXzrj77ru99h8dw/Csf1OnTnXWjYmJMW644QZj9+7dLvvztWPo6e/oV199ZUgy/u///q/Gvnzt+FUtCz7zUdWnsWPHGgMHDqyxTc+ePY3AwECjY8eOxsqVK2vs92w/s/PN0z4OHDjwrPUNo2L5e1xcnBEYGGi0bdvWGD58uPH111+f345V8rR/TzzxhHHhhRcaQUFBRsuWLY1BgwYZ27Ztq7Hf5nwMDaNi6XNwcLDx8ssv17pPXzmGtfVLksvfla9+F1oqOwAAAGAKzLkBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgB4FMsFos2bdrk7WZ4ZPv27bJYLMrOzvZ2UwCIcAOg0rhx42SxWGo8rrvuOm83rV6DBg2SxWJRSkqKS/nixYvVvn177zQKgNcQbgA4XXfddcrIyHB5rF271tvNcktQUJBmzZqlsrIybzel0ZSWlnq7CUCzRLgB4GSz2RQbG+vyiIqKcr5vsVi0dOlSXX/99QoODlbHjh312muvuexj3759uuaaaxQcHKxWrVrpnnvuUX5+vkudFStWqGvXrrLZbIqLi9OkSZNc3j916pRuueUWhYSEqHPnznrzzTfrbfvIkSOVnZ2tZcuW1Vln3LhxGjZsmEvZ1KlTNWjQIOfrQYMGafLkyZo6daqioqIUExOjZcuWqaCgQOPHj1dYWJg6deqkt99+u8b+d+zYoe7duysoKEhXXnml9u/f7/L+Rx99pAEDBig4OFgJCQl64IEHVFBQ4Hy/ffv2euyxxzRmzBiFh4frnnvuqbffAGoi3ADwyOzZs3Xrrbfq888/15133qkRI0bowIEDkqSCggIlJycrKipK//nPf7Rhwwa9++67LuFl6dKluv/++3XPPfdo3759evPNN9WpUyeXz5g3b57uuOMO7d27VzfccIPuvPNOnT59+qztCg8P1yOPPKL58+e7BIaGWL16taKjo7Vz505NnjxZ9913n26//Xb1799fu3fv1rXXXqvRo0ersLDQZbv//u//1tNPP63//Oc/at26tYYOHeocSTpy5Iiuu+463Xrrrdq7d6/WrVunjz76qEawW7RokXr06KE9e/Zo9uzZ59QP4FerUW/DCaDZGjt2rOHn52eEhoa6PP76178660gy/vCHP7hsl5iYaNx3332GYRjGyy+/bERFRRn5+fnO9zdv3mxYrVbn3cXj4+ONRx55pM52SDJmzZrlfJ2fn29IMt5+++06txk4cKAxZcoUo7i42GjXrp0xf/58wzAM49lnnzXatWvn0sebb77ZZdspU6bUuPP2VVdd5XxdXl5uhIaGGqNHj3aWZWRkGJKMtLQ0wzB+uTN0SkqKs85PP/1kBAcHG+vWrTMMwzAmTpxo3HPPPS6f/eGHHxpWq9UoKioyDMMw2rVrZwwbNqzOfgJwj79XkxUAn3L11Vdr6dKlLmUtW7Z0ed2vX78ar9PT0yVJBw4cUI8ePRQaGup8/7e//a0cDocOHjwoi8WiH3/8UYMHDz5rO7p37+58HhoaqvDwcJ04caLe9ttsNs2fP9852tJQ1T/fz89PrVq1Urdu3ZxlMTExklSjTdV/Ni1bttTFF1/sHNX6/PPPtXfvXv3jH/9w1jEMQw6HQ0ePHtUll1wiSerdu3eD2w2gAuEGgFNoaGiNU0SNKTg42K16AQEBLq8tFoscDodb244aNUqLFi3SX/7ylxorpaxWqwzDcCmrbQJybZ9fvcxisUiS222SpPz8fN1777164IEHarx3wQUXOJ9XD4YAGoY5NwA88sknn9R4XTXqcMkll+jzzz93mfOyY8cOWa1WXXzxxQoLC1P79u2VmpraZO2zWq1asGCBli5dqm+//dblvdatWysjI8OlrGrUqTFU/9n8/PPPOnTokPNnc8UVV+jLL79Up06dajwCAwMbrQ0ACDcAqikpKVFmZqbL49SpUy51NmzYoBUrVujQoUOaO3eudu7c6ZwUe+eddyooKEhjx47V/v379d5772ny5MkaPXq081TOo48+qqefflrPPfecDh8+rN27d+v5559v1H7ceOONSkxM1P/8z/+4lF9zzTX67LPP9Morr+jw4cOaO3dujRVN52L+/PlKTU3V/v37NW7cOEVHRztXZ02fPl0ff/yxJk2apPT0dB0+fFhvvPFGjQnFAM4d4QaA05YtWxQXF+fyuOqqq1zqzJs3TykpKerevbteeeUVrV27VpdeeqkkKSQkRO+8845Onz6tPn366LbbbtPgwYP1wgsvOLcfO3asFi9erBdffFFdu3bVTTfdpMOHDzd6X5544gkVFxe7lCUnJ2v27Nl66KGH1KdPH+Xl5WnMmDGN9pkLFy7UlClT1KtXL2VmZupf//qXc1Sme/fuev/993Xo0CENGDBAl19+uebMmaP4+PhG+3wAFSzGmSegAaAOFotFGzdurHGtGADwJYzcAAAAUyHcAAAAU2EpOAC3cRYbQHPAyA0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADAVwg0AADCV/w8bb07AKfk/5wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[:,2:4])\n",
    "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig('_accuracy_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59f1b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(img):\n",
    "    img_transformations = transforms.Compose([\n",
    "            transforms.Resize(size=256),\n",
    "            transforms.CenterCrop(size=224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    return img_transformations(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d61c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(fn):\n",
    "    img = Image.open(fn).convert('RGB')\n",
    "    img = img.resize((224,224))\n",
    "\n",
    "    test_img_tensor = img_transform(img)\n",
    "\n",
    "    return test_img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "466990e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def test_model(input):\n",
    " #   img = process_image(input)\n",
    "  #  return alexnet(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2af056e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=4, bias=True)\n",
       "    (7): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "model = alexnet\n",
    "\n",
    "model.load_state_dict(torch.load('model.pt', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "#def test_model(input):\n",
    "#    img = process_image(input)\n",
    " #   return model(img)\n",
    "\n",
    "#test_model('img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "542f79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_image_name):\n",
    "    '''\n",
    "    Function to predict the class of a single test image\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param test_image_name: Test image\n",
    "\n",
    "    '''\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    test_image = Image.open(test_image_name).convert('RGB')\n",
    "    plt.imshow(test_image)\n",
    "    \n",
    "    test_image_tensor = transform(test_image)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor)\n",
    "        print(torch.argmax(out).item())\n",
    "\n",
    "        ps = torch.exp(out)\n",
    "        topk = ps.topk(4, dim=1)\n",
    "\n",
    "        #scores = topk.values.numpy()\n",
    "        #predictions = topk.indices.numpy()\n",
    "\n",
    "        #max_score_index = np.argmax(scores)\n",
    "        #max_pred = predictions[max_score_index]\n",
    "\n",
    "    # print(max_pred)\n",
    "\n",
    "        results = topk.values.cpu().numpy()[0]\n",
    "        #print(results)\n",
    "        #print(np.where( results == max(results))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "731ef8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, loss_criterion):\n",
    "     \n",
    "   \n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            \n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "            \n",
    "\n",
    "    avg_valid_loss = valid_loss/len(test_data)  \n",
    "    avg_valid_acc = valid_acc/len(test_data) \n",
    "\n",
    "    return avg_valid_loss, avg_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "228c41f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2025966839958923, 0.9318181824202489)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(alexnet, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bb867",
   "metadata": {},
   "source": [
    "TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11741469",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/neerajakulkarni/PlasticID/t2.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#WRONG\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt2.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m predict(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m predict(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5.png\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, test_image_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mFunction to predict the class of a single test image\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      9\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                         [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     15\u001b[0m ])\n\u001b[0;32m---> 17\u001b[0m test_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(test_image)\n\u001b[1;32m     20\u001b[0m test_image_tensor \u001b[38;5;241m=\u001b[39m transform(test_image)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/neerajakulkarni/PlasticID/t2.png'"
     ]
    }
   ],
   "source": [
    "#WRONG\n",
    "\n",
    "predict(model, 't2.png')\n",
    "predict(model, 't1.png')\n",
    "predict(model, 't5.png') #\n",
    "predict(model, 't55.png') \n",
    "predict(model, 't555.png') #\n",
    "predict(model, 't22.png') \n",
    "predict(model, 't111.png') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a1b00ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAGiCAYAAABj4pSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD+m0lEQVR4nOz9ebxc1XnnC3+ftfauqjOPOppnNDIjQAgwg8FgG8/g2E7skMSJEwe7b0IGx520u9PD69z0+3an8+kkvt23u517P3GSzuA4HgBjzIyYxDwLEBKgWUdnPlW191rr/WOtvavq6EggDJaE9o9PUad27dq19i7Vr57h9zyPOOccBQoUKFDgTUEd6wUUKFCgwImEgjQLFChQ4ChQkGaBAgUKHAUK0ixQoECBo0BBmgUKFChwFChIs0CBAgWOAgVpFihQoMBRoCDNAgUKFDgKFKRZoECBAkeBgjQLFChQ4ChwTEnzz/7sz1i2bBmVSoWNGzfy4IMPHsvlFChQoMAb4piR5t/+7d9y44038q//9b/mkUce4cwzz+Tqq69m7969x2pJBQoUKPCGkGPVsGPjxo2cd955/Nf/+l8BsNayePFivvzlL/N7v/d7x2JJBQoUKPCGiI7Fm9brdbZs2cJXv/rVfJtSiiuvvJLNmzcfsn+tVqNWq+WPrbUMDw8zMDCAiPxU1lygQIF3L5xzjI+Ps2DBApQ6sgN+TEhz//79GGOYO3duy/a5c+fy3HPPHbL/17/+df7wD//wp7W8AgUKnKR49dVXWbRo0RH3OSGy51/96lcZHR3Nbzt27DjWSypQoMC7EF1dXW+4zzGxNAcHB9Fas2fPnpbte/bsYd68eYfsXy6XKZfLP63lFShQ4CTFmwn3HRNLs1QqsWHDBm677bZ8m7WW2267jU2bNh2LJRUoUKDAm8IxsTQBbrzxRq6//nrOPfdczj//fP7kT/6EyclJfvEXf/FYLalAgQIF3hDHjDQ/9alPsW/fPr72ta+xe/duzjrrLG6++eZDkkMFChQocDzhmOk0fxKMjY3R09NzrJdRoECBdxlGR0fp7u4+4j4nRPa8QIECBY4XFKRZoECBAkeBgjQLFChQ4ChQkGaBAgUKHAUK0ixQoECBo0BBmgUKFChwFChIs0CBAgWOAgVpFihQoMBRoCDNAgUKFDgKFKRZoECBAkeBgjQLFChQ4ChQkGaBAgUKHAUK0ixQoECBo0BBmgUKFChwFChIs0CBAgWOAgVpFihQoMBRoCDNAgUKFDgKFKRZoECBAkeBgjQLFChQ4ChQkGaBAgUKHAUK0ixQoECBo0BBmgUKFChwFChIs0CBAgWOAgVpFihQoMBRoCDNAgUKFDgKFKRZoECBAkeBgjQLFChQ4ChQkGaBAicglBK0Kr6+xwLRsV5AgQIFjg7zhgb4+DXvo7Ork207XmPv3gNsf/V1du3eS5KmOHesV/juRkGaBQqcQFg4bw7/v3/7O1x66QXEpTJTtTq1akK1Vmd8YooXX97OQ1se46WXt/Pqa7s4ODrG+PgEk1NTpKk51st/V6AgzQIFThCICO9/7yYuvuBMlBjStEoUC21tHTjXydBQP2vXruDD11xBmloOjoyx/8AwO17dyfMvvMQ/fucmnnjqWVxhiv5EEHcCXsGxsTF6enqO9TIKFPipoqerg7/+b/8fzjlzHdNJDV2KQWucdYgIOEFEo1SMRbAOSqUKIgqlIl56eQfX//K/4IWtLx3rUzluMTo6Snd39xH3KSLJBQqcIDjn9NWsXrmEWq2Kw5EmdUxSxSZV6lOT1Kr+ltSnSOpTOJtSr09j0homrbFi+RK+8Eufo7Oj/VifygmNwj0vUOAEQBxpPnjlJoSEer2OA7RSiIIkBSUalGCswdYTJIpwkQEU6AitS6TiuPZjH+all7fz3/7n/4MxRYzzraCwNAsUOAHQ0V5hwdwBlIpwSmMR6klCrVbHmgRjEkxSIzU1UptgTJ20XsWmder1GrValdSklMplfvULv8jyZUuO9SmdsChIs0CBEwBtlTIdHR2U2trpHxyif2gucVsniRUsDuNSLAbEAinWpiRpQmpS6kmd1KRYa0mMZXBwkOs/+xniOD7Wp3VCoiDNAgVOAJRKMV1dnTjRJFZAlSm3d+N0CSNgxeLEAQ7nHM6Bc5Bah0OwFhJjSY1FVMRll17CqlNWHuvTOiFRkGaBAicA2itlKu3tiI5AxziJKVU66ewZQEVlkAjnFBChJEZQKKUoxWV6e+fQ3TPg45rGMFWrs2TZMi6+8AKfdS9wVChIs0CBEwA9XZ10dnRgAFERBsGgKLd3Epc6QCK0LqN1CdExSpUpl7pob++lXO6k0tZFe3tHfjwniuuu/QQ93V3H7qROUBSkWaDACYDB/m4629txTmGtw1pwCEpFlCsdxHEHKmpD6zKRLlMqtRFHZRCNdQrrFDouobUmTVNSY1ixYiXvu/KKwto8ShSkWaDACYDBgT5KpRKC4GwWuwQcRFEZHZURiVAqQimFUj5BZK3FWIsJQc5KpQQ4ktSgooj3XfFeOtoL3ebRoCDNAgVOAKxcugitFZHWIIBr3EQECUSplAqWY/akv1ksxqYIjjiOMWmKQ9hwzjmsXLH8mJ7biYaCNAsUOM4hIpx56mqc8+WSAigEhedPUQqUgIAIiAKlHYjBYSBIkUR5QXykNUorkjRhYM4gV1x+CYWD/uZRkGaBAsc5SnHEwGA/NliUgsKJwqEwCKIkuOMW56x34Z2/WetwTgDBObDiUNpRihXWGRJn2XjRhUSFZvNNoyDNAgWOc8wZ6KXUVkJFGlHBnHQWQ4oxhiRJsM6Co0mjqfy9Fd/VyIETwTrvros4lHKkJmXpsqXMmzf3WJ/mCYOCNAsUOM6xdNF8KqUS4mXqYFPfhKM+jalPkdSnccb4UGdL0zLB4TDWYKzBWZs/r7VChzhoZ2cXGzacc0zO7UREQZoFChzHEIHFC+bSVikhYsEm2LSKqU9ja9OYZJo08fXnzplgaTpcSAQJjjRNSOt1jElx1iLOEYsnTRyUSiU2nHM25XL5WJ/uCYGiy1GBAscx4ijilBWLKMURLk2pJQZjvJQIYwGXu93OWRTin1MWh0Jpwbo6Yi3KxiCKSGlUrIgiRWoMIsIl77mIOYMDvPb6zmN9ysc9CtIsUOA4RqVcYsWSBeAMad2QGou1AALWentSBAve/RbBOAvKIUp7W9M5lAPjHCIaqy1W66DrBGMNy5ctY83qUwrSfBMo3PMCBY5jdHa0s2zRfNI0IUlrWJOCszhrwFlwBl8eZAF8tZCzvq+mtRhjsMZgTYpNU99GLqmT1Gukpo6E/6xzvPeyy9BaH+MzPv5RkGaBAscxero6mDOnh0pbmbgUE8URcRyjlSc3R6O7Ed7exDjvvmek6YzBmMTHPY3B2hST1LFpimAREYy1bDj3HAb6+47l6Z4QKEizQIHjGMsWzaW9rUJnZwelOCaulClXKuhI40RC4sfinMFaT4hZQsg6i8Vvd00WJ9ZiU4NJ0lAZZLEOli1dysbzNhzrUz7uUZBmgQLHKbQSTluzgra2Nqy1KC2UymXiShkVRzjl63istVhjPSlam2fO80x6LkNyOGsxJsXaBJPW/cgL52VJbe1tXHzhJkql0jE75xMBR02ad911Fx/+8IdZsGABIsI//dM/tTzvnONrX/sa8+fPp62tjSuvvJKtW7e27DM8PMzP/dzP0d3dTW9vL5///OeZmJj4iU6kQIF3G5YuHOKDV15MW1uFJDEoKVEplWlvb6fSVkFJVvETrMpAkLm3DojzxOoARGFFMAgpQuogMSlJkiAi1I3lrHPOKdrFvQGOmjQnJyc588wz+bM/+7NZn//jP/5j/vRP/5RvfOMbPPDAA3R0dHD11VdTrVbzfX7u536Op59+mltvvZXvfe973HXXXXzhC19462dRoMC7DKU44hMfuIT1a5djrS+N1CoiEkUURbS3tRMpHUokmyGBKCX/z/+t/FhfpbEqxkXhJhpjvW1qLCxZvpIFCxf89E/4BMJPNPdcRPj2t7/Nxz72McBbmQsWLOC3fuu3+O3f/m3AzxGeO3cu3/zmN/n0pz/Ns88+y/r163nooYc499xzAbj55pv54Ac/yGuvvcaCBW/8gRVzzwu827F04RB//ed/yJmnr2NscgLr/KgKI4JSGgGmxidJ0xQjDi8+Uohkteka0CAKUQqtY5yKsCpGKY2KYt9OTsfoKAanMKmhUqnwb//wX/M//8c3j+0FOEb4qc8937ZtG7t37+bKK6/Mt/X09LBx40Y2b94MwObNm+nt7c0JE+DKK69EKcUDDzww63FrtRpjY2MttwIF3q0oxREfvfpiFi+ay4Hh/UxMjDExMUmtllCt1ZienmZyYirveoQFMfgEj3U46/sggUDerEMBGkGD8p3dddxGqdJJd+8gHV09qFKZWmo5d8N5RWPiI+BtJc3du3cDMHdua/H/3Llz8+d2797N0NBQy/NRFNHf35/vMxNf//rX6enpyW+LFy9+O5ddoMBxAwHOP2stn/zwlSRJnanqNNY5jLVYHOIEJZpIR5TiMnFUohTHRFGEEoU4X6GOw9dgKk+WDgVOIRIhRChVJorbKVe6KFU6MShQEU4Ug3Pm0N7WdoyvxPGLEyJ7/tWvfpXR0dH89uqrrx7rJRUo8I6gt6eTX/vcx1kwb4AkDWN3Ez+eop6mJElKrVrz96GeHCDSmlKpRKkUo5S3MhuBN98ZyfnIqCdOXULHFUptnRgHU9U6iTFY5yhXynT3HNlFPZnxtpZRzps3D4A9e/Ywf/78fPuePXs466yz8n327t3b8ro0TRkeHs5fPxPlcrloJlDgXQ8RuPjc0zj79DWkJsU46/saOXx/TFzeqd0Y34UdQIvCBgtTRKG1wqGxYQ+LQznnzVilcWhEe+IUFTEdrFnnHNak1GpVeOupjnc93lZLc/ny5cybN4/bbrst3zY2NsYDDzzApk2bANi0aRMjIyNs2bIl3+fHP/4x1lo2btz4di6nQIETCp3tFT76/kuJY+1F5w6MFYyx1NOUej0hNakfjJb6XprOWdI0xZoUZ/w2aw0Oh1KeRCG0jBNBtAKliMsVKpUK9cQwNVUlqSWIcWAMYyMHGRsfP8ZX4/jFUVuaExMTvPjii/njbdu28dhjj9Hf38+SJUv4jd/4Df79v//3rFq1iuXLl/Ov/tW/YsGCBXmGfd26dbz//e/nV37lV/jGN75BkiR86Utf4tOf/vSbypwXKPBuRV9PF0ODvaRpHescNvVWYmodqbHB+DMoJWiFN03DnXMCoiHr2m6NFxwphRIVSitTNFBuK1OptBGVNPXplHq9hk0NIgkmqbJ9+w6mpqaO3YU4znHUpPnwww9z+eWX549vvPFGAK6//nq++c1v8ru/+7tMTk7yhS98gZGRES6++GJuvvlmKpVK/pq/+qu/4ktf+hJXXHEFSimuvfZa/vRP//RtOJ0CBU5clEoxWitE8Baj01h8EsgYh3UuzPIR3wsT6wenBS0mhA6aTuHwTTwEi1IxxroQH63RVRqgVC6Rpo5qtYoxBiWQ1mpMTY5yz913F975EfAT6TSPFQqdZoF3I5Ytmst//jdfZvmSIZzyrnnqhDS1pKkfw6sF4lhTihWRDpKjxog1rGif9JEsZhljnQIdoeI2Uqvp6Rukd2AeiVWMjk9RnZ5G0hquPsEzjz/Cb/z2V9h/YPgYX41jg5+6TrNAgQJvHZNT0xw4OEJiLElqMMaQmoQ0TUK9uO9aFClFrDQKQZwD5+VE2Wxfh59CadIEF7odxTqmo72DSqXC2MEDTI8fxCXT1KtT4BzOOuqTE9x9550cHBk91pfiuEZBmgUKHCcYGZvguRe3kxqHSSxJYknrFmsc1jhcagFBJGTFrfWNh22KcxbfGs42DihCmhocPrteq6WI84WVwwcOMDUxAdYgWJKkyjPPPstNP7rdN/EocFgUpFmgwHGCJLXcds8WhodHSVOHSR02dTgjOANYIRKVW5jZzVuXKdb5bDqhZZwJCSRjIEkM1ekqk5OTgFCr1ZkcH0ecoT49ydTYCH/3D//Iq0Xn9jdEQZoFChxHeGbrDn58z8PU6wnOapxVWGNJkwRsnXIpQit8ggefOZcw79xPnXRYC6mFJDXUUkstSanWEmr1lDS11OspzjqsNZikiqlOsvneu7nnvvs4AVMcP3UUpFmgwHGEWj3l2zffw7Nbt+dxTWMSrK2jtKNc0mjlUGKD1Ag/85xs9qSXKBkj1BNLkhjq9ZR6ajE2zD93IKJIkwSXJkyMD3PLLTcX7RnfJArSLFDgOMPLO3bzzb/9AXv27sU5T5hCQnt7RKkESjucgBXBKRX0mRFIhEPjrCZJLPW6DdalITW+U7tNfdmlc87PGkqqPPnYIzz08JZCZvQmUZBmgQLHIR5/9mX+/ns/Ynj4IMYklCsR3d0dxKUIUb5k0onGSoxVJYxEWKcxRgf9ZUKaWHChBt04nE1BXJ7o0UoYHz3Id77zPaamq0deUIEcxQjfAgWOQyRpyg9ufwDnDJ/6yCWsWL6OgcF+EBgdGScxKZYIi/KzgqzPmycm9W556vIGxDiHApQImazT2hRE8eijj/HYk08f25M9wVBYmgUKHKeoJyk/vPsRHn7iJfr7+unt7aOtrZ2o0o4qdaDLnZQqPcSlDqK4DZGYet0yPZ34rDv4AKZziDiUOJz1GXYBJqcm+Ofv/YDxicljfKYnFgpLs0CB4xrCksWL6O7pJiqVGBkfx6GJKxVEtdPe3k2S1picnGBs8iATU1VwgpII5xQiGq01KhvC5iwYi1aWhx/ewgMPP1pkzI8SBWkWKHAcY9WKhbz/fedTaY+p1iao16ugYyJpw6kOKp1ziEyd6QSqtX2kiSGOyjhRiI5AaX8T38hDo3Fpwr49O/nbv/nfTFeLWObRoiDNAgWOAeJI09ZWprO9jUq5FJpyWA4cHKNWr2Oto1KOuf7T17B0ySKsCONTU1iJcMQ4VULF7cRtXZjqJFPTNaampr2kSCyiIkSBihSi8JVDSpHU6+zds5P/+c3/l0efeOpYX4YTEgVpFijwU0RvdyfnnrWGqy67gE0XnMXylcuolEs4UUxOTHPX3Q9w/4OP8/RzL7Fs6QI++pGrUB1djI+NUU0UCTHodiwVunr6SS2Mjg6z/eUXmZ4Yo70S41LfqFic9qWXUYmUhInhAzy85TG+/d0f8MJLL2Nt4Za/FRRdjgoU+CkgjjTnnLGaL//KJzn3nHUMzZtDe0c7EmlMaih3dDI5MYVYqFXrHDw4gtKKoTmDlEolDhw4SLUOljKpi4jb++kbmM/B4RF++INv8z/+7/9OX3cHyxcvoLunm0pbG5X2TpI0ZXRsnNdff52tL27nxZdfYWKy6JV5OLyZLkeFpVmgwDsMpYSPfuBC/uC3foFFC+egIyGKDEINMT5Rk1QnSKtTpKEpR29vO1pr0rSGNZZKuQODJTERETG9Pb0k9Trbtr3Mf/2zv+DpZ54N76WCtEhCxZDDhl6aJ5x1dJyiIM0CBd5BxJHm4vNP5Xe//LMsXtSP1g4lICYhrYZuQqJ9iWPqm7tZ65VCJklQKkJpQUmJzvZ26onCUqIcaUYPjPCdb/89W7c2JilYa5v7HBV4B1CQZoEC7xA62yt85uPv5dd/6WMsXz4XUQ4VKsTFOZT1XdctoBBiFRGLUMdiLIAjTVMwYRqvVShVoRRrktokB/fv5pZbbqaeJMf2RE8yFKRZoMDbDBFYvXwhv/Prn+bDV19IXBGwqe+D6SwuJGBSX/6NRWHRKO2ISxWiOCZCkRqHiPHt3azFpAlKgUVQxiI29SQcWsEV+OmgqAgqUOBtxool8/mPX/tVrvvwhcRxitg6Ckta8wPMrIU0NSRpijEW68BaR1JPma7VqdVTEgvoMrrcgYorOCVYLBZDLZminkyzeOFc/ujf/xvWrj7lWJ/ySYWCNAsUeBvR2V7ht3/tk2w6dw3G1RBlAIu1KSo02kisJbHO97x0vlGwtYSmwY40tdRqKZPThtQoVFyhVGlHxRFJGMXrBBJjuPjCTfxf3/hTLr30IiKtj/XpnxQoSLNAgbcR5525imuu3oTEDtGCFYNVLuub0WieAUDIbDuH99gFDNjE4lIH1lBP6iS1BBDKpTZKpQqgMNb54WsK1px6On/yJ3/Cxo3nH7PzPplQkGaBAm8Tujvb+MXPvJ/urjKiBOMMKIWIQgj3Ij57HgRAgoDLGNV3JRIriAOxFmtS0jShXktI6gZBU6q0I0rjEIw1OIlZs/4MvvTlL9PbW+iX32kUpFmgwNsAEeGSjadx8Xmn4UyCcpZIRyg0SjQEcnSWPGkjThraSVEgyjcXVmCcxTg/VM2kljRJPXEmCWmaEsVxSAAJ7W0V0jTlvPPO5QPvvxqliq/1O4ni6hYo8Dbg9NWL+e0vfJzBvg4ircCBdoJ2ChUaAYdm6WFopPIWJp4slYpAa5yoELNUWJcRrAsidUNST6nXU0xqUGhs8Ptr1Wm6e3r44hd/lTmDA8f2YrzLUUiOCpyw0EqhtCLSiijSVEolSqWYOIrQkUYr3xJNpHHz5GN9HDE0yUhTQz1JqNXrnpCM8c9Z+6bqs+cO9vB7N/wMZ6xfAiQY69BxZo84nHGIk2wKGtYKZDFMFM4FgvQKTkTH3pVPnSdaBJzCOcGJt1jricGKRYkwOjKKjksYm7JsxXIufs9F/OM/fqeQIb1DKEizwAkDESjFESuWLmDF0gUsWzKfeUP99Pd20dvdQX9fD3093XR2tNFWqVCpVIhLMRIGjmX32VjbWj2hWq0zNT3N2MQUw8NjjIyOMTI6zuj4JMMjY+zac4ADwwd5fddeXt+930+JnEFGm85dz/kb1uDEQhThEzwWUVks02GxoRmwArJxFf55h8aicU7hsk7seJIVApk68VYlYKxFKe3bwMUaa1KcKBygIs1FF2/ie9/7AbVa/af9EZ0UKEizwHGNSCuWLprLWaefwkUXnMl7LjiLuYM9VCoxpZJGa+Ub7IYaa6W0tygBQvLFBfdWwvhG7/YqrC0DNOKKIeboM9qWxFgS60dP1GoJY+NTbN+xkyeeep6XX3mV3Xv2U6vX+Mx176envw+tDalzWCcopdCiMcaBH0WOc86L2wMximgQjSLCSYRB+fEVluDLB/IMN5cnjQTrvPVqLYhSpGmCKI0xKQP9vslHQZrvDArSLHBcQilh2aIhPv/Za7jqio2sPmURpXKMSw2iXE5AYENX8pBxVqDyZLT1j1Xjn7l/nY8tmjRkrIVQpSNY68JURk2EpiSCkwpYYWjuAGtWL+PK916AMYapqSmmp6fp7+sktTUQhShBa41WEUoJpZLflkxNUU2mERE/RVKybLrvj2lR4LwV6onRk3nDphWfaRfBhjm82nm5kv9Z8EjrKQpBq0Kz+U6hIM0Cxx16uzu47iOX8is/fw1rVi0k0grnEkw9QevI21wSCFL5xIlSCtE6n/0Noaek1n5H54LF6WObGNBWwPptztqcTP1sHUEDVuuQ8fYuMoBWmkhryuUy3T3d3hB0ZZwzOMJoXQRnDLae4myKWEBiH0sVEHTudjsBE2aRa1He0vXOdrgiPlmUxUBxKnfzrXOkaR10hKQOmxrGR8ZIinr0dwwFaRY4bqC14tTVS/nVX/gY11x1Pl3dJZykIJo40nkiRwVrDgRpkdf4LuWCeHNTlH9ePClivIsuSoF2REQ442fmOKdw1uFsI9OtnGCcDUSMjzX6P8JUR4VSGgeIrpBahzM2WH+glKXUJmAS0to0aVKlntZAC0r8eF3ENUIJqCbTUnxowbn8/bPpkj6ppcL1CAaqs2gVkVrLC89vpVYvXPN3CgVpFjguUIojPnr1Jn7/Nz/H/Lm9RGWFFkekI7RuEGOWDc/ccwllhQAuNVjxo2pRDWIJAUXvgofncA50BNb4DHkmCWruP2nBOsFZCXHQLMPtrVcRQUcx4gCliXTcMA4tOGdDAiii3NlBmzj09CTT9enQmUijlbcYRRzO+R8IrEOJT/yIhHhmZumGWKgoHw/FOTRCaixoR61W49777/PdkQq8IyhIs8AxRymO+NRHLuW3v3gtC+Z1EWmLFkGhILUkxhOABHJsFm9n21xTZtr4J0Ku+dD9fLxToSTx0xldmNLoGkkj51xw1bUnTheCjCJAmkuE0rSOUhrSBFSM1hFaRygdAdo34xBHGshPt5XpbLMYkzI9PUW9VgPn+xz5NQmR1iS+N1yeFW920UU0Pnig8JImQ6QUzkEtSdj+6mvvyOdUwKMgzQLHFHEc8bnr3seNX7iWeXO7QQyimugu+MquZVMjPWKtbdpmEVHeSmzap2GZNhNiazs1v11ycs0sVGvFu+5BV5m7/llWWxmsiB94JilGNKI1SkVoHftyRydYlOfbQHw6UnT3VkiSOtNTU1Snq1hj0drh0gSlY7yGM7MuFYjG4TWdYi2isvXqPCSA89ZmgXcOBWkWOGbQSvH+SzfwW796HQP9HTgMWnllYlYFA+SEmT02xrSQW16WKA1mzcgRyEm0eZtYsMYQsjhY53BWeetUPD06Ce65U42YoujwEsnjo55gU6woT5pKex2l0iCRf6y9FSoS5EYIaZKiVERXbz+V9oSx0TGq1RpRXMK6EGbI2nsoFSRKXpLkudSL3Y11ONGIg7179pAUrvk7ioI0CxwznLF+Of/HL3+M3t4KomxetUPuIofBDVpaCLGxX+OxyyxSa/G+s82CkMGJ9ft7CaQD6xAHDl8VBF52ZF1IzGT3ovJYpidUnQvPlegma4+Qztc4pbGB4FCeNCVNUZFGqVIom/T6UgtY41BRTE9/P5VaysTkNGlqifISS4UTP79cUIhyPpmllLcwo5h6PUFw3HvfZupFEugdRUGaBY4JFi8Y5Cs3fJJ1qxcACVqXQr4mzM0JsTvwlmIWoWwuh8wE61lppBaFwusYVSA2F6pzyKzMQJjWNNx6Z/0wM8hN2vze2TTjXu8WiwmVOYLJEk1ZxZFSXp+pIx+LVBonJmTxNcpoRBuUKqGjCGNAKY3SGiMgEhGXY/oqHVSrNapTdZzDu/iiQzw1c9VDRt8Jxlq01ux6bTs//OEtRRLoHUZBmgV+6mirlLj+uqu44JzViLJEUdQYBRGIMCPDhuVJ02OwxmKtRWmFyqU5gfqaiJSmx7l7Lp6srG0QtLXWW3RNLvxsf2exz2ybzSp7srET2iLGBKJTPkMvGqWtTzZZi9MWXOxjqCrylZc6WKQhx1WuVChFFer1hHqSEkcRKEWSWpQobBDiQ9CoWsMD993HE08+9dP4CE9qFKRZ4KeOC85ey3UfuoQoVkSRQutQYS3kMiGRTMAN2tuNIRZIEKQ7IhHSeoJoLzav12pAqMjRGqUUxpig7Qxtf50LhGuaSLWRkZ9pyWavoflvMoPVu/MgKIeva0/rIc6pAgmm3sXXEUprUBEuNRhVRyRGaeOlU8agtPUuuNJIaEbS3l5Gar7MqVxpY3x8KifLTMcpQHVqkm9/+9uFlflTQEGaBX6qaG8rc901l9Df14FSaSCrjLAyKVGW3Gkkanwj3waUeOswI7nsb+fIe04CTdtbSc+TcoMYmxNPh0CyrkRhT0ce7xSk4c2H4ztjEXFgLSLGW6LWgFGIinHK+DJKZXCu7N1+5VDWIkGupELG3WlfY58YS5okjaU4r+U0KATHo1se5vkXtr71D6bAm0ZBmgV+atBK+NAVG7n84jOIIm8RZkTpNeoZLYaKm5BFn5n08fu7nBCttSFD7pttzCoxaoqBAqFs0jUIz7Val9l75ML4nMh96aLL1xHW0lIr7prCBQZRXgOK1ojXC3n3PUyWFAuiLM5FKOtw1uC0JooiTOr3FadIU29JK1F+xK9zaKWoTk3yne98h9GxsXfiYyswAwVpFvipYdmiuXzuuivo7mpDlAulkB7NEqI8Gd0iXG8Wp7vc5W4lw8wNd/mY3JnjbX1SyGfKG4SZkWV4rWu8v68EymRQWSOQTArUFG/NI6oN5I+sxWQJJ7GIsihdAodvMqIBFaGcw2mHWOXDB8YQRTFBAxVUotqHMfBTLCOEp598jDvvuuvwlnKBtxUFaRb4qSDSip/7xHs5fd0ydCQ0l4w3W5JgaXz3W3WXbwjnwHmX3nOoa2g0bSMz3ix+b07qgPFlj3lz4GYizGKINtdttq6plTBb1+V9aovDYXDKoaxDVIwoB8agdAnrLMpZlNYoF2GV85aoDqQpikisbwiiPAnXa1X++lt/xfDwwTe+PgXeFhSkWeAdhwhccM5aPv6B91AqRYg4oki1WJet962vn0mYh81uNzvIrnX/ZqtyxtEPeS7XaQbSbLZmRYTGZB8h8+Az0pwZOw0Ha343nHUY51AOXzvvbLBTQ79NG2GVj286B2J8El7roDDANy1BhOpUnYnJSbRWud60wDuLgjQLvOPoaK/wyQ9dSn9fR8hUA1mNzRsQ5mw4HGnSRI4znz/8cQ4lu4znPFFmcVW/sUGk2Tlkbj35ebyhVewczhmMA+XT8Lmr7qzFKYPoCC2QOlAqmxFkUVEMaASItKa7u5vf/73fJtaaW398J8aYI793gZ8YxWC1Au8olAhXXHQ211x5AaWSDrpM3UIs8gZMOdvzsxFiiz3XQqiHJ9BZrdWWGGiKr0zKYqQhfikZYTas2+b1zvwxyJ+D0IPdeRm+tT7xY1JcWsOkdUxSx9TrpEndPzYJJq2TpglJUkecycMPIrDylFP4P7/+7/jcz36S3p7uI17LAj85CtIs8I5i1fIF/NrnPkylEiFicxdTNZUfHs4yeyOynFlPnpmIMxM8eTkmRybP2d5HlKNhYWYueqYpbS77nN01dhx6XF/aKbk166zFmdTP+kkTbFrHmjrWGKxJMWlCmiZ+W+rnoDuT4DAg3vrsGxjgX371d/kPf/ivmD9vaNa1FHh7ULjnBd4xVMoxn/nYe1m3ehFau7zaJdNdZi56M5o1l/nes5DkIdIgaLISpaVJxxGTNHkGPBBiU8TSH7vxt28U0nq4LINvbWt2PyfUJilVQyPaJKHCS5Acyms5xfmacuuwqUJUFDomZR3fBZs6lLJEzvlu9Uqwoujo7OLDH/4gHR3t/Jt/93Ve2VG0iHsnUJBmgXcM61cv4aNXX0CpJPipEw05kXOHSotmSwC9WcJsHEeYaUg2BEOzH7t5DS2vC9IiFyxMpTKZkeSJIl9+GeXkObOaaOYaET8f6PAIOk3nvAA+l0f5WUAqWL9O+x+JyEWg8UkjJYiOuPzyS3E4/s//739h64svF3HOtxmFe17gHUFXRxu//JkPMtDficoa/qjMpW00upiJI7nqM4lt5r4N0pwR02za/5DnDtFqti7Lk7vGOfB9P7IyzCwhJBhjc/e8ecZ63jW+dZU+CiqNyGbzG1rnM+vWWe+amwRjvetu0xQTtlmTeFfdpl707gypSZEoQpcrXPm+9/F/f+NPueqKy94wZlzg6FBYmgXedigRrrp0AxeffzpRpEMMsDlbnnXraX1dzllHyEDPdHUPsegO57q7I1mrh7xLU8OQRs161hlea0VqDF7j2RC4N+ROTRasX7Tfnkc4sxNX2UnlC7H500GK5AzY0NJOBGUas9B9U01QOsIkNaJShSRNfdGA0ixeuox/9fu/i9aKW350RzFs7W1CQZoF3nbMndPLF37uGgb6O9GRa0iMZiFJT6YtW1usP5hBPIdx0bNEjR8a3qgU8nHOpooefGNh4wTV1CSkkewhxDUdIhZfXglONMYK42MTvumGs5TLZUpxTHMGPYultlqb4VwJsqIZDp4DrMKvs2kNFudnGOEtW7HePdc2JKgiC67kh8NFJYwk6FI5yPOFqFRm0eIl/OHX/iVdXV383T/+c9HQ421AQZoF3lZ0tJX5tc99gLWrFvjkj7i8OYaQJX8c3nZ6YzjPrEEE7m/N7d98qWO2jydWk8UbsV4iZA2gMVZhVYmXXt3Jgw89wfq1Szl93Sp0VumDBTQ4QUkdoYZxMVbK7D04xXMvvc5Tz72MSS2TE+OcsXYVF2/cQGdbCaSOdeH9JCNyz/US4pF+9LAmbTp3CW66uOamINmvS8jYIz5JBLhUsMr5GDH46qKoDWMciMFSR0clkPDDEJeYs2ARv//7/5KVK1fyp//1Lxgbn/gJP+WTG0VMs8DbivPPXsNH378pJH58LXhry7fWZhwzMdOSFJGm5hqz7y9Zy7gg58mOniFrEWcdGCt09g1hdDsPPfY0CRqkxPhkymTVUjcRTtpIbQQqBmJEYoyF517YSs0ITndC1MPWl/dw932PUEssxja+TEKjNj3j+ubTzQszQyVRlrXP71v3Cvs4JMinrDGYNMGkPq6Z1BPS0NnJGOPlSdnsJAGUoqe/n1/65V/i8790PR0d7Uf1mRZoxVGR5te//nXOO+88urq6GBoa4mMf+xjPP/98yz7VapUbbriBgYEBOjs7ufbaa9mzZ0/LPjt27OCaa66hvb2doaEhfud3fqdwG94F6Giv8DMfvYz+3i5Kpbgl49xKmA0cLs44E837NA9Ta45tWtc8UM2TjbW+abDWvunvyy9v547b76SeGi58zyWIrnBwos4/fvcW/v47N3PfQ0+yb6TGdFqhaiqIVMA45vT1cOWlm1g6b4ClC+cz0D3A9FTC7n37Ga9OeGVA0LyLE5TLTM6sjl3jXIRzrV+54L3PQLMwPqPRjEqzMEXoCepcSBR5fWea1EnTOsakLU2YQSiVKvzyr3ye3/gXv87gQP+b/VgLzMBRkeadd97JDTfcwP3338+tt95KkiRcddVVTE5O5vv85m/+Jt/97nf5u7/7O+6880527tzJJz7xifx5YwzXXHMN9Xqd++67j7/8y7/km9/8Jl/72tfevrMq8FOH1oqrLzuPyy48m7gU4zAo5V3o5pG7R9ZMejSTZ9b2LSOA2fZp/O1wZMTpCUsImkfjb3t272Xv3n3MmTMHHceMjE6wa88B9o9MoivdVLoGefal17j9ni2MTMLYRBXjLJWysGh+PxddcCanrV7K8sVDnLZuJW1tiiSdxjn/o591BxVRZP+Ji8BpHBHeUZ+RoGq+OuGHJSfMLHHmsloi/58NPx7GpOAMznrRu0lruGCFmjTJx3qA78/Z1dvPL/ziL/L7v/fbLJg/7w0/iwKHQtwbFsoeHvv27WNoaIg777yTSy65hNHRUebMmcO3vvUtrrvuOgCee+451q1bx+bNm7ngggu46aab+NCHPsTOnTuZO3cuAN/4xjf4yle+wr59+yiVSm/4vmNjY/T09LzVZRd4B7Bq+QL+7I9+g3WrFhJrSxTrnABaNYyNksTD4RBtZlOz4cMlgZxzoYbbi8CzWeUutIKzVoFUqKaK0WpCNbE8+9yjdJbbmRxPiKI2Nj/8AD/zqc/y8rbXuP/ee+ntUSxd2Mell15EFHuy01EbzsZMTydosYhKiLXxZGklxG0Fm8mJXNB2hnirFRs6KXn4JJTO+3Pm54XCqUwRoEKm3E+yJOhHcwtUKURHiI5ROkbpCB2VUFGJOCqj4xjBD4Rz4hNgSW2KW2++mX/1b/49+/Yf+Ek//ncNRkdH6e4+cinqTxTTHB0dBaC/35v6W7ZsIUkSrrzyynyftWvXsmTJEjZv3gzA5s2bOf3003PCBLj66qsZGxvj6aefnvV9arUaY2NjLbcCxw8q5Zif+8QVrF25kEgLqmV8hTRZiUdO/hxOzO4nN6pZ953NrXeuIT7PdJtKaUCo1xPGDo6Q1ia54NwNnLZuHeVSmR07dnDOOeew49UdbHnoYSYnpjjllLVsvORSpNLBlse38r///iZq0/5L015xdLRBORI0sU/aa4Xxzd9wYv35Nt+L9f77zPPGNTVSDuoB11yllLnxQta3syG9Cm66SXEmwaYJJk2xId5pTIpJDdYZP69d/OyiKC7zvquu4mu//xWWL11yVJ/3yY63TJrWWn7jN36Diy66iNNOOw2A3bt3UyqV6O3tbdl37ty57N69O9+nmTCz57PnZsPXv/51enp68tvixYvf6rILvM0Q4NILzuATH7yISDuU2BlxS9fyuOF6Hh6HJIKcm1EW2bpfQ15Eo0NR/u6BOMM62toqxJHC1KeoaKG7o4OzTz+DD33gA5y6bjUT4wd5z8UXMNDTzZyBQdo6ehkdt/zo9kd59rkDTE2XeOSRZ7j7rs1MTtYQV8aZClq1kxgX9JSZ2x1UAmJx4QYz5Egwi+EdVp255k05oca1a2SWXNBzOpvibArW5MSZu+k2DSqCbI2aqNzGRz76Ub7+H/6QxYsWHvEzKdDAW5Yc3XDDDTz11FPcc889b+d6ZsVXv/pVbrzxxvzx2NhYQZzHCfp6O/n0Ry9jsK8TpZyfDqmgUbzYnC0/PGZamG9m35nbssZDmT7TBXlSVpM+MnqQPQfG0eUK84YGKZciatNV9u8boZoY2rrLXLDxXJQrEzlLZ6WETQyPPfIkkXTQ0d7Lzbdu5plnHuaUUxawYlXC6PCrlHWFBYsH0aUIJaDxPw4KvAwpl7Vnde1vrkLHr78hgJ8tkSYiqOw5HNgU43zZqgmka4P8KkL8mGOlEa1DyEJx7sZN/N5Xfpt/9x/+iN279xyyjgKteEuk+aUvfYnvfe973HXXXSxatCjfPm/ePOr1OiMjIy3W5p49e5g3b16+z4MPPthyvCy7nu0zE+VymXK5/FaWWuAdhBLhmisu4D0XnIGOFLgUlVcOulm/5EfC4eq11SzdkA6xMl0QsTdqJsnDAfl6HKIsccmP752cnMKamNRYolKJrt4uqrUqU2NjrF69gkiliEkoi+X8c8/g7nsfYf/IQXR7N4tOOZ3v3XoPE6Pj9Hb1MPBiJ++94mIqpQjEoUyKdb5JiWSJKZilrJJgTDafY6B6o0DCdcxKUAnVQE3nlR87/KlwfjxxmhUQAE0lrJGAU34kME5Q5Qof/PCHmJia4j/9p//Cnj173/yHdhLiqNxz5xxf+tKX+Pa3v82Pf/xjli9f3vL8hg0biOOY2267Ld/2/PPPs2PHDjZt2gTApk2bePLJJ9m7t/HB3HrrrXR3d7N+/fqf5FwK/JRx5voV/NrnPkKl7C0sUVk8k1b38yhwSCONfNuRtZqQObU+phnCgsGt99MhOzvbWbJkMb093dSSlGqtztT0NB2dndTTOipSpDZheOQAu/bs4uDwAWJJufTCszn9tCVc+zNXsWTlPOYvXgC6xP7xKT7xmU9z7ns28syLL/L67oPcs/kRHn3sOUbHqziJcDbCoHFo/ESfN3NNWhvK+Xr3EKJoihVnz2VSp/yknfNaTpOSmuCiZ/f1OmmaYo3BicMpjSi/xmuvu5avfe33GRwcOOrP7WTCUVmaN9xwA9/61rf4zne+Q1dXVx6D7Onpoa2tjZ6eHj7/+c9z44030t/fT3d3N1/+8pfZtGkTF1xwAQBXXXUV69ev53Of+xx//Md/zO7du/mDP/gDbrjhhsKaPIHQXinzS5++miWLBtDaf2u1alTpzNbaDRpf9qwa25LFIiXEAsNrbVPMDhvSK1nMcqaPqppccciraIK8J3tPpSAWg1ZCSZdBhNGRKRzT9PW0MzU6zsjwMOIc7ZU25sztR0UpYxMTjE2MMjAwwHm9PYguMTExyRZSdu54hb179mIS2LdvlHs2P8aSxYt55vntfOADV9FeKREri0hKpIKD7sA0BSqzdbss4+OaLEeyuC5+WJzzVUANw1VaY7h+x6YOSTYkiITUacDgUoMSjVLay8KUn9GupMz7rr6a5557gT/7s784ROZVwOOoJEeHsxz+1//6X/zCL/wC4MXtv/Vbv8Vf//VfU6vVuPrqq/nzP//zFtd7+/btfPGLX+SOO+6go6OD66+/nj/6oz8iit4chxeSo2MLAd53ydn82de/TGdHhNKEcbyZxpDDkmbrcbz4xuZf+jBzJyuXDH87sdiMNIOUKJs3nmWbs4xzFtOk8TC327Lss4/xaZwTrFU+tuc0I2MTKB0zMjpGZ2cnPT09pKS88uprRKpCvZ4yMDiYZ/NfffU1nnryGYwR1p16Og8/8hiv79pDb18/zjouu/QSNm++g+WL53HWGavp7+skEr92k816z5uDNJFmODdsa8IoG+gW2kU1aTjDtsCkFkEkArxESSlFFJVwuoSKykSlElGpDR3HaO2/c9YYnDHgLK+99hq//9V/yT133/2G8eV3G96M5Ogn0mkeKxSkeWwxb04f/+lf/xqXXngqUeRQWkK5ZPYlf+PKHgAlLpNWEjjCz8xxmdvprUfrGjN58nuCy958zGCuvRnSdCFB4p2tCGcF67w0fXxiAhGhvbOTAwfH2LVzH+2d3dTrCV093RwcOcDcuUN0dfUwOTlNudzG6PgU//TP36O3b4AXX3qF9773Cl7dsYPnnn2S1SuX0t/bztKF81i5YglaNSzNrHDSLz0PCB+WNG2eZPPxSZX15vRBz5AEUyC+H58LpKx07KdfxmWiOEbHFXRcRkcRWutc52qtJUkSHn/kEf7Fl379pEsMveM6zQInH7RSfOIDF3P+OWuIIoXSKifKNwphzqqnbGpO4fv6BGmOZCJ4h7KCcgplVV6emB/LucZrckjTrRneopTQ4SgSh3IpuASRFK0Mjhrd3RU62jXYGspaFsydx7yBOSxdtJja1BRtpZiuzgr1ZJK9+3cxXZukt6+Ta6/9KOedfzbz5g8QlxRPP/Mkfb29tLd1cPbZ51KrG3AK8cMuDntd8jPIL2ieycnF881V6tCUX2q2QJuO76zB2tTPGkrqpEkNm9ZxxuAcKIkQ5W9Kx5x62ul89rOfpVKEzA5BQZoFjgprVy7i5z95FZVylBk3b4osZ8Y1fXefTB4UOgyFjHee9MmeCQJxrzC0gPFWaE6cTZU3NHVXb7xjeL7xOBeRY1EYFAZI0cqCS/w4CQUDfd30drdRKQlaDLWpcdoqMZFSTE5MYNKUUhQzOTnJ+Ngo/X29vO/KK4i00NFWYd7cIdafuo7unm7Wrl9PFEcNu7eFFJvWGyzmFiszxHybrx8iuWCgRVnAYZP0OGt8bXpSx6ZJGOwWrrdSOKVxShG3Vbjmwx9i9ZrVR/5wT0IUpFngTWOwr5v/45c/waL5A5TLUZN16Y46U97cWCNvSREsInJCE6wDH9FMcZJiXQLO5IpH5/CWm8uqvhvHz112GnHCTNIjaIQIEV9i6F1dfwSdn4tBqBNHhkgbYm2ZN9SPTeqMHjzI1Pgk7eU2yqUKe3fvRamIifEJpiYn6O3p5HOf/RQLF81Ba0/IIinGmey0Z6ApPtlkgecd3vOyoEzw3kyyrWqD5mNKi+XpfDFl6Apv02yQmwlJH+XLL3WMQzF/wSIuuujCt6SCeDejIM0CbwoCvP/yDVx+0ZlEsWBdmuUjENUsM3rzx3Su0b9HQtZXhZpqEYWxIEqTWosRR+p8/zVRLrdMG5Srg7t/6MozgXirG6zx9dvK1387CW67yl/jHGgxiNQRSdDK0tfbzbyhIbra2+lqb6M2Nc2Oba8QEdHb1cPUxCQRQixQimD9ulOYP68frVNEG0QadfRCU6Y8vyhNhAk4UTglYVNDn5nHah3MdMdFsnOdXaKlEKwxpCYhSXy5pd9PIaJ9/FNHSFzmA9d8KC+TLuBRkGaBN4X+vi5+5sOX0tVVQWlLFPmxD6hMJEiTy3tkNEokG4+9FSiB+BQ4hVIRiQF0mdRpLDHG+Q7qBMtSXIuhNSMUkFmYM2KpKJ+JdnlrYMiSMblYPEIkQgS0ZOWhFq0ccSSUShG93d3MH5pDX3c3C+bNJanX6Olqp1adoKO9RKWkKWshjnwfdm8hh5UF/VQmOPcLayI/8RMmD4E0k+NscduWE2263hnBNuKpNjWhlVwCzserIx0T6ziETxTLVqxi48aNh3+PkxAFaRZ4Q5RLMb/06Q9w5qnLEXFoLUFOaBsWU24JvbnM+SEEmwu0BWvBWMfkdJ2nnnqBV3bsZ2xSkdg2jK3gqCDEM6y0LN6Z4VBC8eSkGntLFk11TX8Hladk2XYgc+URcA7fjyShFENPd5k5A51Uyg5FjYmxA1RKoEloL2kqcYRYh1hBuWhGbPVoMCNGm93epGmfRSpSHFYaagecw6QpJk1xxseKoyiMDY5KRHGFyy67nI72onFxhmLcRYE3xFnrV/LJay6lra2EwyCicF4NCLlllrnmb0yc/h4yJxMA8brM1BiiUhmTWOK4jVrN8P2/+x5xuYvT1p3CGetOYai/C6UkHMT4THseuzwyMqLIsu0imRubvVw1LDTwvTCDG68QDAaLAZcSRRFJ4t8/1pqu7hLlch8IdJTKOJMGXhMvDXJZl6KGpd1qBDfCHK1X8HAE+QbnO+NpG07KNdWrg08OJfU6outoHVOOK1TiEtO2jopi1qxbT19/P5NTU0d+v5MEhaVZ4Ijo6Wrn16//GIsW9oMYSiU/VEypEEN0DQnMTBy5+YYnPIfFSIpRKUY5oqhCUk9RWpDYMbRwKRM1zWuvHWDv7gM89cSTvL5zB44UHEGGpLFIEMAbkBDzFLCBgLL+lpnMaaZn6y3LzHX1r/A2pwqPPNFmNeBRFGFs6kMUzmBdQjkSOtpi2koRiEFFkkuyXBj/eySHulVm1LiGh+41kz+DBZ01KnZyiOueFx04izR3uA8JJmstJq3jrEEDlVIbsYoQNMuWrWDB/PmHWfXJh8LSLHBYaK34yPsu4MLz1yAqDd2LrO9t6WgIq3McRZ2EOASDQ2GVL5IU54hcZr+mJNbx6FNbGRmfZuM5Z/HRqy+mq62OU9N+gJiLiFxE6sCJwSpQNpTWSEPE7sSvV5ooEXL6aYolZiTTPDNdYZE8butfkYnto/AevlUlzvr0kijMzByPcvkbtzjaoRrocPZ5Q541M5bZgBPVlOwK7yANL0DC3yr3Dnwz4tBi3kuXcBjrk0OpSYl1Ca0iIh3R3tHJKatO4cGHHjrSJ3rSoCDNAofF3ME+Pv+z19DV2YaOLEo1pEUz548fCUeyOL00s4RyNlSk1BEtWCL27Z9i8/2P09/XwSWXnE17u++UnriUbLKlc4nP4usEZwBXJhvZm1ONa0QrPfn5bumtmP183nz19aFWnWt+qsnlP+zr5ZAtLX8f8bXN+7XoP7M1Zc2MZ/xwKEGU+BivtX4wmzEIWUbdoZSwatWqw777yYbCPS8wK+JI88kPXcKqFfMgzPuZSZgzm+nOVm9+JMLMqAyrEVtGO41WBrBUq8K99zzJ+FiNc846lbmDFWLth9/WUk1KG6mUERXn83KyeGUWK/WkaUBMg0Bz+slGaWRZ+9asu4iXJM3mT1trQwlmiFeqqJEBz+O6zbN+ZmBGQxNEDskPiWT7BbF+fq2z4zfVrs88fNMxGp+Pymuumi5RS1gZGp9XmvqZQz52DX0zGoufzCgszQKzYuPZa/jstVcAhiiOcivzcELn2bYf0Qp1Ga2RayRxFuMSjKvw+OMv8/DDz7Fw4QI2bjidChZlHbv3jbH5sWcYmLOQdStOobcSI86gVJnEhKQQ3qAUmv1eRaNqyDXdH3ImzMqUzXsHq825BgFnbnCWUGqxNA97jbKa8+x/NF4cYqz5cfO1Nb/60MCsy11zyV+bEzpN75G9d2aVi6Czccs4jPGd3q3xbfXK5cobWLsnDwrSLHAIBvu6+OLPf5iF8/uIS+TWTfNws6Mlyew1jabBYFUMWES8TtCSgkRUq4rbfnQ/taph+fL59He3E0kNrNDfN59SZT//+x9+xFDfQ5x/2krOPG0F/YPdiNJZnRBgaI5iujCYLEwr86w663IPl6pp3uaahPQy213LcVobDDeRZ8s1bNrWVPUjzewrzftmZyb5b4A0HdNlVm4g0pbKoHAvIWGk0IjyzT+UKD8W2KQhxllHlG+mXBCmR+GeF2iBCLz/snO5aOOpiLL5dzgbx/vmm2Idul8+lzx8oa0JfYdsFeuqoIXUlXjxpT0cGB5n4cIhNm5cT6zrKBJEDDqKmDM0n1oComO6+noYmxjHoKjWhKlpjXPtGFdGqTLGhHUoh8VglcGIaazSNVvQjfZq+cVossqy/Zsrj/LR5oe5loe7Mg7f1amFe5WA8kma1mM2E7lqaGPRTYQaEl5N1UaZJdlMmFn6zVvEjec0QbuJ8qL30MDYOZ8gmp6env1kTkIUlmaBFqxYPI+f/cR7qZQ1SjkvqRGLdSbP0GYW48y/M2SthZ1kA8+yQWHk333nCBMiDVFJkySGxCgmqjH33P845bYK7730PBbOaUeoYlwdJRpjI6q1SVJbY/nKhZxx1hraSgbnIl7dOczttz/A0Jx+1q9bztBQN5VKidQlXqIkPvGBKLDeqjKhy09GhC1EF7LvrVAtrndzVdMbIW9SksWACZZi5oKLbrqmzRYphzXzRATb4oJn/TQPtZhtWLASPzI4049K/rcfsVyvpxiT4JzJfygPHCjG/GYoLM0COaJI8zMfuZRT1y5DKdCRwo+KyGq0D036NCd8AHy/yqwfe3OyqDUp5BUvFrGQJA6nShg6efDhrTzz/HaWLJ/PmjXzKUsaGhGrYKkapqYmcC6ls6udUkkh4p+fM28xlY5e7r7vAe665z62bHkcaxQiEb50UpMaf29D70ilohnWGIFEG9flEAu7OWzYRLhHRGt2Jr9JOFijEV5m/WYva45cZlU8jURQfoymz6Rx37j+LvTWzAjad20PjUqUQmvtY5rgM+hpGsYKexnSSy+//MbneJKgsDQL5Dj/zNX87MevpFKOUdqGDuXhyeYvYHMbskOy5C4nTDh8gkgArxFSiCtjXZmXXznA7Xc+io4rXHb5RtorBm0MVsqAC9ZPxBOPPUOkyrS3dYBorI0YGatz0623MzY9yS9+4edZOH8A7VJUBBCBLWEtoEtYk6KljpCNBp59nbPHNpuMviwsekjM8fDI4o6HWKazuuNN1qI0rTHTnQbLtRG7bDpCUywzbECH2e8iClGNpsVaa6IoQkR8cxTjbyj/Y1edrvLi1pfe+OROEhSkWQCA+UN93PALH2OgvxNRNkiM/HPZF2tm7XiG1u0EM8xnJ5przLMadee8NYoDY4THnniWrS+/xvMvH2BkZIpTVi1h4YJBIl1FuQjnYlJrQjKnTHdnD9a8Rl/fHFJb4uDeg9z2480kzvHBD17C3HkDvm9kTfH67oM8+8zzvLJtB9O1OpVKB4sWzePUNYuYv3AIhyPSGgnzcDKLT1zDgmuQZHZBsrvWAWhvyJmS9b90GXtmV6UhiGrOfB/uMJ4x8cGDRl9SkLxEktAtqsnEDT94Pm6rJEKCDEmUJ1FrbAhXpORNWIzllZdeYteuXW90dicNCtIsgBLhY1dfxMUbTyeOFUqZvO0b4CuA3iRy1zBrjGGz5sKBgGxjpo+IgNKcfvpZDM1bTkfXc3S88BKmupfXt71MZcUCnLLANIJgJMIgRBVQklBp6+Cl7fu56Tvfp6u9jc986mNU2hzYKmM1y5aHX+SWm++lVk1Ys2o5K9cMUS6XeHHrC9z/8P1ceuXFbDj7NDpxaOsQUViBTFKkjGCVolGE6f/zxGpBXOBRQezM9HJDR5ltzxI/jU5KhB+RVnJrXMxwnx/XhuMGHargww4Ei10Chbos292IdSoHzgLih6iJikE0EkWgNKmzZP+58H/twJmUl55/noPDw2/638C7HQVpFmD+UB8/+/EraG+PUMrm/TFnTpY8XBXQTFG70Jooan6ueZvvyamwzjA01McVl21i08azOXhwPyPDe6nW+onaIrSypNYhEgPCxNgESmle2badZ558jPPPP5tVKxZTqfjjToxbfnT7Q9x51yNU2tv5yEev4rRTl9DZHmGtcPaZ63nu+Zf4wY9uxtkSG888lS6tcLaOxIq6ccRKg5hGezbnxfiNNIDgR1dksvmG2Mnlc3xakSeQWpJN+bNNt+br5g5Nw+dWapaY849bpUxZ7NLvY8KxlfiWe+Az9arJTTfG+Jv1dfvOWurVaR7ZsoWpInueoyDNkxylUsSnPno5SxcP4b+gvmwuEzzPtDLfSIs583FzHLT5eaVCFtdaRHl7CRE6OoSO9kGWLh5AYdHisDb1NlDoej41PoUYxxOPPMoHrrqU5UsHaa+Ac1WMLfHk4y9z148fxTjhissvYsM5y2kv18HWEFUmKkecvm4dW195jdvueIj1q9fT1iWUS5parY7EFarWoRWIWFQIOfjlBxWBC93fXTZ6OFMM+PPziS8CD85iPc6KzEpvZJkaWfpZYsONq5nHNQnudsOS9esT659X+Q+iRUmM1j5jb4wJ8d0sC+Znp+/ZvYt7Nm8+CqnZux9F9vwkx4bTTuEXPnUlbW0RSgeJEUeu/skw2xdpNuLMv4wt2xw2TcFZIjFoSdEqIYoMWqU4W0cFMrIulEiKw6UJpjbN+lUr+Mx1H2XlsgW0VwRnJlGqjnUpY5MTOOdYsng+Z5+5ilKUEFmDmBRtapR0QleHsGjRYvYPT/DEc1txSpOmhpdefI0HH3iavQdq1FyF1AmpiB/5K6HevcldBwlTM12u2fTdlLKT9Xc2bJcWizJk0JtuOdnN7DdK7pRDZt3muzcnjJruJRwHQlKn0dRDRBFFEVEU5VZmFkoRnI/xWsOdd97Bnr17j/jv4GRDYWmexOhoK/Pzn7ySeUPdKO2F7EoJxrpgCao3tDBmZs8PaXgRCHM2197LfcIgNQCxiPMSIqUUzli8n6i9dWcdztU5b8N6zjjzHHp7OhASxNbRSpHYOgZhcnoUK1VWrV5Ee5tQUgqXaMoqwtgEa6dJncPYGhbHrr17SdxaIlViztzl3PXA7dxx/3NsOO80Npy1kvZyCVQJhUEkBAczSxJA0qC9zG6BnMjc8aaYblM40x+kuZ48f8GMFFN+xfwus2gwWw4bkkA2UzNYhVK+075kMiOt834CebjEWhQOFT6v0ckxHt7yMMYYCjRQkOZJCqWEj7//Yq685GxPVqLyqh/fA1LNKlyHQwly5qTJ2aZPziy99HrDrAckiJhMpBiOmb2X4GwJh88Kx7Hl4vec5V1nJn3+11nfGV37SY9tlQpKQU9vJ3GsgXqYKyS+1NI6Uguvvb4LRIijEqIijDW093TR1TeHR59/mfShR3j2mUdZv2YdG848l87OEk6lCAaNCokV6xMxovEjNCS/juFMmTW5A40svFOHWPWSE6e0vPTQT6PRrck17ZNZrVZJ0JISrqFu6tpOixeQFaGKCMakvPjCCzzz7LOHvOPJjoI0T1IsnDfA9T/zfnq6O1AqsyRcELJLSzInw5Hc8ZkkaW1rU7VZm+u6ELVzmUSmAZu5uC4TZSsgRZOitCGr1REnIBHWeTKMJWbunHmUdBvTUylJ4kClxMpQN4KzGkMbe/dN8vzzr6CdorOtHWcVqXPsOnCAJ7Y+TVwRrr7qvSgsD973CDtfHeEDH7ic/n6NFYNLHUqXSEwt7zMKEUiWMCJnuEY7tkBKuWvuH2UWZ+sFy/8HTcL05vioaxK4N46beeUOEY1zoLUOFmZmWXpdpgufU8uPXCh1rVdr3H33Pezbt/+Qz/xkRxHTPAmhlHDtNe9h7aoFKN1I+HgLpGH1NFf5vJlWbxnezPNeq2lC9VAYVxGCgo3uQUHag8MnYBp1M42/w1oFHBqcZvXKFaxcvoQnH3+a0bEqTleoI1gdYaIO9o0avv/D+xgdGaOno0xfW4VYIqbqsPmRR9m5dyerVy/l9HWrOO3U07jo4kt54onneOSRp6gbH9GcqKXUTQRRBykaJMoN5dwSz6+eX6l1mbwow2wx44abLyKh7LPpFvY5tAd8U0cjaQyJ84/DlQqfr9Ia0Tr/cTPGQFMSyJiUsdGD/Pj22wvXfBYUpHmSQQQ2bVjHpz96CeWKhDhmRpaavH9j/gJ8AobDW51vhVCdc2E8RcPSyRK3eb26GJwYkMTfyNq+acTFiPPlkRYwYrHO04ku1bj0inOISin/9E/fZ9u2YSYmS4xOaV7acYB/+O6PeOqFlxgY6uXaT7yf889eD6lly2NP8+N77qe9p49LLr6Uii4hghfWi+K5F7ZinDBVTbjjrs38+I7N7N8/RZLGWOt/bLLxFo0fn8zKBCSMC84bg7TKjBpyI5ped/isuWv6u/E4+xHxM+Ml3GfHF+VHdZB3hPLSInE2t1BNmnLXXXexfcerR/wsT1YU7vlJhq6Odm78wrWsWDYPEYNScZOVGWQuLnPvoNFC7dDUxMz67OBvt+zXqFjJ3HWXaxWthBJGF9zs5lcGXaQPvZmwDI3v7JMlRBpHdEEK5EiJ4oQlyzr5+LVXcv/mZ/mbv/muP79IGB6dwBKx9vRVXH7JBlYsGKTkIK3D6zsPUE1At2lee30vywbncWByP/fdfy/1tMbU9BRaaaK2Ds47bxP/7//zbe5/8DE2nL+GjRtOpb+7E9HeUs6rdaQ5qZORYkaETSMqMk+8mSSzCyXNG2YOXQuvkkZn9lxXqyS45xE6CgPiQlOQ7BfK15cHNYD45NH+/fu5+ZZbSJLkkHcqUJDmSQWtFR+96gLOPWs1fhSvQuvMsgmZ6qZkgJ8Q2dAnOmtxaAy+HM8TlfNJHOsjbNmIHv/1b0r0IFiXestSDA5QweLKIn1N6WYgjDkLCSQn2YFtvqttIhTJstDOW6IKYfFgH3Ovfg/D55/J3v37mZ6eptzWQV9/L/1DXZQrGi1CNW1j9/AETz77Mj3tXZy6bh0333ob99y5mdRMMjkxwrIl87nw/PMpSwUrNbp6KyxeMY9773uMx554EVNPuPTCc+nu6ya1iW+khE+wEX4YlHNekdDyqTRim4cUTjqV1xWBhB7qkA1785+TCpKnTGJkQXS4ZiFWLP7HRqIIFYevvEn9EE+TBv1r5pobnn3maR5/8sm39o/sJEBBmicRli+ex2evex8d7SUQkwudweVWJhCE2a1WY6PVm5cF+Y0Zy6kQo1RYLIlNibTCOeObRFivtQSQJkuxUWLomt4n+780EWhzNhl8q7qs63t2dhJWAhrtKUgJuuyYP7eLefN6fFIp/ACISkM8FUbHp7nrvgc5ODLGWeecwSUXXshLz79Mz+A8Vq+cR1s5Ze3KpcwfnItW/px15BgcGsA6WHHKOt73vkspqZR63fDa7n309fdR6WjHpIZyrHGpn1xpUoNonZ9Xfh2a4pVA0xy33ATNSdXNJNiWeGezk57NN9eQ3ZSEAnuHsw6s39fiCdRZw+23387k5OSs/4YKFKR50iCONJ/6yKWcunYpDhuqXd5YwA4EI0Q1EjTOhHESgrVCYoVa3VGrWtq72tGRwbgESLCujoRKHpEInA4370rjsnipy4kZ8FMehYZN1cqqrcvLyjNpGjlBdn5ekG6tA2XzaINSZfbu2cM9mx9k155pXnppDyWt2Hju2QwNDtBWKpPWEi7YuIHOdoMmQZPg2StFa81A/yCgGB0ZR0cROlLY1LJnzyg3//BeNm68gGXLFlOSCCU+8dVoezlbTPNoMbsw3jc2yeKqPjGkmhsUi7++NotBi49r4iyvvrqDBx/e8hbXc3KgIM2TBGesW8EnP3wZ5ZJGa4vS+C4OTRbMbGgkaZSP0zmTk6B1Cicx6DI/vP12nnhqK0uXLeeC885k4cJBFIpSbDHUUOLQ/tvqv6DS0A9mhGmb6TMsyTiHztaXW76qZX0ZfCLD/9XIXHunVmsJ43YdCo1LFXMH53HpxZew9aWdTE+m7BseIdaWSMHiBQt46qnnqU5P0dMVo1yClgjrQCkLVtHZ2UlHeycTY+PUE4MoYXyyxr7947yybSc7tn+HU9et5r2Xb2LuUHfeyLk12dYqIzpatPYGAIeCkGzK30tCfBNfGikuSMIc3trUgLPYNGXzffeyt6gAOiIK0jwJMDTQw5d+8ePMG+rxtdQqdC5qyjE0C9ln12MGmYvzVTENciqx9eXd3Ln5SUYnUl5+/Sk23/8MSxcNcfrpq1m/bhnz5nQTRQbnEnAG6xI/l8ZlRJeFBWzuKrboFpuNzcy1z7z3XJ6TvUZoGmaR06d1fvJklugSiTAmYbCvl66z+li3/jSee/55tr34NPPm9rFo/gCPPzzNC889z2DfOuK4qYsQQqRLdHR00dPdg00Nu3bt5fXXt/Pgw4/Q2dnDhz/8IVauWIEzNdrbfa9KJREiETYfPSlN5/HmIPmPzWylrp6AnQpKWx2E7GH+j2if8HOpDZ8pebWldY7du3fyw1t/RL1eP4oVnXwoSPNdDhH48Psu4PKLzvBt37SlaSqszxvIoXKh1r/DvfUZDWdDHFNKVGvCHXdvYbKm6RtahIimPjXO1u37eO6lV5l3Tx8rly9gw9nrWLpkDp0dFVI3RaTwrcecaWR+w4LFj0D3HZAyBpTG81nvyGYELzOcszRtd7kSQEugUQFxlihSWJdQjiK00px95mrqZjkOy4IF/VTaFM888QxnrFtGua8CujHNMkkcQkRbWxu7dr7Kd/75u0Sx5txzz+W009bT0V5Ba0FJBVwayC7GWhpDzyBUFfHG1qYLFrjz19673i1Pk0U7QZPHAYL0KetmZK31CT2bVWkprE0xScKTTzzOs0UF0BuiIM13OebP6eez176Pjo4SokxDMM2hxDMbmqVEKrQ8c0rjiEhszFPP7+Chx54nautn176DpC6lr6edwUWLmZ6YYnS6yt2bn+OezU+wfNk8NmxYz9p1y5k7p5uyWJQEp9w2Mrg40BKBAU02q/uN7TFv/B56TodW5ABiAIOEkb+RcogDXXIYl7Js6Vyuu/bDPHj/w2y+9yHef9XFxJUI60J3c6t5/oWtvPrqDuLYsv7UdWw87zzaO0sICUrqOAnX24mP46JQIpnatFVrdMiij3CizSL33Fz0mfIWt58Qz1QhAURD+pXHfRFSk5LUq3z3u99naqpoAfdGKEjzXYxKucQvfOr9rFg6F6WdbyycS4yCpZLXJreS0kzBuhIw1oA4UidYIsarjh/d8QAq6sSgma5PoUrCwelRJqoTlHQblXIHi09ZyN5dr7N91wSv/PPd9Nz5ECuWLeS97zmXeXP6aW+v4EydOAJrUzRgU0ukVODQhhmZhQUa5DBj3bP9GDQRRPZz4b3/4L4HsWikBINBIkUZOPPMtSxftozpyf1oragnKagyqVNUa5Zt23ZQrdVYvmI5G845k7aKIsJgSVHKBa2m89IrArnZTBoErQQ3E6GUtTn0OTNjnkFJEGg1ds46GomI77qPJ0wbMuZZXNg6h7WGzZvv58mnnp5lHQVmoiDNdzHOPWMVP/vxK+jqquBI8pkv+VRJ12qBwUy3vGHhWZsiWIxzWImwusJ9DzzCth17IepmarpGHGukEjE4bw4dlXaWLlzK9he38fTTzzJ3cJCOuJ1KKWbfvr3c+9ALbHnkWVauWMwZZ6zlvA1n0N0RE4khdQk6ymRKoS2ca5poSUPgfUj2v7lRxkzNZ4BFcuLM3FmFxjnf5Sc1hkjFpCahu7dET08fTqeIjXnhxZd46ZXXGJlKeeLp5+jq6eNDH76G7u4ysTLgLLEGaxyRjvIkml+zCe3lWgvxGmKv1m35vcwQtDclflo3CTbMLs8rvJQKjZTtIf0yfYu+lOr0FD+67TZGR8co8MYoSPNdit7uDr54/ccY6O/AYYgiP4pX8sh/a8fx2Vq3tSaGvJDdOnBKs2ffKHfft4WUErEuMTDUTVtnhck0pa9/AQvmz2fnjld44eWXMcqwd2QvgqMcleju6aF7YJCJ0XGeeWkfz7+8kzvvfpAzTlvJ2WeuZfGCAdpiQayXRvkFNvHhrF5tRpLN2XRBuSaCbXrWZj8ezgX9prfGRCl0sMCds1gMohIfJHCaufPmcmBsiudefAKbWgbnDdA30IdSVbSkiBJMCnFUwabBkJdGWahH6Ph+mJBDluwJjHnoc03nkSfwQpxUyKRF2YRJn4SyNoRZxAexxTWszF27d/Hggw+9qRBIgYI035XQWvHh921k04ZVxCXxhIlBSZRbXE5CBsJqsi+Tr/XOw4rYkCnyuk4X+luWSEwb997/KK/tGaOrbwHd/XModbXTO9jP+MQkEJPUa1Snp6lVp9HO+kmQzjGdVJmYnEbriLlzhmhrb8fZhL0j+/j+jx7grvsfZd3q5Zx/zumsXrmczvYSME2kgthIJJ9+lrelyLx1d6gVh2QuuLf2cgsuxBP9HB0HYnyduQs9RK1DKV/Zbi0QWqp1dpbYcPZprDplFU88+QxPP/0MLz33NKevX+4rPC3oKMZYF2Q+EkiTJilQWEP2V4vFGE5mhkU9Y6cWVz1EGsh1mEryY2Vd+LPJQjbU+IsCm6akaZ1777qr6GZ0FChI812IBXP7+eIvfJCu7hgRE1y1iLwAKIsRuoatmWVmfdVPkJTnjTO8vk9JjLUxO14fZvMDT9HRPUDqYGjBfKbShJ7BORilGOzp5bZbf8jwnj0o59DOl/p5yZKgnWBTx+49fsJhqVRm4cJlTE9NMz01zsOP7+DxJ7cxf+4AF246j/VrFjB3TjcaEOstXl9T47PFJnfDZzQbwQSDLZMKNFudLicmf/YmlGVGjWip8+ysxM/UcblG09DTqbl44+mcuX4FI+NjmMQQh9k71oUfIbJO7/i1uSw80ioTaoQdsvU0WZMyc9/Dxz8FcIFQ/YTJTIrlO7I768s4Dc7HpzHsfH0HN9108yGt/AocHgVpvssQR5qPvf9ili2a62U2uvERe+Oq8eVwACrrHCQ4FCrrNmRDxQgWwSIqpp4qLGXuuPNWhg+O0tE9j4MHhrn3zrvoGeznxRe3+hJKYGz4IC4x3jLMwmjhTbMctjVeVlSvpby6YxelUky5VGbBggFGR/ez47V97Pr2D7h3qI/1a5ZzyXsuoL+nQjlyWJLQgFhQTe61a5zZ7FYaR+6K3lxR1OQE45Mr/jhZF/Q0MXR1tdM30AvOkLXp8McJcUznKa1xyFmy+/4FLe812woFsC0Jn8wyDaNDMxlS5ppnjQVy61JI0xREYY0hqde5/4EH2PbKjlnercDhUJDmuwwXnXcav/AzV1EqabRWvlZcovCdnCVeiSGv63bgbOb+OnAWcQYrjsREqLiHp558iS2PPkNHRx/VqSptccx0tca+117HKC8PioIVG4siVoo0SfKkTG49OR8zTFOHVhH1mqFWNUxIlQPDIwz09zI4bykmqbFz9xi7dj/FI4+/wJVXnM+mc0+lvezdXhusTuWyOTyGQ0kze5SFJmBWg20GGnPC8w35y9K0jo4iYtEkNg1OdyM73/QSDh8qlBnrONyipOnW2NY8fTIrl8x0mRJGWZBVdIVfLv+5e8tz5OBB/vk732W6mDR5VChI812Ewf5ufv36D7NoQR+iGjEtrQVjzCx15pk7jmeSMH7COUOjda635kS3s2+kzh33PIzoNtIUpianAcGZlMhZxIAVl1f6aASTpH7qpAvuqvP9HR34xh8ujFxAYZwNyhxh9/5hylFER3sb3X3zcc4wPLabf/jOrUxOTnLFpRvpKEWoMAVScCFO6ZqSRpnWyOAF3z6O6zl9JmFBtqGRFDtUQJ6FMuI4DCSziZf0GNck82k+XjONSut9/rBBio6ZZJ/FOJsblEj+Hk55ZYATrycTaSSCZib3UmtBCTa1pEnCAw88wHPPvzDzIhR4AxRNiN8l0Er4xAcuZuOGVUSxTwapENNK07SpX+ZMZJaH+NCX9c0tRHyvRSTCuQgjbTz8+FaeevYVdNzB1FSdNDXYNEGcH8YVAbGDKNSLuyBxsca3K3PBxHMOT5D49nQifg1aKaI48p3FRVNNUg6MjbNz317Gp6fpH1pI6tq55UcPcPd9T1CzJVJKWKUwkmBDI+JM4mNt6ALvRYp+aFhQAbRCZvlbWv7DSznzZzI5lg7dnETNdozsYVNTDZEm8oXG5MqZhNkgS6Sxb8avWWWTP3wYUdIUPhDlRfRZU46sisr4C8T46Ai33HJLUTL5FlCQ5rsEpyxbyM985FLa20qhQCR8y8V/iUJKJ98/63bjtXqeLL0H5xDlSNMEFUUYp3C6wvBolTvvfhBd6iBJoVZL/XGc70Tkvf+GC+isCeWWgShcIysPgZQxIAZRvienkOBs6i3drGmxQOrqjE6McGBkhN6+uUxWNXfc/Rh79lcxqkLiFKkIiQNDhCMiNcondVQJkRgyEjdmhr8swcrOudXX2btGyEKcoFDe4m6KSTbkPs0WanNCKmzP0zSNt2wkofz9IYQp+OsmzYM9fDOOzDHIwhzZNfbEqdA6QpTKs+UZcfpmwz6uuXXrVp57/vm38C+tQEGa7wJopfjURy/j9LXL8woQpSXUmId2a+KTOs3fXa+ssbkMxXOJxTmDxWEsWImp1hW33XEfO/ceJCq3c2B4JHf5DC643p4Yrdd3Y60/prVNN+ewLgUMygWhvMu6h3uidKaOmBSFRXK5kM/0jo2PkySO7p457N43zgOPPEdKB+M1Ydvr+3lq6w5e2LaHPQerJLSTuAqJjakbhZMIg8bpOGTbM3LMrobkrm/jAh3uijfFGHOyyh5nLrpqWJeHS+yEGGSreF01vaZBxnlG32/Ij61EYUVCpt6XTCoV5aEC6xzGNX4ujTHUqjXuuONOdu/Zc7gTLHAEFDHNExwCnH/2Wq695hKiWOW6vEzEPlt5ZGPMLo14Zh5n830fUZACxmn27Bvn/gcfp1Rup143oLS3TtMUJRaDH00R6neC1ZklfVQgpyDDyZoRiyeW1GSzJgVnwnpVvhQsPvTgjEVrYc/evSxbuoQDwwd46NGnOTAywovPP02tNo5zCqXLaKWYMzjA+jWrOfOMdQzN6UNbQ6QUgvEljqGJB9CIFbbEIg9/xaWJwbLelc3WZVadk+/PTP5tTew0E+bs758dt1W3qQLBNmfslW9hhUPyHy4InaPEW5k7d+7kR7ffVojZ3yIK0jzBMTjQw69+9hrmz+3DWoOOdJi7nTX+PXQOefNj30mdJrGMt/Cc8m57YhS3/Ohuhkcm6Oqbz979BzHO4owljgRrgkWJ9ZYcTY2Dg+NuA3mHiThNsVVFpmb0xGB9CWC2EvGZcayXFWG99TwyPkqls43Xdu9l1+79KIG2Sh+V9nasCKMT44xs38eL23Zy74NbOOes09h43lkMzelGK4UWP7RDHUIazWx4ZOJsWKUqEO6M0simZFTWab1RF9+a+c6YrblE1OV7NyWJmshY8uOEkkmlQAsq1Jk3hyCyqZOpMRhj+OEPf8ju3YWV+VZRkOYJDBH4+NWbuPziMxBlKZcjCNMdJbN4XCNx0dwzM+e10CdTHLmY2zdoj3CUeO75V3n8qZfo6Z3DwbFxH+t0/gtsrG9GYTOLNU9Iq/CF10HyJKGphPOSppxWpdGYOLwi2+7jov6YPqOufBjBWOr1hI6OTpwTenv6UE4xNTnF/v0TqFJMb08/XZ3tjI8dZGJ6ittuv5+77n2ATRdsYMM5Z7Fo/iBlrVHibWMt0si8O0tetWOzxI3LL1lGSMGezz6JbJGtbjStln4jm53t0UyYM63OnBb99WhK4uWlsJJt9+SpxHdRIluxC4Prsrdxjh2vbOPHP/5xMZr3J0BBmicwFszt52c/cTltbYoosjjqXp+X/dectMiso/zbnEl9pMnqDORgfa/MiSn48R0PMVlVlERRr6chuZNldLJYWnh9IGZfXBJm9ojGmASlQxmmE4QI5wQlflhbFsOzztFIpghZulpUqLIJ7nCsS3S2dTE9UWX/3n0YY0JttWCrjj2TexiOFZ2dHXR29CEdvRwY3sePbn+YLY++yBmnreHyS85lzkAPWlmMq6PFEUOoVW8kdHzf4hm0GbLXeZw4u3JNFnTzKxqfAXkCDsilVw24LG7hidkGq7spjiBNOSYbYpp+9LIm0nH4nH0izv9wOpzz18/U6zx432Zefumlo/hXVmAmCtI8QVEux/z8dVezcvlCn/hRTd/ZN0CWSW1YnQ7yGeSCJcbRxpZHH+PxJ5+lq38RB4ZHqdfrh1irjaYe2Sbn65qtQauYNK2DeF1j4lKc8ZImpSIs1jcatlmcc/YTaH5ea82B/QcZHRnHOpvHJ0FQEvSoCMZYxsbGGR8bp729Ql/fHKIoYv/+Pdx59/08+cQTnHP26Ww89ywWL5iDU4bU+fnfCq9nVIKf2Nhs5WXloKLAZaMlmtFwn1sI85BzkyYDVVpem/2tlArC9MYx3AzpEs6htCKKIl8BREbGweq1Xu6FddSmp/nu975PvRjN+xOhIM0TFGetX8lnPv5eKuUoEKYE1yygJbHRSABlf2fwDXItiO+V6STGuTJ7h6vcedcWym09TE1XqSf1pteGmGOw/lo6pVsXmu8qkrSKUoqBgT42XXg+P/rhD4njEuPjU1hnEeWtUK286+0PoGassSGNysg+iqKc9F1IflhjsM7lJGyNJ2/nLOMTU0xNTtPe3kFv7yAiMHJwP3fc9RBPPb2Vc88+jQ1nnsbcoV7KcUzq6kRakRqDIkywzPSVkoUXQnOQrGJIZrnoNOKUzR9Ls2il+SPLClwbtnazFjOzOL11L6KRbA3hcRYucMEtxwlaRd4ST1O2PPww23cUJZM/KQrSPAHR293Bl3/p48yd00MUeRdMSSN2mWFmHLPZwsxG9EqerLA+Tuk0iY24864HeP31Ecpd/RwcPuAz7bZpLo1z+eOZ1mZubYkvlTz1tHU4Z1h/6hrWrl3Hj2+7gzR17Nq1h7hUwpgUpTRZiZ9jpvXmV6jDPll1k4jgbNZ8IstRZzFCX81jrX+dtY6pqSpj45N0tHcwNHch9XqVkbExbvrhvdx//2NcfOG5nH3mOobm9ZOmKToQUZ79nhF3PGSU7ix58mbkme7c3W76rPLY5EyCzQTtmQWbxVUFURrBzzgXFaqtspSeI/8BSZOE6alJfvCDm5iamjrs+gq8ORyVTvMv/uIvOOOMM+ju7qa7u5tNmzZx00035c9Xq1VuuOEGBgYG6Ozs5Nprr2XPDC3Yjh07uOaaa2hvb2doaIjf+Z3f8U0ECrwpRFrx6Y9czoXnriWOfXNZLzNqWJGH64l56N8hCZPpJUVI0ezZP8nd9z6KiruoJY7UeBfRE1tjRo+Ianovl9/8Jj+BvL29k4PDo7S3dzA4OMiLL77IKaesore3h8suvwylhCjSRJEOrqgNyZjmtRt0aKzZ3I1npmSmeT2eUCHSfi6PKI11glIxk1NVtr+2k5HRSQaHFtDdO5fxabjph/fw5//tr7j5h/ew78AU9TTGisaKxmQqgOwWztlrXxtWMbkFStO2pjXOXHvuas9031UuE8oqfly+XygncN6FV8qXT2bb8gYezoXm0Y4nn3icu++5p5AZvQ04KtJctGgRf/RHf8SWLVt4+OGHee9738tHP/pRnn7at8n/zd/8Tb773e/yd3/3d9x5553s3LmTT3ziE/nrjTFcc8011Ot17rvvPv7yL/+Sb37zm3zta197e8/qXYzli+fyuU82z/zxAnDvks02ofDwyLLlOD+ONzGaqRp87+Y7ODheR3SFAwdGwiTHUFbpfHci52h5r4bFKeAUJgWcYmqqytNPP8OPf3wHu3btZXR0Audgzpy5bN++Hecc559/Hpsu2JgTo0922BDT84J8F8hSqYb7nnf4Ieso1JqJBpX3wrS5RMgf39iUqVqVl7bvoG6gq3eQ/qElHBit8/2b7+K//4+/5bY7HmJkokaCwqoIS4RBYVzmAqf4GUOHzjAKyepDMeOj8eWQTUL47JzC6N3G64JlLQpRGhXqzEVroigKx/KJJJt1WMJhTUqtOsV3//mfGR8ff1P/LgocGeJ+wp+e/v5+/uN//I9cd911zJkzh29961tcd911ADz33HOsW7eOzZs3c8EFF3DTTTfxoQ99iJ07dzJ37lwAvvGNb/CVr3yFffv2USqV3tR7jo2N0dPT85Ms+4REpBVfu/GzfP5n30+lrNHaEUUqJz8f58qskVZrLfs7c9HDBsRC6gxWR9RciUeffpX//j//EUsvU1Mp1aTmyQGHEu3dVq0xxhxiuUomZM/igOBJRQyiLFocvb19RFGF8887n/sf2IzWmtWrV7Nnz14GB+Zw991356J7rYPLHgT7vi1kVg3TLARvFfo0e79ZHDIfqiaCYH0TZudb56WpReuIro5O+nq6MUmV0dFhsCnzF/Rw/nnncMYZ6+nv7QGMn6FuEkpRjAvjMpWKcLYpMz7js2vWZWbVO7bxZPgjH+Dko6OORvZcxMu7glUpEqF1RBSV0DrOY67ZKA/lAGdI6tM89sgWfvPG32VPMc/8DTE6Okp3d/cR93nLZZTGGP7mb/6GyclJNm3axJYtW0iShCuvvDLfZ+3atSxZsoTNmzcDsHnzZk4//fScMAGuvvpqxsbGcmt1NtRqNcbGxlpuJyM2nLGKqy87m3JZI2KDZdYUx8vEMpkqaAapHUKggBOHqGBlVhW33Ho3k9MGlKJan/a6zJDwSJIUENK0ofHLLMyso5J1JgjrbRN5+fft7OpieHiYfXv38sijj9LZ2cnZZ5/NyMgIbW0VkqSGjhRz5gywYcPZ9PR0eYvO2tBwhKb1+zNuEGZziCLcgj6U0BAka/nuhNCD08c8fQjQcnB0lFd37mI6dfQNzKOrd4gdrx3kH75zK3/1N//MAw8/TT1RGBuhpIxJs/k/KkieGu89a2izqTwyn0g5m1cgDdlY66GayzV9Bt/LjaAhhQrXB0uaJkxOTHDTTbew/8CBWRZU4K3gqBNBTz75JJs2baJardLZ2cm3v/1t1q9fz2OPPUapVKK3t7dl/7lz57J7924Adu/e3UKY2fPZc4fD17/+df7wD//waJf6rkJfTye/+aufYtmSIZRyRFqArOpn5he10Zyjdc7PzDigw5GClFCqnUcfeYqtW1+np3eI/cNjiBJ0sBqd8zHN7HU+FhkxPT1NHMdUKhXiOCZNU6aq0ySJwdnUa3awrF27htNPO41tL2/joYce4bVXX8M5y/79B1i3bi3VapUHHnyAjo4Ozt94PvV6ws6dr4cMuAtyJ0WkSzPOYWa2uikxNavbbEPjDe3PjZBxdxYVKZw49g8fYERrert6WbBoFaOjI2x9cTevvPJ9HnroUd5z0fmsWrmU7s42cLaR3W9mysOESNyszwWynxFaycsms0RQ/rwXsmulZwnFNMnInGHHju386Me3F2L2txFHTZpr1qzhscceY3R0lL//+7/n+uuv584773wn1pbjq1/9KjfeeGP+eGxsjMWLF7+j73k8QWvFxz5wCRecuw6tfS02ksUvXXBZCeHJTAaUEaTL3dSWbC0NjjUW9u47yO23b0brDqo147V8SnmdXyi1FCV59v3UU0/lhRdeyN9namoKY4wnoMzxFE9abW1lVq9eTRz5IV+nn34ay5Yu5/nnn+fFl15i8+b7AycIS5cspa+vH5MayuUKOOjp7WH9+nXs33+ArVtfxPf8VI2Me6jEyTs3QWhLFyxpyA3RrKgxd90zyZVzoHyZoYoiUmPZPzZGW+8giZSYt3Apw3t38cLW7by2YycrVy7lvZddzIpli4k0aJVV35Nn1HMBUguxzcicN8VfWxDUEHlgQQKphtdISAAppfJEVOOH0Xd0SpKELVseYc+ewi1/O3HUpFkqlTjllFMA2LBhAw899BD/5b/8Fz71qU9Rr9cZGRlpsTb37NnDvHnzAJg3bx4PPvhgy/Gy7Hq2z2wol8uUy+WjXeq7BmtWLOKXPvN+Oto0kc6SP8HVy3WTzXZOxpK+Itwq8XXbKN/4F+d7T2IxTiGqwt333cW27fvo6F3A3oPD/vU2RcRXyVjE15xjGRjsp5ZWOWXNSl7Zto2DwyMoHQUSymRINrf6tI5I6im7J/ZTS+oMDg1w860/4LJLLmVsfITOzi5ef30X09NVtm3bgdZlFi5cxPDwGFqXuHDT+Tz99OOUShUGB3oQFTMxMUV1OkHENzXOKne8jtv6KYzO5aTjMmIMxGkkizNafEM5UNZn2J1V1Kyj1NXFojNOZ9/rr/LK048yt6ebgb4+xieneeTpV3lu2z9w2qkrufKSjSxdNOgjAYG0JSfp7BNp1NSLsz7q3GIkGjRRS7DFBtr11V3KhxucCqGQrF9qaPrn/I9fJra3zrJv7z6+/4Obivk/bzN+4tZw1lpqtRobNmwgjmNuu+22/Lnnn3+eHTt2sGnTJgA2bdrEk08+yd6mgPStt95Kd3c369ev/0mX8q5EHGmu+9AlnLJ0HpFuVQU2C81zyynE7CB7nPXSzLb4NmxZPTiqwvbX93Pv/Y9QautkqlqnkSfKurr7xxkJWmvp6uyiva2dBQsWcOZZZ7JgwQK09rE9H/P0VTneCp3mttvu4NFHHmfv3gMY4xgYmEMcxwwNzWHNmlVceOFG+vp6QCxPPfUEt9/+Y6rVKcBhjEPrEkNDc9E64rLLLmXDhrNpay/hSHGuDmJANc4ttYkX0Get2gJp5cSVnR/OE5LLBpz5Rr2Vzk7WnX46+8fGqTpLd/8g+/cfZM++A8SlCgsXLyWxsOWRx7jr7rtJU5ML2VsNS2kJvWZZcsniljm76sZO2ecbFjrTAxcleVPpbN22aV/nvJW5+f77efHFomTy7cZRWZpf/epX+cAHPsCSJUsYHx/nW9/6FnfccQe33HILPT09fP7zn+fGG2+kv7+f7u5uvvzlL7Np0yYuuOACAK666irWr1/P5z73Of74j/+Y3bt38wd/8AfccMMNJ7UleSScc/pqPvmhy3IR+2yxslnbv7km/Z/L+lI6cL5euZ4arCim6sItP7qX8amEuNLL8P5xn8zJbaQGoWQWy/DwMM8+m9DZ2cnAwADWWoaGhgBhfHyCifEp0jRBRz7GaYwwPVVjeroOOLq7elmzeh3PPPMM5513LjfddBOnnnoap5yygra2du6//34qlZixsWmstWx94SVWrFjDvn37OPusDUxOTPHMM8+wcuUy4jjm6aefxlhLtZbgRz6E0bUOjDUoyWrBVbgWzUwkuT3ngmCc0Byks7MT3dvPeKQZ27OLVGts3TC5ey9Lly+jFGkSA6edeipxHJPFmPOEjXCI/rLlsxJp8tKbE1qNbb7rU9BgBqJVolFZEXoIjwgq8LPDGcPk+Bg/+P4PmJwsxOxvN46KNPfu3cvP//zPs2vXLnp6ejjjjDO45ZZbeN/73gfAf/7P/xmlFNdeey21Wo2rr76aP//zP89fr7Xme9/7Hl/84hfZtGkTHR0dXH/99fzbf/tv396zepdgaKCHL/78Rxia001c0uCSllLJvAkHuecZHpBLf3KE+TveuhKQCCRm2459PLRlKyruYqqWem1f1pmoeaaPgAk9LZ2DkZFRRkfHOHBgmJ6eHowxrF27jtHRMeYMDvH4448zMTHhezp6dTnZCNutL2zjuedeoLurwvj4JJe/9wrqtToPPPAgH//4x9CR5uGHt+CwtHe0s23bK+zevRdRwvwF83nttVdJkjrr169jbGyEKD6NfXv3UGnvYtsrrzMx4YkiimKMyfPUNEw+39XJk5Yi62rv47GOxKS0xzG1ep3JfQeoTU9S6ehhamSMNJmmVCmRJHXq1SlWLl3MymVLcw5utiQbmN2hczMIciZhZtZo5usLXqOpsybDWfy2OU7qHDatc9+99/LIo4+9uX9oBY4KP7FO81jgZNBpCvCFn7uGr93487S1axBDHOtG8UkTZsqJCPo+kzvmvp26WItSEbUUnC4zNmX4k//r73n6uZ309g+xa9f+JtIM2kunEKcxeeVLVsboV5mXKmpNHMesWrWacrnMwYPDlMtlnnnmmTx51N3di1KaifFxolhTLvvse1dXJxMTkyxevJjly5dz7733kiQJXV1drFq1ip07dzE6Oka1WkUpobe3m46ODnSkOf/883j99Vd5+OGHuey9VzI+Mc2TTzzNwYMH8SEC3xxEmiRK2f8drqXXpxLBAClCpbeHyz/wQQ7WLdu3b6c7Eva8so3p4WH6ersRl1CbHOHaj1zNZRefh1K+I30LYWZVOs2fa5NFaZvkUiJhknvIlDdIU2GdgFIoiVBRTKTjEEP2+xgXWtVZizWGkeEDfOUrX+XOu+76if8dnmx4R3WaBd5ZLF04xKc+dhnlikIrh1ZB492EQ8gy24YLNcgN7SBBFJ5aixVN3WgefvwFXnhxJz1989i/f7QxJVLC/mEcRKMtW5MwHm+JekG3z6pXqzWeffYZnnjiMUqlmPHxMZyzbNp0AUuWLGbVqhXUqpOUyhprDZOTVZxTjI9NkaaOffuGueee+6hW6zgnDA3NZcGCRVx55Xvp6e1kzZqVpGmNAwcO8Oqrr/LKtld4+qlnefqp5+lo78EaePjhh1m1eiWLlyzGGNOkZW2QWcvvS5MUy9isqgomJ8d56IH7Gd6zhzPXn4Y1lt7ePlQU0dXZwcToQZYvWcg5Z56OuEZH+gzZ3PPZMTObPrOBcVir9YTopJGR953ZG69v0eA63xLuqSefPKLuucBPhoI0j0OU4ohPXPMe1pyyGDCo0KZstrrrmTXlLdtChkCc/8pZJ1gnGBcxPFrj5lvvIXUx1VpKPUnRSpH74pmL7/wANCfZQDKFby7s3e0s5onzjXuNMVSr1TwJODg4SBzHdHd3MTjYT0dnG11dnUG07sstndMIMZMTVdIEtIqxBnq6+xkdGWNqeoq1a1cxOj7CWWefybr160jTFGfhsUefYPjACF2dvTzzzLOICN3d3ezbt5dyuRTi6Otyvae1BpUTaX41AW9pKue7xUfOsfOVbVCtUh09SEelDYCBgX6mpyfpaCtx/oYz6eooo1RWh5OhQZgNd101EWmrK37oY3xYpHmVwQJtXXH2Gfm/jbXU63Vuu+02hg8ePOK/sQJvHQVpHoc494xVXP8zV1GKII78F1xmmpkciTiDS57pJV1jGiQSY6XMHfc8wkvb99PdM8Do6CiCxZgUnJcY5V9kgawbvLfXsoYUjdGzOBUGpxFat8UkiQEnHDgwzObNmxkePsjIyCiLFi1mzpw5nHXWWQwO9qOUoqenh4GBfpIkyRsKK+Xjmvfccy/79h/gmWefY8WyFaxetZZKuY329g7OOutsli1dhjWO7a9sZ/u2V+ju6kYQIq057bRTiWPNxMQYOvLNPJRS/jxzS7zF7EQrhXJ+JHEs8Pzjj3DnLTdTwmGqNcpRxOTIQRYtmMv6tauIInC5W54R4+FwaNyS0I2phVyzUcfiLdasKYeOdMs4kFx/G1QNzhhe276dH93240Jm9A6iIM3jDJ0dFT7/mWuYN6eXKFK5eFkyAeAMHD4i7Stx/BdLguUiGIRXXt3NPfc/QkfPEJNTU6RpDXAoFQZROIKeU1okTP79MplT1iQjI4ughURIE4tWMSIROEWaWnbt2sPTTz/Ljh2vYoxl//799A/0EsfCmrWnUKtPo3Wmm8yOBcak3HvfZqam6uzYsZN9+w6QppahobmsWLkC5xzz5g3R2dlOFEXs27ufJ554is7ObhYtWsxzzz3Lrl07qddr3mIOq7ehwUkupyIMubBhLlGaoI0hMgnUpnni4S3Up6c4OLyfKBLO23AW/b1dWJMgZEPaQrT0EIXD4Vz1BgFmEGaxOmmqtxf/o+U/d5cVhmKdpV6v8f0f3MTwwZHD/aMo8Dag6Kd5HEErxcevvphLLzyDUilGJA0lxj5mmCumIdxnllK2LYvdBZfaBkGNI2+eO1Wz/ON3bmLP/nHaexYyPrI/F6N7ssxkLA2doMGhRPIklAtzdLIvs9ah1DINzX8lykdeiIT1OcvkZJXJyWnGRsfp7evl1VdfZf78+UxNTbBq1Qp27tzF66+/jkkNcRzT3t7O1NQUtWpCtZoyMjLB5GSV7u5u2traGR8fp1SO2XDuWTz22GO0dXYwPDLG7t27UUrx4IMPMDg4GMo/TS5mtw60ZHFff9myqT8+LihE4lvBibNESuPSGlo6mZieYsn8AdavXYVSLm/LZ53jEK48DGabB3QIXOZdNJJKSjUaejRe7/+fJCkvvvgi37/ppqJk8h1GQZrHEZYsmMOXf/ET9HS1+TiZiE/p2MbsnKw8EFx+nyF7bJzD9xwXlLVE2lG3Canu4NGnXuaJZ1+n3NbL+OSonzeOd62lyR234g/oM8sNwmz05nGh7Ro4UUHyNLMpr22yhH2PySzbfuDAKCJCdbrG6OgYQ0Nz6O/vYcGCuYyOjjI6Ok53dzc7d+5islrPpU979g1zcHSclaesYO+BYeK2MlG5xBlnnUFcbueWW26jWq2RJIa9ew+wZ89+H1IIcVoJlrvNV9VYr4FGb9JMyO/T0gx291OfnqKkHBdfdC49vW0giQ9HaH2Iq988Zyh7p1z81KQVza34/D6rBwr9SlEgGlE6P6YluPPOofEJLJsk/OAHN7Fjx6tv8l9bgbeKgjSPE0Ra8ZmPXcnyJfOItM/giDS+iK7Z9OPQrLmHa8qeQyQarSxJWsWK4sBojR/efj+GClqVmJ4cQ6Hz+msngp9N3nr83PN0GTX4NWRGjzEmWGyZlWoPcVEb5ZX+YJkG8cCBEcbHJ3DOV/P09fVTrdaYP7+T6enp7LT8OsIx60nKs88+h7WGlacs5/s33cyKFcvp7OxmfHwCrSNAYYwljkr53CCEpvPy9K61F8Nn1llz0wwhDC9D0dnRyYF9O1mzegVnnr4WrRvJHy9DbbX2w4JnfD5Zcqhp3EUuM2p+vvlZ8TXmIV5qfUYrk0T4z9wYDuzfz+23336YfxcF3k4UpHmc4KxTT+Ej79vk5/0oQl0xjS967hrP/qVoJTkhQkiTutcKSoWUMnffex/Pb91BV98iRkamUS4mTxpJq6XUOG44ZvgiN7/LzE5K2fbZ0Nx9aOaIjHo9Zdu27USRJk0NaZqybNlyRDSrVq0mMZYXtr7IxMQ4pVIZPxPHaxtffvkVlBL27NnP88+9jIj2HdtVTGIT0iSdlcBn+3u2aypOGBgc4MD+vWgF5204k+7udpypQiTBiqWJK2eLSTauzOzv1nhNq84za8wRZptL+LHK93b5OIt77r6bXTt3HfZcCrx9KEjzOEBne5kv/Nw1rFg2Hx355kLel8z2mBnHbGBWAnAO7Xx3pNRYrG5n9/4at935CLrUSZoapqenERfjQtzUuRnEm2VkQ+ll7mXOGHzWeE02aqI50pbPRGw5XksUNg8qKtLE8vrru9FaMT1Vp1JpY3DOIHE5YmhogDVrVvHCCy9SrVZpb+/EWke9XiOp19mzex/WgOA7mdeTOllji5nXqJlEs3lD+eTHZhK1UCpXiHXEtKmzaOEg69atQEjQUfgsQga82fIOJ9zyI9F8XVs+v/z/MxNCfja8UlkyMFzlph8x58AkKcMH9nPzzTdTrdUo8M6jyJ4fY4gIH7j8PK6+7Gx05Duc+xrzJsvPzW69zGYleevIoZwFl2JEmKzD92+9jz0HponLHew7sA9n0nw+ULaOlqFeodsPWRbdSk6YrevPxk3kG7BK8pa/KY4UR+IsVgSnFTYklxJrMIAJbqcRjYorJFY4MDrOngPDPPP8C+zdv4+OzjYmpyZYsWIZURSxZMkScJDUDZEu5+tUKnO1fUwwm4feer1CfNg1/928H/lzXV1d1GtVlFje857z6eyIETE4TE6yRzNipPXaNRp2NKqvhGwQka8nb4zy8O8VRptYi039lMnN993Lo489dtTvX+CtoSDNY4zli+byS59+Px0dMVEEIlnH81arb7a/jwQXmm44HbPt9b08sOUpyu09TE1P41wgZ9Ug5qwjkXN+rIRWrU5IK6E2XOuZZJ5ba0KLteXPqMk69eJErwww1pcROiGpp4hTaBXhLKSpZc+ePRhjmD9/PsPDw3R1dTE0Zy7Lli0njmK00pjUokQRx35kShRFh4QN/Nv62GYQZOXkbsm4KsQYlSBa6OpoZ2pyjMWL5rJm1XKUGLTOlt/InLeM0zjMZ+Sart3Mz7S5Wikjy/z2/2/vv6Mluc7zXvi3966qjiefOedMjgAGaQAiD6NEggQpiAqk7GualmhZ1/5Eg1qSaGtZsuWrtJaoJa8lXQdKy1eWqfs58TMlZjAgEWBCJgACM8DMYHI4aU7uWFV77++PXVXdfWYGwCANBqhnVs85p7u6u6q6++l3v+/zPi8ur5ptm55NY1heWuTLX/4qrVYeZb5eyEnzAkIKwc9/6J1ct2sHSnUsxSyG3u6R5PrulrlzoHtej8anHSu+/PW7mV9qopRPqxmChXKlSLkSZBGTlF05NSl7JoSmS2qTVOaTiUQgZUY0qXwnXeafbT+FcG7vW7ZsYXh4OFvOS6UccSb/umocbnlvJcVCmdOzp5mcnKZS6WPv3r2MrRlj1zW72LhpI57vKthx7HKYURT17H/nd1e9T4eQIUh0jjYhP1y0bC1rxsao1ZZQIubmm65loL+IksZVfpJ5Re71OkemsodAUwNhzsgfZ25GopO7FKIzZbI7GnatoQqrXZ/5vffdx9PP5C2TryfynOYFxLZNE/zC7e8m8BVSduXauirM7s8zCyir0dMVBMTGolWBp589xv6DJylV+qjXG+jYoJTPpk1baLdCSo0Sy8vLxHFMFMUYY5KKrfs+dYPUPGKjE9WOpVOIEVm1vteY4tykHkURp06dor+/j02bNjgruVrDtTcmc7o7j+IeJwxjnn76GdJZPJOTkxSCIsePH2Ni3QQWzabNm8BaTp2aZGJ8HbOzs7Tb7cRfU3aiTF6oINPJeUopCYKAuYVZJiaG2bVrJ74H1mhUuqRGJIqD1TnJM3OUyQvYJdk6M4fZ0eGm1nIya2pIz7BMq+dYlpYWeeCB79FIVQY5XhfkpHmBUC0X+eef+Bm2b1mH8gRuFGzXB3vVEu/FKsA9kZ0QWOWzUrN8664fUm9CqeLRbKzgCY/R4VEwkqHRIeIZzZo126nXG8zPz9NoNIhCTRRFeJ7npix2SZ2cG8+ZzynSfKc4NyFJKVFK0W63mZ09jVKCiYkJBgcHWV5aYXnJjZjNqN+CFU7rKaVPHGt8FaAjQz2qcfR4C+UrFhfnWb9+HY1GnW3bt+D7AWHc5vTp04wOjTEzPdt1DtPLaoMNkZGl1prBwUHarSZCam684Rqq1QJCanwU0ibD0YRbHeiu+XZZbvScWZSzULZNvyDS1UYXYWb36ipexTEm1hw5eIgHH3o4lxm9zsiX5xcI73/X2/jwB3ZTLPpAd34wbZPrLMnOuGQal+7r00d2H74Ynx8+/DR79x6lXBqi0Wi5JbuGRi3k1KkZwihCSEmhUGBuznUG7dq1i5HREcrlMsaYxNbNZB/ms9nQuSRgSnWdCnn3xVgnj2mHoeuekRJtDSdOneDkqZPUmw1iozttgVhMIo4XSHRkkSi0TkX/rs/9uef2Mjd3moWFeYZHhjAmZmVliUsu2c4VV1yeEbVDZzbQuVgt3X54eJiV2gob1o+z6+rLE5eprop4umTOTsILQ6Q8epanTXOdKamn/ead/GXXtsm5bTabfPXOO6k3cpPh1xs5aV4AjA5W+Wcf/zBDAxVAI5Wgk7pyRZRUCG5t0qHcZdNmrXGuQ7hKuRLKORkJQGi0hMmFiG/f/zDSd+48YaudLftr9TortRUOHjhEqVji1KlT1Ot1yuUShw4dpF5fYfPmDWzfsRWlLNbGeMoZWQhrkovbuexfyhtZTlNjdAzpTHY69mXpEtNlMBVxbGg324kZRTqGOM1wppMXE4oSzo1dJY7mUnpobTl69AQPPvgos6cX8fwiJ09NUSyW6OvvZ2hkCKToESFYC9YkecykYG2MwWjDwMAAy4uLmDjk2qt2sn5sCF9ahLEIqTBCoFMpJQKVRJ6di7vFLd9xkb/oPk/dO+IeQ6Tem0l/uUEk7Z9uTK9JTKVN4pl56PkD/OCHD76Wb9Mc50C+PH+d4SnJz9z2Di6/dCOeL5HKJtXs3g9St+YxlWy6tr6UuLSTqKAw2hFRbDXaGmKh+Pb9jzMzX6fSN8DM7HwyiUFihcEkZNtoNDlw4ABBEGRV57m5OXw/wFjN8vISE2vHMcawuLhMHMV4Wa5TuyhQJbNqDFkULISgUi7RaDTI+trppBSUlG6OUPLlIBKvT9uVzxQ9/7tKcXpG0hwlxsl0TKJZlEIxO7NAreaMjZvNFiMjI2zfsR2LYHl5hXYrdAUWa53oUXQMSZRUSKmolsucPj3DyFAf17/tSgJF50vB7U2XwqEjxcpi0G5ZrehEoxZI7fTSsDMz6BDJ4DThCmxOrJuwuUjX/24V0qwv8bVvfIOTuZj9giCPNF9nrJ8Y5R//vduoVouZQe4ZEhWbOhS5n+nkSGstMvHGFImG0prkAysMFoGQFU6eXOL733sYKT1arbYjKJsuA4GuyDCONfV6kzAMmZ6ewRjLmjVjLC+v4HleYvMWMbZmjMHBAVLD4fTz3UkZGLSNMVYzPDzM5s2bWbt27Soxuds2jmOU7M2Npr/3oqOv7LnFdrbtEaobjVSKMAyZm5vjxImTTE5OUSiUGBkZSUYMC4yJoasgIxLDZaygXCyiowhMyO5brmdkaBBjnJnxWaVE9mxJ3F7VQ/Y8q6rpyQEAIplMmWwjk8gzi+RNsrlF65jjx4/zta/nxhwXCjlpvo4IPMWv/IMPctmOjfi+h+svTz9znZfCCgOiM14CLFYY0n5tadPlpVuiAkSmjZUe9YbH177+Q2r1mKBUZXmlCdaJxzOJTPphtBasG6OgY4uOLZ4qMDe3wOnZeTyvwMpyHYGT86wZG2V4ZID+gYozRlYyEVsnY4WFpVDwGR8fZ//+/QwNDdHX10elUqGvrw9I5DVKJsTr9ifNnZ4LNiEVN6a3M3vdWNtl9pZ29bjZ5al3pjFuXMX+/fuJoshFiRIsLr1hjU3ynBKlPAYGBliYn2V0ZIBdV1+O7zvbu3T/zq7BlD2C+E6E3LVtmgPovk6kTlFkj50OdoOuL7l0pWE0rWaDv/vil3KT4QuInDRfR9z0tp389PtvwQ9E4mIECJvlLHtKC8mnJf1HFiE693GjE69MAVYajLTEQrL3wEmeevoo/f0jNFsRsbbYJC8m6JIsZRFeZzY5COI4pp1EpwvzC5kN2/T0NL7v43k+xhgu23kp5XIhizjTaKxcLuP7PmvXrsP3fYaHhxkeHmZgYIC1a9eR9tG/9A6ahIBst4bR6ShNEjF3R7NpW6T7KZmfX+Do0WOZfjMl3NTSzhGtG7BWqfTTajWxNuKaXZexbt0wIonwU1LuJvveaPLM6NJ9sbluql6dfcfdSCS5WTc0TZxh/NGRSVmsiTn0/PN8/wc/zCvmFxA5ab5OGOov88lf+jCb1q/B9yVZlGnTXBUZMWYfyeR2QUISxuXgTBppJpFWjEULyXIj5M5v30+tDbGBlZW6iy6FdCTr2Cp99OQ53Ic9jp1GUybLZikVjUaTo0ePcvz4CaIoYnFxgZWVZbZs2UK5XEZIwYYN6xkY6MfznCP68vIShw4dotVqsn//fqrVKjMzMywtLTEyMszo6Cj9A/1ZpOt6q7sNPM5sS+x0KiVLcikQqYfnqqV96lokpcoE7mHo8pjp87njc9t4np/1nZdLJdrNBqMj/bxj9w0o4ux1ApMRciZd6okcVy+/033v5EOzTqlk+2wdkeyvSMTsndbK7FGwRqOjiMcfe4wjR4++1LddjtcAOWm+DhDAh9//dt6z+xo8X3QKP8J2fUggKyX0RE1OJmSNEz9H2oAMEF4B4SkMxg22kEUee+JZnj96isHRNazUG2htEpJIIrEkKhSJvjBd2rvtvC6Zk8BLZpangnJrLFNT0wghmZlxusdCUGBgYIBqtcrmzVsYHx9nYGCAdrvF/Pw87Xab5eVlxsfHGRsbY2FhkbGxMdqtNkODQwwPD+N5HnHcIaOztWumRJiSzhmZz6wFMSHQZEKcFC4yVEolPegOLq9pk5nsMdYa+vsrWN0mjppcfdXlDA/2odBZ5J8+j7V2VSrhhQaonWM/u4tAgsyOTna9F0RPlOm0mQvzc9z5zW9lY4lzXBjkpPk6YP3EKP/g595LseS5fm9JJi1K81cZkujJkZ1CG4uQHgiPSEtQZSIdoG1AqMFKgSFgcqrBXXc/jJEBrbhNs91EKDfWwclqUgVkp7BiEUlxBKzV2biJYjGgXC6SKCyx1lXKdWxZXqoxOzPH9NQslUofp0/Ps2XLFubn59Fas2PHDkZH1+D7LoKbnJxkenqakydPsrS0mBUvxsbGaLfbbNu2jf7+fgqFQu9pWLX8NEn13JDOIkoywVI60+VVxSKbkL+rbHdaFB0ECok1TsLleYqB/iqLi6cZGKjyjt03UwgEKuvNX31ZjU7E6SJLmVzOXjhyqodOpC+EcBMmO8nNxGmfLNVgreHe++7juf37z/L8OV5P5JKj1xjOXPi9XHvVdjceoWukbPphxrrZPNriJDDWJmMjnEbRaIExguVag4XlOYp+H61mg+GRMpW+AkYWeOCBH3JyskV5YITTCwtIJbDGEpuI3sJEJxWQVmrTnmYA3/cZGRlhdnaWIAiIogjf94kSX0r3IbacOjWFwRAUC5w+PU+r1Wbjxo3s3/889XqDvr4Kw8PDTE1N0Wq10Frjea7T6NLLLmNudg7P8+jv72fTJsXhQ4d78oU9RSLIxtZa20U0ItFCpstfkSl5kqpRMsMouyKV/bjJnC5toenrK9EKmyANu3ZdwcTYCNK23ZnKpGApWb5QVNl1fllNr6m0qHONTZfjsivK7uo1N6TaUMnxI8f56tfupN0OX+D5c7weyEnzNcb2Tev4+Ec+QKHgozwnI5JdkyVtEnhmFdL0BgFGuxykEIpTJ0/x+NPPUB4Y5dC+H7F+7Ribt46y47JNHD8xy4M/fAYpB2i0NO24hbQKjE3yhST5yrRvOXl+KzBWJ4UghTGGcrlMFEUMDAxQLBaZmZkBSEhTdcl2DNq6WecHDhxAKZX0kdcQQjI+PkG9XmNiYoLZ2Vmstays1Dl8+DBCSIqFIiOjw9TrdU7PnXb+nmkvu+24vJtkpobISLMTYTqppulEkN05w56T2UVf1kWhEg/jDDip9JVZXlqkr6/Mdddfi1JumqWw+izpgDMlYk47mv2X7afbBdGVEnFSou7qeCp56t0/m9zHDeNotZo8+vCjuTHHGwQ5ab6G6K+W+NQ/+XnWTQwlbXhA8kFApI4+tqsN2n2MTFr8SYpC8wvz7Hl2LzfffAuaImFNUC4XKVQGaFHkOw89yVwrplIdYWp2Eqk8pCFZ4rlJhVYqdPJhV10sIC1IpRwpSkGtViMMQ9d7HYYMDA9hYo1UHmEYZwRhrCEd45uO3Z2dPY3WmqGhAYSQ+L5PoVBkzZo1NJtNBgeHOH78BEJY2u0mp05Nsryy4s4DnbRE2vnkTpPCClcEc7rvLn2n+8U10dApjjmkEWvyVyIy72QJnZtTpVJy7u4m4oa3XcPWjeN4QmNMjJDd0yJTFharlt2pJqiTl7a9mzuSTGUGohNFZwWljEi7VwTuZxSFhM0mf/elL9EO8yjzjYA8p/ka4j23XOXMhZVx1m+ZtEgmY89IPvSuYJG28llribRFowi15cTkJKPja5gYH2e4v4+hwSLSh6G1Gzm5ZCmv38F1H/hJTL/C+hJjlVvSa4E2Ais8kIqsYitENl3SQ4A2SFeqx2hNq9Vi5vQsc4sL1JoNVCEAJRkcGcIvBkkelawbyPN8wBVXpFC0m23279/PoUNHaLdDpqdnmZubY2hokLVrx6lWy1mudXFxmYXFJZeTFMK1Cwo3TMxYhbYk8iqbCb4zzSSuH9yNHXYze1LTDTdfKW0Q6DjQp9IrcOmGat8gK0vLlAs+N157BVXf4Is4KdBIN9SODsHZdIa8e3TXVJBMttQYNDppNCDx5hSZRDPVwbvHS2RgUiGkhxAeaQumSF4bV4AzPPLIIxx4/uDr++bNcU7kkeZrhIFqmX/893+KkaG+ZDmeflx6DS+yNkPREVBbK5GqQL0WIoRP2JZ4foVGM+LgoSPMLizQP76WtixycGaKqDzAxqs20jexmePPH2L/nr0szkyjDEhrHSlqixKpm3k6RM0SJvvjeqNJ8oTOEo44JoxConaI7wcEQZAVbaampojjmJGREebn5/F9n1arhZSSRquRHdvKygpDQ0Mo5X4fHBzk9OnTjIwME2tLu92m2WwlaQTjcqyp4Ny6SndasEqX72kqoduezp23znk1xiQkKXp0nOkK3gLVviqeJ7EYrrzyCjZv3ogxUTaaNzUj7n7sbCxGz6t9Zr4zjWy7/8627tqfs6kGku8vrDGsrKzwrW/fRa1ef6lvvRyvMXLSfA2gpOSnb72F63ddkomoIV3B9STbHAQu1ylAJ1Xqo8eOcujoJJ5XYmR0jHqzwZ5n93Pk6FEqgyMsR5Yjc8tMN2JE3zDzzQhb7WPDlVcytnkLk8eO8OyTT9JaWEYZjS/B6hgp0gWlyTSeiE6BRFhHCmkkamNLM2qgi4Zmc4rh4WEWFxcxxtDf358cVy+ZQYdkpqen6etLvzgEo6PD+L5PX18fzVbIwMAAURRx4sTJ7AvEWOuW4bbTy75+/XqWlpZot9s95NjJf3aI02k6U/LpJkzXvSSTSnW1v0qttkx/X4lbbr4JJcHzFFaHSBSpSqnrK66HMN3juo6hnsp91sbTyWe6e7twM0srJF9iHdKUkMyft9ai45BjRw7nYvY3GPLl+WuAS7as5Vd/8WfoqxbxPPehSvvMU/OKFNa6qE4mbBbHmqNHj+EFRa684mpAUm+0GV0zwfMHD7Hz8su5+R3vgqDMkel54kKVhvBZsYI60A4CZH8fW666kvf97M8wumUThcF+QiHA84kBpOy0JWKy3KkQqSVE4qxE4jkkJK1Wi3a7zdzcHGEYYq0lCAKEEIyMjFCtVrNoMC06pS7jKysrrKzUqNfrRFHM+vXOfDi9n5MnCYrFAp7nJWSnXaEm6ciZnJyk1Wr1OMqfCZFoT9Mlujwjgksj/UKpCNbQbtbZeck2Nm2cQEnX2SSVSuYsrc6Riiwa7y76pJrLbr/OMzgu0cBmVX/hcqrOnzRpp8zeE25ZHjYbfP7zn2dufv4lvOtyvF7II81XGcXA4//8h7dz2Y5NSGlJBeVOciPParIghXQRVjKbZ2JigqA8iNaK4ZEBGu0Wa8YH2Lp1A2vHJ5icnmWp3qZWKBKXPOqNNgjl5u1YEJ6HFZbjp08zvmMbG2+5hblTUzy/Zy+Lp09j4wiMRAqLEqDSinqarEtkPY6yDEJKMC7KarfbKOWq6AsLCwwNDeH7PrVajSAI6OvrY2VlJdvOfSHITNY0OTkFFjzfI0juPz09jRCuu+j48ZPEcUypVMqkSt3L1+7ZP6ujzKzbNMsNnvn6CCFQnsfomlGWF+coFSXvedfNVIoeQsZZtJskUkhdhlYvt7slRDbZ1mYVoHT7Domm/xxxKqxQ2eNmj5ece5vklvc+s4cfPvjQy30r5niNkJPmq4y333Alt/3E9XheZ3Z5GuF06xA77j4y0blLl3OUgkrFQ2PAk1SqHpHRFIKYLdvWc/zUSVZMQBgZntq3F1MeYO3WrRQrFUciyQe3Xq+z2GiwY8cOin1V1q4ZZuySrTy/51kOPrOX5uISvrbOIzMhxDRaEtIj1to57wiBNgYl3Wiv1FxDCNd6OTc3R7FYpN1uMz4+TrPZZGhoiKWlJWc63LWclkoRhRGe8mk2Wpxonsgi2IGBAay1hGHImjVrmJiYYGZmhqmpqez+HYelVZKf5FxKoc64Pu0XT4nbjbHwsVbTata44W1XsGXDBAoNVnciScderv0SsvlBWUEoocC06TUla2xn3C7JdbJrQZeJ7Lu+CM7oaLKWxsoKX//Gncycnn1Z78Mcrx1y0nwVMdBX4R995H2MrxlAeTap4HYt1YTJqrid5ZvTISIEGuOMexOTXmsMzcYS6yaG8WTEwFA/sjLO3qOnOXTyIDt2XMZCqGnX61RLRUAkAnlBvd5CeQGFSpUVrUFJ/L4KE5fsYHh8nNbiEvuffJrG3DxWazcsLI3Ukkp2R4ud5AgRjpjSaMgYhIVGvUmxWMSTPqWioBW22LBxI/V6nfm5eSwQhhHGAFJlw9mwgtnZOcBSKJTQ2lKpVFi3bh3tdpuFhQWESJftPvV6HZVYv6URZ3cu1RWTzj1HyfXtx4yPb2BxYZ7+apF3vf0GCr51A9NwBbFk4+zVcX92hEDZWelaoYtkhPDq8PaMYDddmmfLcplEmGkvvmuZPHz0KHffd39P+2eONwZy0nwV8a6bruYn3vE2PF9iTJyMWXDEmRYlIFXydaq51jn4YhNzCJ0UU44dPUKgBBNrhhECQqmYaYScWm7iD4zSNzrGzPETVMtVVEJEAtfCVyiWKZTKmHS5KCWx1pyuN9i0bj2lDZtYu2kLJw4eZN/TT7Mwe5qi7xO3QjzpluoCV0HuVit2r1HTXmkda3SsOXHihBObK0GhUKDVajEwOAi46ZBzc3OkPfAprykl0dpw6tQpqtVqZiF3/PgJWq02o6MjDA0N0Wq1qVQqTE5O4nneGdZyqRORQ2+xzUWoMQhBf78rSrXada665jIu2bqRQBoXZcpEImSdjKmjvEwzjl1kmd6a2e2lUWP2rN1nLdsPt/RIR1l0VfSTgpE1hiiK+O73vsfc/MKLv+lyvO7ISfNVwrrxET75Sz9LqeBhTEwQeGSLtmSZa01vFJNBWFJ/R3AfnPmFJcZG1lCuVFAorNHUI8P+qQXqqsjA+k0cOjlNqdpPpVLCjcAQWOkRW4FfKFCtVGksuVxju91mubZCf98gIijQssDgIJuuuYaJ7ds4fuggB5/dx8rsHHE7cmZlNskOptrtnsKI+z21XGu33dxtIR2JTk5OYoxhYmICpXyEEPT19WEt1Ov1bKmcFkeUUiwvr1CvN1hedgPWlFJUq33MzMxSq9WoVCps3rwZay0LCwusrKzg+35GoE4Any6Fe2VIUrnl+fDwMNPT0ygJN990PdVSgNV1lCewiUUHyZeFSMriIj34blLOePnM58sKUFleM3uhXaEq3ceum6SURFGE0TEnTp7knvu+k1fM36DISfNVQOB7fOKjt3H1zq0EgUJ5zkC4E0m4pXpWtEjrFsmHwlhnmuF6ot0c8JHhIZRQTrNoFKGVnFxocboNstKPaMHIWBGtI+JYI4XFKpX4TIKvPEaGRlhZXmGlvoRSitGBIYJSCYQkwmCUQAmFNzjAtl272LhtBwf3PMuRfftozC9gtUZZEvF4snwkyeMJl58zWIx2OU9jjCtqGQvGdfHMzS0QBAHlSoViqQTW0mq1qFarNBqNLEpMl9lCuK4kcBGk6yQapN1uMzg4SBiGGGMYHx/vIezMLakrx5gtpwVYY6lUK/i+jzURV162jcu2bwEbO5lRopVN2yzTJsgzvt/SSPkc74XUQSrdidWdSKJL7C667tPJE0fcf993eP7A8y/7/ZjjtUVOmq8Crr5sC7/8f/wUpZKPkB2dYvrhsYDV0JMPy6IIi8RDGJMt242QKGlQpoGSkmWrmDJVnj3doKX6kFaxtDzP6dlZ5mdPI2JD/8AAa9ZNUBoYQACxEKA8KkPDAImWULucp3HpAGFICh2W2BOIkSG23HQDm3ZdzeFn9nBi335ac/MQRvgC14stBbE1GCGyrp6syCIk0rq+bh1rlJREYUSz3abWbBAUCpgoThzSB12VWggayURFpbwuETsIFDPTpylXSiilKBQKLC0tEQQBUko2bNhAsVhkamqKxcXFhPjcDB+XO3ROUSCRAkZHxpidnaZSEPzE7qupFtwxSNx8HpdnNe5LAonpiTBXe2V2S45ApfnrtKiTLOGFEFipQAhMEpUqYRHKAxQWdxvGYEzMwtwsX/nqVwgTL9AcbzzkpPkKUSoG/PyH3s3gYBnPE9k8rOQjeNaOldXLrrTinUZwEosSbohtJDya1uPE3ArLIRSqFabn5liqLVHtrzLQ14cycOLESU6cOMGWQoBXKhG5J88c6Eiymx0eEE6LaAxxFCF8lyeUBR/P97j8+rexY8d2Thw4wMFnn6UxP+9ym8mSVSERxuAllWCb6Cm1dZlVpwO1mUNRrDVxo4G0MNg3QOB5FApFoihkbGyMpaUlms1WT8VbJGTcbDYyrebY2Bie51Gr1di8eTNzc3MMDAxQLpex1nLq1CkgmTJpoFBQWCMoFQsoKQjbTbZuX8dll+5w0T+uqq91nORxe0dQnJ0wk3PYRZq2x+LvLFonuqLQNH/Zrc0UFh1H3H//Axw5euxF3nU5LiRy0nyFePv1V/Dh9+/G83FemSKNMBMnm7PkpXq0hUAqMNe4wpGyBkuMER4tUWS2YTk2u4j0Bqg1m9QbDYZHRpwQ3IAygvVCcPT4caanplm/cRNKeuhkxpBI55Lb9PnJtIgnjh6jWa+zftMmyn3VLLNqA59gaIAdN17PxKXbOP78QQ7ueZb6wiI+FqHBE9IJwJOqfeyok5gkbZe0IBprsg4jtHEpg+UaftFnaGgIrd0wtlbLyZOWl5eJotj1w0My6xwajQZHjx7FGMPo6CgnTpygWq2ytLTE+Pg4jUaDkZERhBAsLi9jrcuvCiTjY6MsLsziSfjJn3wPxWLB+Yjimgsc54mssJOcqXO86uIst53t7w7hdjp/Or6eaUQNFqs1x44d44tf/gphmEeZb2TkpPkKMDLYx//nH32YdRNDCJF2qnQvwc8+jKuj0Uy3dj05Jon+sDHWxGi/yoIOODS3QsN4yEJAY2EFjMXzfWIsnhLE1tI3MEDf0iKNeh3ZtdTMPpTW5Rld7i/C8xSnTpxkaW6O/mqVU0ePMji6hmK1QrlacbIg30MIiz86wo7BIdZt28Hx5w9y+Om9tBeWMFrjCVdpdrxssznfCGevZqzBRonI33bcjKy1tJotpsNpyuUycez62tOhaKVSmdpKPVuup+c21UsuLCzgBwHaGPr6+mm3nQypWChQ7esj1prllRpCCMoFHykMUbvBZTu3snXz+iRjmVbgkyKdTCPMs1S9u363Vp7j9k6isiNkTzSZqRFy0sJpsxWAk25pHfPow4+wb9+B83kL5rgAyEnzZUJJwc998J3ccv3leH5a6HG3Wdv5IHUT5BkdLDj5kcDi5l6TJUCtkDSMz1TdcnyxDcU+okizOL/A0NgaYq3BV4SxpuD5mDDGGtczbrQGz1W/SUhGAka4WTMKINJErRYD/X006jVWajWmpqcJiiUu3bmTcn8VFQRERoP0kAVBaXiEK24cZsOWbRx59jmO7ttHY2UZzyYphkRcr3DLXKE1wloUIKybbYQUztHI4lolrWFxcZFKpUKtVmNwcJByuYyUCh0barWae7SkyOXOrSQ2ELdCWtOzVMplGo0GBc+jXCoyMDhIPSkySaEYXTNMrbZIEEiue9vV9FdLKC/OIu8sHWC7q92i6/W0HXK1sqcI1EOYyc/s9uQLRFgJSrhZ5kJ1CTyThgJjaNZr3Ped+2klKoQcb1zkpPkysWvnVn7573+QSjkAdMoakOgi3QesV56T/uwmTjc0DdJBYwLQVqJlwGKkOHq6RqhK+KJA2G5goxhhLMpTRFqjhARjCaMIg6VQcPPUY2yyOk66hIQAbZlbmKFaKlMpldi4fh2nThwjKPiwAitLS6wtVXjumT1s2boF5XlUBwcolIqOrFSAtobixBp2jgyy8aqdHNyzh+PP7UfX66gY/LROnBacjGsbtS4ngEniaqRAGpXkERWtputnnw3nqFQrBEFAo1lLdf7Zees5d8lj1hMHIKkka0ZHe857qVQADI3GCls3T3DdNVeghMaaOOnYSmcPnWsp3o2zbXNmrjMzGu5plUyS3ZZMcuReb4uOY555+hmeePKpl7APOS40ctJ8GVBS8o8+8n62b1kLwo2/NSbMIgwhzowuV6P7urRe6yUdQzGKUJY4tdhkdiVEFodotSPq9QZaaxq1GkF/Fc9T2bih5aUlGs0m27ZtS4oxMqNwa1xOcGZ6iqNHDlMKClx52U4KhQJbt2zBaM3I8BLVah+BX0SsLHPyyDFGRoYJG03K5TIjY+OEVmOVJBKC0Jd4E6NcOfIOtl5yCUef3ceJffuJG22kMSgJOnbRm5QSvUosDrg2zUR742b8WIyOWVhcwvNccUbKLl3jKqw+t61Wm+MnTiA91zUklaJYKtJo1FDC8PZbrqdUlCilEfZsj3i250kJLn3dMmUSvYQpOhuQ1doyXeYZhsOJ2kBHEY1aja9+9WusJFKrHG9s5KT5MnD1zs28953X4AcCISxax0lyf3VkefZqeTcsOPMGa/CsIYoNsSwy07ScXGyxVA+Zmz5Bq9Ek8DwUgoMHnqd/ZIiBwUFGhoepLy+zsrzM6JpRvGIBLQUqMey1WuMpSdRuc/ToEYR1bu17nnmGkaEh1q9fh1CSkZFR+qoDzM8vIKWgVqvTarZQCGYXl1g4Pc/6rVvB9xGBG30bW4P1PSrj41w/NsElO69k/9NPc/LQIZaXlgiUQkmZ6SntquPu/pmcqCQQc91LNrlOvkAU2BN14sZymMg5nPuex+DgINOnjrFzx2Z2XrodX1qEic8k4m7hZBdS4+NVV3Y0oJ0ru6LVDjlKmSzLSS07Ehf3NHViDM8+u5f7v/e9XMx+kSAnzfPEYF+Zf/bxD7J2ot+5GAFCKITopgEnbs/mdL/I0s8kph2SGCkskfCZWo54/uQcy6HCWT5YwlYbEKwdG6fWqHPkwPPUR9YgkQz1DdA/PEQswDhnDayxKOVmnishuHTbdgqex4+feopmo87o4AA/fuoppPK44sqrEMpjZGKcvuEhDhw4QLPZSqI0ydLMNGEUMTIxhl+t4BVLBIGPMRrjB9RjTXHdBFcODrLx0kvYv3cPk0ePEa6sEHgKkcz6IVmSulyvWhXCdZLCxhqUXEW02aZn9pfbJLSzxvWPCykZHBpkYWEOpQTXXruLgYEqgjBxn7JkSWQrSfzfObdsvfvJsq/Dnte2k0JIzfW6jTlcIcjiCmJOmmlot1v83Ze+zHzeMnnRICfN88QHf+IGfurW6/F9ARiU8hMCcH+7pFYnyjy3bKULwrmHow0CyeJKiz37j9JoC+LY5SnHN27EWjCxYW5hHiUl69eu4+SJk2zcuJlCqUSsBFoYN+MWgY4j5hYWqZZKVEtlhgYHwRhuvO46jh8/ztLiErWlGkGhwP4DB1ipN9h+6SUMDA5y1dVX02w0WJqfY3pmGtX0WFlZoa+/jx8//QzXvu16hO9RrFTAl8SeoA34fWX6Cuu5Ye0YSzMzHPjx08weO0F7ZcUVjAQ4bwyb5XEN6QTbxOQ3qbQ7g5Aug94UtlOdRnSTaOq4LgkKBQpBwMrSAps3jHPt1VcSpDraTHuVlMhe4CU6k5w7Dvdn3z4NMnv3W9Dxz7RWuC8brXn6mWf4/g9++OLvkRxvGOSkeR7YMDHCP/77H6RU8hN5ioucFJ0PuxVubkw6HC0dnrY6IumGcv7pRNInxieKQ6q+wNTnGR3fTHFgFFksgzVEzRVGRgdYbrQIY8Po2nXM1euMVasYAUIbPCmQRvPcs8+yslKnWq5w+WU7KRaKGGNQfsC6zVtZWlygWK3iCcGBQ4cYHxtn6tgxfCGgrw/l+wxNTFBdM8rRg0doLK3wzN5nCQKf+vISCyvLXH7lFURRG2MtyvOwQqA9D6Ek1U0b2TU+xvzUNPuffoZTBw8hW03KAlRSnY61QSqRzNtJcp9COCs6C9K6zpzUKNnF3SJhJ6fbsSaRNLkSE0JIqtV+dBhTlPCuG69lpC9A4dIoxnqJXVuXPAzoGWIhXM41bVJwxagukXvPa9qZ7UMiHZNCZO8RgUQlQ9rc4zipU9hu8f3vf5+FxcVX+62a4zVETpovEYHv8YsfeT9XXb4N5TkpixQdWy+B6xs31mT2Yqtdbs4FYXUy31tilMdQf0BZRNx4+TYWIkFNNwibMQQFvGofIoopICkYiCPN6blZ9OiQa8hUHmhNfWkZE0Zcv+saFhaXqDebqKCAlRJjLV5fhaFKif6RIaZPnuTyK6+gUizx7LPPUfB8ntuzl6BSZuOmTRQqZbZfegnN5Ro7gCeeepL9zx/gml27kNYyOzXJ9OwsmzdvZnBoGIvEoIgwaL/A0OYt3DQ2zsLll3PgqR8zffgQKopRxiA95RbGxlX5u0dM2Gz5bDPNuRXpMt2ma/Jklk9yxqXEWkG12sfp6VNsWTfK2666DI8YYTVag5Uq6W5y90m1o6telWQ/uhftZxewi7TEnzx/J9WwSo6UEKZUEh3FTJ48zj333XdWY+ocb1zkpPkSsXn9GL/w0++hEHhI0RFFk2kOk+WmkNAdkXD26nkPkjyfFGB0yPLMAmuqARsu2cR8I+LYzALztRWWWhEL7SL9pRJDAwPErRZtmoi4hdJtUAViHSMRFApFhodH0QhasWaor4oWgBJE1roeJAWqWGBi6xakFSwvLBBawzN796KU4tTBg6wZGuLkieOs27CRYl+VQCmuuuoqjhw+Qr1eZ6C/n5WFJQpC0VxeYX5qjqHhUfoGh1zOUygXaQcVxjZuYWR0jNlTpzi4bx/Th48QaNDNNgUpIXZVfivASjAqXbp3aWAF6FSdkLQlOn9Kx7jKSsbWjFJbWUSKiBtvuIah4X4QkSMrY9I5oLxkpRGdKLL7b8eNqWNRGlUmRJq89rZLNQBueqbWMdZqvnP/dzl85OhL24EcbxjkpPkS4HuKf/CzP8n6tSOASUveYA0iIctMqJyGPRasSEjgHLKjFFksYw0KQWNpjsu3bEN5MYWKZGTLGNNz8xxaDLHtCNM0tBEUgwLBQD9BUHCPoWMXaVqQhSKjE+MsrtSYWL8OlCKWrh9cCwtJXzdSEUaWYmLusWPn5egwZHlhAYxm/3PPUms02LJlK1Nzp1FCMDY8wrXXXEMchtSWlmnX68zPz7Nw+jTr121ibmaW2lKDcl+VodERrJTERqNUAVNSjO64lOHNW5g+dIhT+55n+uARwlYbz4qMjIwUGOnOrLKJu1JCqCbNe8rVnkbSyYyKBWqnFxhfM8iNN1zjvuSkRZsX/wLLmhI4szKfbJHpRbNoNE1kJoSZeWt2/Y/opGqMMczOzvKNb30rjzIvQuSk+SIQAm5959v4ex9+F0oZPD/1yTS4nvG0+OOiTmk7FVOdCtbFmXnNbjhxeyqIN5i4RSAMtOsMBAUiEzMw0c/IgOHESsxkYt7RaMaE1kOW+ggqAwg0OmpjBURWoAoFBoPAFSBsUoCRySxwkzyXECjlEWqDrxSja8exUczY+BjCkzTrDa7evp0TJ05wYnqacqFI1GgxOjhIqVgkGBxg29atbNq4kYNHjrBmzSjtZsjjTz7FurXrCJsNRtaMoQLf+X16PqHViEKJtZdcxtr1mzi9/RjPP/U0i9Mz6KiFlC4iE+BMRXDUY9KlNBYjyIyCJRJh3Sz5/r4hwmYTTJubb3wH/X0FpNIInIkHSV5x9Ur7hck0rYT33CO7T+qdmfaVp3lMKVVi1Cyzu1priaOQb975jXyW+UWKnDRfBCMDVT75Sx9mYs1gIu1LrctcC6A1OutaSaUkwlonQ6ITsZzRycKqLiHlEYXO7d33AgqJk48nLEUvQMcRa3xB/5BiXXmAI3MNji+2mWuFlAoeFkOhUML6Ho1mE2MNSA+RzDu3xmLimDCZyeN5ikLg4xWKLgeb7JPGonwPPMUlV1xBvbZCEAQc/vEedmzcTNQOOXXiBFMnTtDf18dll13GwNAQsdFsMJqZ+VlOz8yxccNaxsfGefiRR7jiyqtp6ZiJ9RvwyiVnJQe0lUdQ7WPtzp2s2biRU0eP8vSTj1NbXMCPNQUEyibEqVL7uU53kRJJUUgbfM9DSo/RkWFOnTzI+olR3nb1TjyZfrk5vedqvuzuBlr9+oiEkntz06Lr9esI2IVwFm+ZGD/ZRqq0bRLAYoxmcX6Or915J61W69V6m+Z4HZGT5gtACPiFn3431165DSEtQeC56CwxrHWVaC8Tt1vc2ASSvBWy8+E5F5x8RaGtQHk+cRSjPD+zRmvU61T7BDaO8SUUPZ9CIBlY18dEf4n9p+bx2m1suEwjbCILZYJKlQBNuxW6OertkLnZ0ywvLmG1QRiD77tRufgF1m7YgPRU0hdOsnx3y8nCQB/Ehs2bNzN1/CQ2yQkuLC5igf0HD9E/NMDgyDCjE+MIC+VyhdrKCkePH6ZSKfL8oQNs2bKNhx56kL6hYS69fCdeoYBRklCA8STSH2CsspP3bd/C5NGjHHr6GVqzc9hWhPAksTVIBRaNtK66LhLlAkkr6dDoALOnp8BG3HDtVawbG0HJ1KfUvaAiLSJ1E6FdVTnP3gDpf2exhuvZKL2kA5Bd5KmUh9EG3w+IwjBRWxgeffRR9u3b/xLfhTneaHhFc8//5E/+BCEEv/Ebv5Fd12q1uOOOO7KZ1h/96EeZnp7uud+xY8e4/fbbKZfLjI2N8Vu/9VsvMs/6wmDLhgl+5gO7KZUCJxJPJjEiXNuf9ALiGKz10LbbzMHl0FIdYoruUbQ91drESMNag+d5rBkbQ1uL8nx8v0Cr1abZbGKMRUcRZWWo2gYbSjGbii3evn2ELf2CQRUiwjrt+gqEIX3FAiUlmJ06yfL8HMpCX6XMYH8/ge9RKZcpeB6TJ0643FoiQNeAFhBLZ5xrPY+RNWu4bOdOLtu5k1JfFeF7DI+MMDk9xdTUFE899RSHDx9G65i1E2Ns27aF9ZvWs3n7VvyCz/DIEFbAyNBQMkrD5X+NBW1hoVZncmGBpUizeecVvOu2D3HNO95FeXyClhcQez5tBEiFJ11nlLTgSYkUgsD3KZWLxDqkv7/MdddehZ+I/CGJTjNy631dugnzXOToNherLnQV/dzQuXSip0y7fpLt3LwoWF5a5K677iF6A77fc7w0vOxI89FHH+U//+f/zK5du3qu/83f/E3uvPNOvvCFLzAwMMCnPvUpPvKRj/CDH/wAcPO/b7/9diYmJvjhD3/I5OQkv/RLv4Tv+/zxH//xKzuaVxFKST7xC7dy3dWXASZxwokTowyLFR6RFiwsrjDQP4SQFmMiAgUQE+sIoXxeTNye5u1cO6ZByMC59jRbGG0pFEso5fJltVqLoFAAYmzUQsQxW4YLqHKJkQEYq2mOz9c5XaujbYm41WJxfhERhVTLRSYm1uP5vpMceR5z83OYZhvdjllYWGBkzYhTJSZjLIR1XpwSiDCoagmrDTt2XsrQ6Ajzp+cY6OujtrTCJdu302g2OHn0OOVKPwMDA4yPr6cZhoyu28DJyWk2bN7MxIb1WOnhYkSDMoLa3CIHDx5k7fg4La/NicYM6ybWMnFVP2M7LuXIvmc5su85GouLSGOwJkQaS8HznXWc8in1VdFCo2nzjre/k9GR/qTqnkzozHKSaePBSyybv+CmXdX07oKPSKdNCqR0o44FEEcRe/bs4fEnnnhpz53jDYmXFWnWajU+/vGP81d/9VcMDQ1l1y8tLfHXf/3X/Nmf/Rnvfe97uf766/nc5z7HD3/4Qx56yA29v+uuu9i7dy///b//d6699lo+9KEP8Ud/9Ed89rOfJQzDV+eoXgVcd+V2Pnr7exDCJjNk3AybKNaJltDj8OGT3HPP93lu3yGajRilAmLjCg4d09nOB+usvcXWItB4GDwJ2Ngtm62bStlotWnHBi8oUu7rJ9aWKDY02zEW50geSM2wb9k+5HHd+jLXb+xn0LeIdpOV0zOEjQY7tm5haGQQEXjEStCwMcWhfqqDA4xPjLO4tOiMPpTEYJLCkUBZ6XK1nkdLGGJfEivB0OgI27dvZ/vWrZT8gMnjJ9DNkLnpOQ4fPsKJk5PMzi3g+QWk5zOydoK1mzdB4BMjMEq6cRuR5vSpSSaqA2yfWM/miQ1oLalbS7NQoNVfZfOuXbz7Qx9k1003UejvczpLpdz+SoFQksGRYaamJxkcqnL11Ze7fK0UTv+ZRoap7lP0RpRn09OmGs7kJTorBF2rB0EyKA7Slsnu19wYQ7PV4itfu5O5ufmX/kbM8YbDyyLNO+64g9tvv51bb7215/rHH3+cKIp6rt+5cyebNm3iwQcfBODBBx/k6quvZnx8PNvmtttuY3l5mT179pz1+drtNsvLyz2X1xLjowP8xv/584yOVFAuYCCVkgghsVKw3Giz/+ApKv1rOXlqkfu+8z3qrSbWGrSxkLTLvRBSbSckwnhj3URCoymVi1QqJYxuU6svUasvY0xEIXBtm9pCoxU6JyETUZQRVdlmLIi5ZEhx04Y+JrwmfmuBLevWYKyh3mqjggKVviqFYhEhJUG5SFApYYTNBpMJDUKnPkkxQmhs2EZGmlNHj3HsyDEWFhZRQYFCpY9LrriS/rFx5lst8D1q9Rr7D+zn2NEjNGvLnJ6cpKw8Aun64FECbQ3aJhoEaxCecvtgNCrwEcpPzreP9UuogTVsuvZGbrn959h2480wMEA7SZMMDw3TWKnjY7j+ysvYuHYMaw2RiZP+zEzCjpODubp755IimZvuVLOcEWJmHbJdS3SRenAmzyBTwhTuPZBU67WO2btnD/fc952kip/jYsV5L88///nP86Mf/YhHH330jNumpqYIgoDBwcGe68fHx5mamsq26SbM9Pb0trPhM5/5DH/wB39wvrv6siAF/L3b38m7dl+J51mkSj0XwRibtfvtO3CUWgta7ZhqyWd4cMDl0LKBEeqsq7qzGU1oOlMKIx05yZFfQAhJpVwitppYxzQbNXwVoJSiVCqjdZxVwqUEYTVSGjxjmCgYJgptLpsYoG+snxqw1G5iTIRRilJQoFiuELVbRHGMtMa1LWoQ0o0hdp09xhVa4pg9TzyJUB7FUpmZeouwrRlfuxa/r4+RSpnS6Citeo2R1ihPPvkk1VLA00/8iNjAQF8/K62QysiIMxNBIKTACMXQ2nGmT00SzM9ilUIWAldWscK1W0qftonB82FohK3X38jGS7ZzfM/TLJw8Qbmvj9mpWdaOjvD2667BFyYxLTaukJ0KzFPVfEqUPRpaV8jJOn8sWQSZQbi2TqTXIVohs7ZJd52rnltkRq4CS21piS9+8YvZpM0cFy/OizSPHz/Or//6r3P33XdTLBZfq306A7/zO7/Dpz/96ezv5eVlNm7c+Jo818SaIf7RL3yAUtHH89zMHmfI4fSNAkltqc1jD/+Im25+L3v27mPN6ARX7VyHlLFrIBTKmeh0e0ycI7rI+pWNQQiJ7wWEkabRaKKUolgs4gmP2POIpcZqN2s8nc7o+x5KeWAN1iQu5AKE8vCwrB0qs3H9MKHwOTW3yOzSCo1Q0I4CvEKFPi8A5VEwlkBarIiwCLRIPu4mQMdw/OgpgkKByy+7nKBQYmGpxuz8PO1WRFAsIKRHuVKlXCoijeHyKGZpYZFaK6RSrnDk6HFUsURlaMhFlcZihSA2moGhQQqFAvVGjcALKPdV3XHEBl9KdBziKwHEWGkQ0iMYGOaKW95JtLLM3PHjTM8vsv3S7aydGEN5Cmt1OqKI1RGjPcv5dznI3ts6UrGOCCLVZCbhK0JKzOpnSF5rqQRRGON7kgMHDvDQI4/m9m9vApwXaT7++OPMzMxw3XXXZddprfnud7/Lf/pP/4lvf/vbhGHI4uJiT7Q5PT3NxMQEABMTEzzyyCM9j5tW19NtVqNQKFAoFM5nV18WCoHHL/3C+9m4diSbhS2lyggzJbb5mUWiZsSxQ4fwMJQDQbEgXaQnEj1g0l4JLyac7qDdbgOODD3PJwwjarUGvq8IgoBSoYgxbm54GIa02y0KhQBIq7OJHtM4wq6Wi9RrMwzJBqVKlYlChYWhgEOzy8w2I2qtZUJdIAwNg9Wq2z4OMTbJAhqDh0ccxjRWmqwbH6dYKGGFZHBwgIWlZZrNJoVSGWudftUIhRaS9Zu2wnrD3OnTTE5NcXJqmutvvNGdE+vOY5zYvwkhKZZLlCplYmOIoggpFZ6QNOt1astLhO061b4y1f4+Ys8nUj6hX2J0aA3FcoWtW9Zz/Y5xvECS2jr3nnWBG+ebLqk7M9Kh60tt1WvV0dJ2FEgmMXlOiVZIlWh0uyNTg9FuoFy9VuPb99zL1FSviiTHxYnzymm+733v4+mnn+bJJ5/MLjfccAMf//jHs9993+fee+/N7rNv3z6OHTvG7t27Adi9ezdPP/00MzMz2TZ33303/f39XHHFFa/SYb08XHnJZv7Bz72XYsnHmBipBCQuN4ATtBvNpo3r+emfuo2VpVnKRdh52RZ8ZfAVGWG6LpAX0vd1YNzwHLQ2NBoN2q02AkGxWKRQKNBuO/Jst0KEUJRKJcrlsuvkCWNarTatVquTQtAWYQ395YBKIBgMBFVaDPuatVWPqzat4cpNa5joC5A25sSp4/QPDKC1oVKuUgwKFKXCMxovblESEf1Fj1Kh4KI3azk9M8vySg0/KCSzeGQmH0L5CD8Az2dwZJSt27ZT6evn5MyMW5InZGKFs3GLjUUolbV2esoZaugo4uixI5w4cZyFuXlmJ6eZnZwk1oZYejRQ1GJNUA7YsmUt46OD+J4EoxPFQXqGO8vydOnt4HSV7ryZ1S9LZ3vSAWmqc4vomAqLJNUgpeyKRMGtUjTzp0/zwAPfzaPMNwnOK9Ls6+vjqquu6rmuUqkwMjKSXf8rv/IrfPrTn2Z4eJj+/n5+7dd+jd27d3PLLbcA8IEPfIArrriCX/zFX+RP//RPmZqa4nd/93e54447Xpdo8lwIfMUv//3bWDs+iEgqodYasJ0xqyIta9sWG9YN8fM/937acZvAD/GlRmKRQmFN2hXipEQpzrlExxGt73uAJIoitI4plkoEQYDneYRhSKvVJoo0QeDj+T6VsiLWMWEYUq83iGONlBLfD5DCMjzQx+kZj5WVJYYHB5HCUvYCEIagz6O/OMxjteNsXjeAKApi3SasGZTnE/gFrGeImi0KBclVl+9gZaVGfXmRY8dPUWuFtGLLyRMnuOSyy9ygNymwShGZTmVZBQElpbjyqisxJPpWqYi1xkicw7w7u06h4Pu4Yo3l+InjKGu5bMdlVCtFlhdOM7swR6Ud4QV9VCtVWoszjPuaDSMVKr7BxBFKpd1XIjvD2dymlDTF6kKP7fzorqxnEqI0P9n5MuzkQkXSLplEsclDGWPQccy377qLyTzKfNPgVe8I+vM//3OklHz0ox+l3W5z22238Rd/8RfZ7Uopvv71r/PJT36S3bt3U6lU+MQnPsEf/uEfvtq7cl647sodvOuWq5NhW51Z3SnNdfguxlOAiCgqQVF6KBEhkh50l/9KCgpd1fHVQ8G6CVQ6vQrWeqiCyEiy3W5hdJMgKBIEAVhBGMY0Gi08L8bzFJ7vlvNSSsIwpNFo0d8nwcZ4UrF5yxYajRaLtSae5yNVDAh0O2R+aoaNA2WuuWwTM8tNphfbzNdahFqjZYAMPArVMkIYKlIxWClx+Ohxlhfm2LB5K9XBEU7MnCaKYzeiIiEcl+MTSEGiJJBIPzGsSIhKJi2QxhgC6WF0slxOxoYIa0HHjAwO0t/Xj1KSkdEx6u0IHRv6lIcXtakoWDtQYrgg8G3kiHvVl1Mn7nP/u93sLLKytkkrso7HM8cup7J4erk2+dumVnWJnClN6Zw4doxv3XX3G0pOl+OV4RWT5v3339/zd7FY5LOf/Syf/exnz3mfzZs3841vfOOVPvWrBk9JfnL3NYyNDmQTHAG0dtFKh/DSj02E73vExpGflAph3NhacN0nZ1vsnQtprd1PlqfCV4BHHGuiMKS2EqJ8H88rUCqV0FoTRRFR5PZPSonvFVEyQIo2RhuiZh2vWEZ5RaoDJaanpmk26vieM7/1g4BtY6ME5QAtFhkaLLG2XGVmyXJyqc3pZot2WxHFEr9YpITBhg3Wjg7Q3rKeobERQiMxOk4s8ejJ91lwc9yl88m0lkzK486TRVmLp3yiVptKUCCMIsIowvc8EIJSUGBoqN+NvTAWi6IdaYZLFcoeRLVF+lXIxuExKsqiEgOSHlbrmG1m5lSsKopjSUyJz0RnVEX2tsiiVPdUItsuTTkILNoYbBxz7333sWfv3vN4N+R4oyPvPQeKhYDLtm1ACIjjiNRqw3UBZZ+UpHfYtf/ZhCSFlAhclR3SfhPnfSRE1wTGF8hnWZzTe/oPK/A86Uw1CgHtdkwUxURhCFYkhSKPdrtNo9HAWkupVML3fYrFIkIYmiZkudFEeo7Ex8dGEGYQJcBYDULh+R6KEIEhtG2KQYH+NUUGyx5TtYiTi3WWohjdNMTEDJYDRgf78ZTk6NQc87UmoyPDeJ6ipQ1WqaRY1l1kSX0nXUtmSldCCCQG4hhp4fCRI9TqTfxSgLCWSqHISP8A05OTaH2S4ZE1RNbil0pUK2V8E+PbNhuG+xgsFbCmQSpjz6Q/qTgdnClJz2ugWI3Uls5mr5d7NDdd1N0mhcQK6Yo/CUkiJEg3rrd7JbG8vMyd3/wWcZzbv72ZkJMmUAh81o6NEoYhSlk8v0AURXie73KayXZuSee5ZWeSFxMkInbrxlwAGGFAuIhr9VrubEt099FMl/FAZgjilvuFQuAKLTqk3W4Tx3HWfVIouEJMvd4gCAKCIEB6Cr/cT8mPaTZaYA1eoCikPCE9YumhrUTaEtbEeAKMbuJ5mkJfQH8xYLTic3K+wVythScNo0N9RDGMrxnGL5aZXqoRCZ+GjpDKJzQGZ4rhTNyyanIXV1kXciaGIECsOXzoMMODQ2zasNl1LEUh7XqdUydPoKymXAw4deooofTYftkVeJ4iaiwxHAhG+soIq0H56NgpS7Pn6jLqcD9T4uyOROkp2HVu7ppNnt49exG7pEqQkGUiA01e0yiKuOvuu3OT4TchctLEzTH3fZ84jrAWGg2TzN3WSOkhMiWeTTqCVOb+bazB2mQAGOmUGs7Zs3z2Km235CWZ/51U1F20q1GeoK+/QhxbwigkCmOstRQKBYJCAak8rLE0221EJCmUS/iexOvz3RxytDMRsa4Y4/ZDovHciAZiAg+0jRAmZlD5lMsBo4UqtVaR0GqKfR5L9ZDJhRrEsGZoiJUI4mabpo1dPthYpE2/HFyUq4XA2LRIYsEapHUTKg8fPcqm9euplPucq7zWBH6AqkC5WqWoFBNrhmhHMQdOnKJULmKkARtR9AQrp2d5bM8pRteMsHXDOIVEwZDNcKITNaYFvSwCTXrr09epuzOo45WZ/u1IVCcJBiCJMrtkS7gvVq1jFhcWuPuee2k2my/tTZjjokFOmgACoiikHbZcBVv4bplmDdZGKKWyCDFdfnc8F534XSM7xQPbmRtjs6o7Z0SYKVxxISWUNCeXbquzCry1BqUUgXVz0sMoptaoofwCQVDAKxTwlELHmmat4bw5fQ+hFMam3T0kxG4QInY0n5BJbAFU0nmoUaJFUcFAWRBJ0KqJ8DQrIiQSinoUofDor5TxTUit2UBbjTQKKT20lEQWjJBo0RmmZq1BWk2jVqNaLFCtlomNRkgfCWjtpnyOja2j3awTIWlFhpJfohQEtJvLLM9NYVorrN2yhf5ilaee3IMxMTu3bUDI1DezI//qnHOV3NItPSIpYJ25KuhUzJPXV7qLSQbICyGTFKd0hkrWtcIeO3qUJ5588uW/J3O8YZGTJhDHmnqjQRSVkEkbiTEGKQVKuaWXUt3i5Y4juxAglMAaMns7JzR/8VLQi+n2OkvDpOKcLgc9QUkVUEphGg101KJlNJ6XmBj7itga4si1WfqB55btMtWdptFwGhd38oxZwIvLySa1ZqQVxFozGCiigQCvofHikKjZotEEIQOqhSpxIGlHhnYYIowlUIo41vjSkZQVZNX1qNmiUioRa430fLSJkcpHSrDaua3PzswwbSMCP2Djho3IsIVt1CjGIYOBz8hglYnhTaw0V5ifm8VsXYdM8pVnfkmtWpZ3W/UZ4/KSqyCSJGfvTPP0sdIvSrfsF0KijaZRr/P//rf/Rr3eeNH3QI6LDzlpAq12xNHjk2zaMIBA4QeOPFICFUIjhBtf0KNEsR2yEYkFWPrhSnOO3aMuejwcz/H7mdd1loluBpF1HpFC4vuKarWMsbhiUdjAKp+gWAIJypNYbWi1WiglkdI/y9GLczxf94gOgTTOv1KiGat6lMs+1dAiFppEYUhDQzuy4FfwChWUH9Nu1SGOKWARRmCsRUtnYmGQWKuJQkOrXqfc34cnfeeEj3CdQkoS+M5TdGx8jP7BPqJGnZFA0T82Rnj6JNVqgci2iVo1Bof6sqXzWX1LVx9594t5riaEVaYrokuP2blP54vWaM2PHv8Rjz3+o3M+b46LGzlpAq0w5Ed7DnDD23Y4l5sksgwCD51WVK1FSotSLtqU6Uzrrg+b7ztSSodndazCevFSIswzf3fFJoFACYFOPDildHIZrxQQeq7ar+M2OrIIKSkXi+ggcDnXNKfXE3B12j2hQ5bZ86abW4snLZgYawwDnsL3PMp+lb6i5vhcm4VGk2YIYeQ0ooOlEjqOCNttZ7orE7WBFVijXfrCWtrNOtVqBSlNsqQ2KAHTs9O02y2GhsYo91eJTIQkYm21zJUTm1noDzh29DCxCfF9uPyyS5CqY8l3tvPcO8qi9/rumpXtCrl7xexkAvhucbtL1RpWVpb51re/xcLC4gu+xjkuXuSkiatSP/zkft7/zmvZumktQkjiZJysNhpj3OgCz/MQIok2Re+gLSe16UQ3Z7oZnZ0oz0aQZ5/BbTsCa5uYkXWJ78GpACwQRzEm0phY04oblEqlzDl8dTlbZwXmc0Rmjqudma6JsFbjCQFRRFFYpBQEgyVGqlUmFxocn6uxFLWJIkEcKfCLBKUiNjZEWid5U4EnJZVymVOnJgkKBfr6mhSKAqVcAa7WarK4tECpUKSv2g9CEbabDCrJ+EA//YFHZWKC+VqAFjHDl15KqVig2y3/DJH7KsJc7XVqzxZtJnWj3i/ITv7aNUIk51Jrnnv2We67/4G8ZfJNjJw0Ezx/bIb7H3qGiTUjzmcyEM52zVek/clpRKGUM4UwxmRFIiCLLs+I1uAMQj0XSa1eJru/U+WhSKrebtIlpivHlkiVBBD4PkoVaLdDWq0WcRzjed4LRl5nu6472oytxgqLFBIM+Cg8q1A2xvfaFH1F3xrF+MAAJ043mV1uM99o0rKGVhygSlUKpTIqiohbTaQ1VKsVqtUKK7U6U1NTSOU7FUMU0Wo0iHXEhnXr8QsljBV4wOhAlbKvOHLwAPXaAiERxb4SfZUKBWt60govtkTvnRDaex46r6P73b3Oik6xXWTvCXBRpokj7rzzG6ys5PZvb2bkpJnAWMs3v/sEV166mat3bsZaH2OSYWl4GJN6ahq0dq2Ojjw7SzeZdPSk5Jkuz1OiTC/d17v8mHWFiK6ulN4Purs+NpZ2KyTwg2TKIbj6vVlFEK5KXih4KK/kilnoNEx90XNx1ug3KTZrNzA3GZkr8K1FmQgp2wQS/MBnYN0gp/tjZmohxxaaLLTbRE0IkSip6C8XMVFE2AqZmBhHzpxmcWmZwIdWo4nWmmKxwIaN66n29VEqlqk1axRsjKnX2Hv0eUYCSV+1RF+pj0bUYt/+fWyYWMvo8LAjdlzqQUnlVAPZalus5sf0hs6vXV8k2QjetAsoWY73GnO4KPPQoUM88eRTL3p+c1zcyEmzC1Ozi/yPr9zPrw/9NONrBiiXgy5CEl0RpeMea93pk4lbt00inbPlMs82vjfLu52DyDqtm+4+URjTascEhUrStZL0xyfFitT0wiaJSCEFXuLMk+pDz5A/dSTg6bOeIzJL9iPpz05nCFmco5KMNVIJfA0Li9O0ahEbRtcyPjTEsbklDp+a4+jULG0NXlBkfGIdff0DhGHE2rUBw8MjNBpNwiimUCjQV63geQo/KBDFISoOEe06K4vLTJQVfiBRCvpLRTZtWMv88iL7nn2OUlCgv7/fHZuxJB2byRGcPZ/5Qkhfe2s7HNmdy0zPbaxjvvHNb3Lk6NGX/Ng5Lk7kpNkFCzz13FH+x5fv5xc/8m7GZT/WpppNm0SaCind3HMXVbqo00WW4owIM0U3WWmts+i0W+bTjbN9MMMwxvdLxNrSaDTQJqa/WkV5HqkMCpHun+gR0p9tGe7yeN1i7/Qs9G4jrJMcJarUpE3UpByaRGIe1gqmJk/z+NPPs337JUwd2s/OK6+kMhgwtfcUlw31YYuDLLY1U6dOMREEBIUilcCJ80vlClp3pFxgUZ7rEPLiNu3TM0wMFVi7dpi1I0OUpe+0n0oyPFBl69ZNHDp8mF27drlZ6VKes5ngbDh7qqJLf+tambJtLZ2K+akTJ/ja1+98Q05VzfHq4hWN8H0zQhvLA48+y+e/9n1m55ZotSLarTC5tGm3QqJIE0carTVhGCetjTppe3TO6saYc+bSUlKN4xitNWfbqjuXCRC2I4y1FMplGu0QpIcQPo1GyNLiCu1WTNq2qXXqNN+7zHYP2UvGvU+ePGf6I0tpJlLxpF0ULFYajDAY4cZ/IDy0USyvtKnXWoyPjlESlrJpEy9MY+ZP8fYrLuGmndvYuXEdA0WFDVuErSZR1CYoFChXB5FeAVBYA0W/gLIWWk1a87OMVcqM9lcZXzOCH3hYqVHSIo2z5RseHkbHMe1W6wXzmd2x9epIu/vLTGRL897leHf0aa0ljCLuuutuZk+fPuvrnePNhTzSPAvakebb33+GVlvzsZ95N+vXjuDHEX5g8DxNYCQETt7jeekHKZmv3dWBotTZl4HpBzrLgeqOV2ene8URlWvllLTDFoVChSiMMLHF930EbiRHq9UkjC3CT7SlwiASl3lrXUcL2XI8SU6etc+ztyiSuRNZiyUmvau7ezIDyXbbvRk2TIwwPTnDwX17uPSSS0EFGCuIojaENSaG+qmqAkFzkJWiz+l6i0Yc0oojTGwIiiV8IZFxSNXzqC/MsnL0ANHSDJGyVL2NLM33MTY6hBWWWDp5kjICXyiGhkdotyOKxXJy/LpXgrnqkE12Xa+u1oLT5Uo3z9xF2WCFRAuFMG6Zr2PLkUNHuPMb38ijzLcIctI8B2JteOCxZ5FC8A9/9t2sGR0AQCCJ0Unu0nUie8gkklMYo1AqJRuyZbvLicps2dmN9EOddiGlsF1LTDc+OKIdhXh+QLsdUq1Wabddv7zTiAo3fqLgI4VyYyuETByUskdd9fMs6QF7JqG6QlD39Um0KtLf3ZiPwf4q77jlBoRSeJ6PtjHV/jJDQwM8+vAPGFkzhpCS0fF17Fg/ynw95ORinZlaSC3SxDokkh6lQkBo2hybPE6faXP59s00a8ts3rSJyckp+ipF+ioltDbJBEiFTNsak8hQa50U6zpV8qwQlB2zoJsw0xckiy7TQ05vk64vy41pdo5R3/zWtzh46PC53ko53mTISfMFEMeG7zyylyjWfOzD72LtxDDGQKwMWqukT93l8lIdpMt72qQ45ORJrhtH4lblnU9sugTsLKXPdEBSykmFyuUy7TCm1mgglUJ5Es/3qNVq2UyhZqvhfBytsywzFqKwje/5ZMbIZ00GvDBc/u5skSldBNKJuIslzxmOSI0Slkol4J3vuJG9z+2j2ayxfuMmNk2MUKDFSF/ARGWImUbIyfkas7UGTS3RTUkjCpk5PcU73n49w1JzetZSCDyGBgaZn5unWtngqtpCEhvj5sTX62zcsAHo7uha9ZPO8bj7izOOqdukOH3VSLa1BqRyXqJLiwvcc+89aJ3bv71VkJPmiyDWhu//aB8Ly3X+4Yffyc5LNlEq+Firkw+KxfoqkxN1y40cj7pT3F097xR56Mk99haQOjmz9H6+p+jrK6ONRMeaWm0ZIZx1XBy7sRelchHntOOKRZ7nueIKLlpNK+s9sN0/EnLs0TCCI5hkO5v+3YlSO9V4AGcyIqRAG40vFX19Ja5/2y7Akb00ENg2mBiBxK96DJX6mW9oJhdWOL1Yo2Asw75g7fAgxbjJ0jwszJ2mr1pyEzhdaxZhokNdWFxgaGgomyKaHtVqAkzPZ7rk7un0sSQOSZ2INc1niq7trTHEUcj3HniAU6cmX/A9lOPNhbwQ9BIQa8OP9x/jr/73fTzx9EFq9TpaW+LY0G47m7Y4ionjmDjSxJFxP+PUYT0iimLC0P1Mo5JeAwiS67zk4vJoafU3lSgVA59ywaPoe8TtNqVCgDGuv1xIiVIuyms120gUEo9GvUWYWMml5hO2wylkZfC0ULSaCNPiTybY70ivXDdklyBfCDQaKy3WuJHGxkYoNJ60BJ5AaI0QBi0jtGwjVURRtOmTbTb0Ka5ZP8z1m8bZ3l9irYLG7CRrRvopFwLmZmc5ePB5CqUykTZoCwbBSr3O7Owso6Mj2ZePW5rTtd/dx9XR2Gb7ngxa6/bXFCSRqBAYgYvirdPVzk5Pc+c3v0Wr1Xplb7AcFxXySPMlwlp4/tgU/+UL9/ALt93E26+/kv7+ciJHColjSaFQQIsIpZK+c5GO/wUpyQZ+GWOcR2Ymjk9fht4Pd1oocs9vOzuCJAg8PK8PpdxYjFa7QV+iT9TaoGNDpVJJilXu8RuNJnEcUa6WUUmPdqcIRc/K/YXaALs7hdKZPln4apKuoZSMSX3tBcpKpElbPyWx8BKdp0VYTYDF2BaB9Cn2B4yW1lAId3Dy+CGqBZiYGGPu9AxKSebnFwiCgEKhwOLiPEtLi6xfvw7f9xO9bCcn3Ntl1YmYswp7du67VwLZ0XZ+TXKc1jr7t73PPsuPnnjynOcpx5sTOWmeJ05ML/C5L36X04t1bn37LsbWDOH7EqVMktuUFIvSOR5hUckyXUqX6/SU5/weDRjtPtRSKWeciwB6pULdLZpOxpTa1Lm+b5tY0Hm+l0WlUeRmClkhCOOYVhhRKpZoh22KRTc7vcML7nnTYpN7jLMQpluXdpF4ulmX6tvtMKmivKP7TCK4rqjOmdMrt9QRSSQrLFI4D9PAEwg0O3duZs1whfmFRYoFn127drGyssLS0hJhGDI8NIg1mvXr1lEqFJPz2Ku57Na8rs5vZlE2q+VGvUt0EvmRxaVUGo06/78v/C3NPMp8yyEnzZeBpVqTL3zzIY6dnOMjH7iJbVvWUioFCOFynK1WG6Vc1dsolRWGpJZYL62op65JCQEp1ZmbbZ0jfPcHPO1xN9pgYo1Fu4KQkqAE/f19OPJzUVCx5AaVGWPw/QJhFDvhuwpQnuudX1xaxpce5XKZThW9W8jdge3WdpJudxZJlQVEr3NSj+ZUuOq1sKASvY9NswMimQEvDNa08AOfKA4ZHh1gdHQYhGJuYQmlFGNjYwwNDbppnEqiEqf99Hx1P/fZROvpdrYrQ9Wdb+6kTpLoUnSKfMYY7vvOd3jsR0+c4x2S482MnDRfJtpRzPd/9CynF5f5hz/9TnZdvgU/UBSLfvLh0plcKC0OpQ4+rm9d9ciJnK5TJctJkg9sd1UalPBBuI6iSDuBvRv+KJKRFS5SDYKAOE7zp1AsuKFr9XrDEbjwaDYbCBRKebRaLaf7zCLJ3pTA2YTigo7YO0WWO7T0kBFY53SOcHpWodyAMuNuS5/RWhJdpDM8tlGIh0B4AcZKTp+eY3pmjqGhEZrNBtbOs3bduOtR0hGe8pIW185+d85lZx87kac8K6GKLFruEGm6f8ZYlpaXueuee2g0cpPhtyLyQtArgLHw7MGT/N//79f56j0PMze/RKsVEkVR0iFk0FpnnT9ad66LoogwDJOfnd/TbqK0o6gntyhACmdR5/mJw7nVSXRqsVYn6YECxUSG5KZZtohjR+K+72OMpV5v4HkBnhcQx8ZJmxAIoToC9q7OF2yv6UjPpXsHE7gBZ+5isQgJVlqMMGgbE9mYWGkiZYiUQctkDLABYksRj6KVlIxAJtHdoUOHqJTLbNi4gbXr1hFHEcraZHVv0UZnu5BG7edqH+0ubHU7Tznrv1X3TVsmrctF79+3j0cffexVex/luLiQR5qvEBY4vVjnf3ztBxw4Osn/8VPvZOO6EUqFIFmqxkkkaTBWO9G5MdmH0xiJUhpjHKF297E7t3iSyY2pmNxikujJ9wOMtVkuU5DcT4pMdmO0xcYRUZj4akpDo96k4Pt4UtJuNikEBYRQGCGoN5qEcUgQBBSDwPGmycbFnSX6dLFit+TInrFsl2CTIxAyuz3LeYp0pk8S2UlBnHQHSJJZ6VawtLhMudzPyvIKCwsLNJotjHCFJqGciF+sqvynVe9udAw7nLNUtgzPIk+R/JNYoYiNRSoJNiZuNfji3/0ttVpu//ZWRU6arxKa7YjvPbafxeUGP/feG9h1+Vaq/SV830s0nQbfVyhlE9I0SKmSHJlAKXHGUj7Ve7pOF7dspYt0IBVwpx1JYHQyRljZRFgPyitjtVv+h2GE1hGlUpk4dpZpSnpooN1u09IxQvloFI12BNpQKhSwyVhhegivUws6M725Kv+ZDZsTWbEmnRNuk5xhspbudDBJN+zMakeGO7Zv5+FHHuP5Q4eQUvGOd+7GWOuGyZkIJVO7vA6cKUnydN25zu6968odi3SKZXquhYcQhiiKsMbw+OOP8cOHHnrJ74scbz7kpPkqwljLj/cdZ3J2kffceAXvf+fVrBsfoVgqIiWJkYZOCNNFnsq6ueku4lQZafaK5JP8p0idlNzgtiTmy/KfaXbQWpMQc5KXA2SSN/U8j1KphOd5NJt1ioUC0ldEWtNOqu5RqPE9iTWGditMyM4QBH6XwN2moWFyRdeJOEfhJUVH67m6Si96ItbOw1mkp1i/bi0/+d6foNFsMTA4wMDgAJ6SGB25Ecb2zBj3rMWqVGeaciNdRaC0ao5z5tddM9xXlpf50le+ytz8/AseX443N3LSfJVhgZn5Ff72roc5enKGj952Czu2rqW/v5KQVtqLDlKkXUSyJ6/WTZ7dxSIpFQZQJG7yuCq0BYwQnWW8tciufKiLUjs2aV5iJad1RKTBxq6wZaxF2mR+uzEYbSkUillHUQcp4XV3BaWtiS8V3VFf6sDUGw2m20kJ1mgKRZ81wRDC94ljjZfMRFKpFvMM9VOn66f3us6IClbd1lt5T/cO4jji2ef28p18lMVbHjlpvkawFh7bc4Sjk3P87HtvYPd1OxlbM5BoOgWer1A430shPKRIhkwKZ+3mSNK6iNTYRLYEQguMcrPY0+Vrpp+06apSItJcpxVYYwl12COnkVLR39+XzBuXSc+6xWiDVB5xZJFIgmKJZrNBMSj0ah9lp3ouMmISq2izmwDlGdcLIRKT+dWE26lYgzPFENK5xStfEhtHmFEU4SmFTSRd50fakNrkiSR/jOxUzW13isBo6vU6X/ryV2k2m+f3HDnedMhJ8zWEsZbpuWX+v1/5LnueP8HP3Xoj2zaPUakUSXWOzrotxiYzyVM9ZipBSsdsODmOW6pmUWlCnKJLOpM6wQvRud4ktnWp3MmZFCe5U+EKNzKpwOs4wtqYICgSeAE6salXngcijTazsk2iYUzWuLaX+Hp/Qoc4k+sSHb1bFXdvn5J7ck3SVWWsTp7CpSNSB6P0uNPun3NVzXvhniObBSREZ658mrcVbmqmNYZDBw/y+I+eOF9azvEmRE6arwNaYcwPn9zP/iOT3Pr2q/jgu9/GmtEB4tiNCZZdZJgidU1KCdQYg5AaIb3O31ojlOs3T0fXIjpL9LSHWkqZtVKmEqjMYQlHWqViiaKFRrNFs91GWFc5bzYbFPyAHk7rErbbroiQhHi7b3dbd0egq69fTbRnkQilt8pOlT0bF0KvjvTcussuYk5F/F2SoiyyTZoLSCrq1mjCdot77r6HycmpMx47x1sPOWm+TrAWZhdW+OJdj3Jico5f+OAtbNow7gTmns4iJSE6Jh2ri0JCCoQ0WGPROkYIZzuHZ9BKIqRykaqUWWsgkBFnShqpXhQMVkiEsI7WhKRcKuF7PkYb4nYDT7oBbZ0ok17SESRD4bJn6xGTO5PiVeJykvulGsleRl71POBMNDo/bVdk+mLx5JnFJrpTsgnpJxGmTY2GeyPVqVOn+Ma3vu2i7hxveeSk+TqjHcV8/4n9HD11mlvfvot33Xgl42MDBMGZs4W6B7S5WT0SaSxIg0nkMdokbZpKoaRGeB5SeSBUlutcbQKSErMxBh1pjDCJJtQtd6XvYTznnFQulUB0e0X2RoOdXvQ0mjv3sXdkShZhuvYtKWad/b5Oo5kK7XvOR5paANJ59Lar/fTM508LY2n+0h1P1mu+KidrjKFZq/O3f/tFZmbzURY5HHLSvACwFo5NzfM/vvZ99j5/gg+/71ouv2SDIygEUoqsIyiNMl1F3GAFxCaZXSOFM7gwBmkURnpJv7tGqKBnJnvv83dIuVBItZs6qZxLEBZPSsqlYicihGTpTxaNpX+kFfRUcyllyo7pcycJWTp/dlrRnR6ys1IW2SZZDjQjzLPIlBJxfW8+tQurWkBXS4tUV1ojy9haJ9vSsebgwYPc+537z2tAW443N3LSvIBoRzEP/fh59h85xW3v2sVt776OsdEhAs+J3RGuoyfwfYSSaCvRAkiE8cJKjLDIJL9ppMF4brCb8gzW85FSORcl2VWEEdJVpAGMTezULFqD0RqkRMcRMpM+OU6Smfs7dCvaDSk/pgSVbtLJGbofq8YDS9G1WW/F3D2DzdTzIi00iVWDeIUFQ2eqpujs2WqK7eQ83YJcJXrM9LgMnc4rYyAKI+677758LG+OHuSk+QbA/HKDv7vrMZ7Ye5Sf/snruf6KLYyO9lEo+LTqNTwhOmYdiahcJ+J1JSVI5Vod0wgv01lqhFRI5WXFoM7YC5nkBlNiclIoIaWbkCmEk88LkrbNdHxvakfnpEzgyCZ5Wkc6ZrW8qLdNMtVlGjok5kT5sjsV664XnaU/CESqxewOYpNWSStsV6Ta7RPaiTLTPfKSDqvOGUtn1wu0sWhtmJ2d4+5773v5L2yONyVy0nyDIIxinjs8yZGT3+LmXTv48PtuYOf29RS8IhaJsWCiGFQqKDcgDBYPnXzy3ZLedQopE2RdQVLFWdVdSi8p1Mi087pnP4QQSKWczhMXnbntSK7rrXa/sAQnWfZmxOxgTWqy0X21i6wdk9MdyCaBaiJBSiVOcNYaz5noPFCnICYdYSbLctcckJ0ApBTEkea+++7lxMmTL+VJcryFkJPmGwytMOa7j+9j/9FJfua9N3DzNZeyZqSfQsGn4CtsKheSju6MjpP8XG/+0tgWWvtOJG80UrpijlIpcSYbpsUnuovV7rGlxRGycRV7K1KBUMfYogO16ki6plxmAZ/NnigWNpNGpUtztx9ZLRtsOvRM9j5eF6GeC93Gw93XZVZvqbAdkSgIEks+C2EYMjV5im99+1u0w/CFnyjHWw45ab4BYa1lcnaJz33xfh5+8gAfeOe1XHf1Ngb7Sviec4dPpTdufer8Kl17pkoq7SKpo1iMcd1F2liU9FEqEdGr5OXvbjdMo7lEQ5l2D2FdwekMsso0jSmSPKdIIsd0G9tJcdokZHTdUKv0kymPplLzVZX11IBDpNX6s7Ln2Qkz0RjhUhNp0Sshz4RM280WnpB874H72fPssy/l5crxFkNOmm9ghJHmyX3H2H90ihuu2sbPvO96Ltk0QalYQCkQMnV0B5PmGdNwzIARFiGTJbpQSOlhpEVL7QTxnkYqz0WtshNtuWq9y2U64nT3t1KgbTroLa1Ii4zROm5syRK7RyIkevOVXXnKVetwnCNSz8a9xfHu5+m5Mp251PvcqZBfpoJ1Ui2my8cmtSaMjpECZqZP8b//9gtEUfRSXqYcbzHkpHkRoNEK+d7jz3H4+DQffNc1/MRNV1Es+AQFH8/3UJ7C951bUtoe6ZbCJqkYS5AkfeUGIaUb7GYMqBgpvSzyFCLpZyeVF/UWeVLicd2VJlFJkriwi6QfvhcWkiJVN5F13Z61fQKoF+khF4lJybnGWHRIOB0dkobQQqZu7N3bCbIJk9YShm3uufc+jp889YKvSY63LnLSvEhgLRyfXuBzX/ou331sH++64XJuvPoShgcrVPtKXX3XFmNipOksj62QGJVUma1xEhsZo02EjD2EVBjfRxmDkAqETnSgCilURqFJDTr5PyFQ4QhUa+cgLxPX8zRFYMWqCDPD6qEBAqxKVsvnkCitkn/2jq5ICdD23NdRcdc+Jdu7zikPLVxhyn3HGBYWFvjyV7+ej+XNcU7kpHmRIdaGfUcmOTp5mseeOcRHPnALl21fR1+liJICXwmkEpljUrpSFsagcRIjIzTCCKRRSOmI0lqD9QxCeoBEZC5LzjjEIpydmhSuQAPJNMxkMqZJlvSxy3vKxI+S1d3lSdsidMWdNinMZHoid2tarFmdSE3bMKFLjrSKmLMIm96lflpBT75O3LEKpzyIo4gnnniSfQcOvKzXJsdbAzlpXqRotSOefO4IxyZnefcNl/O+3VexYXyIcqmAMgIp/Z5lqDUuCrVJVxG44W9CaIRQaBWjdIxUgYswjcJKD6NMIm+SpCYW3flIay3ITh7U4mb1ZJbIPfnFlDDTSnkv17ncYke7aW3XEpvsIc6J1eSYVpBSokyvF1IlYnbXSWSMG1ZXr9f5yte+ThzH5/lq5HgrISfNixzzS3W+9p3Heeq5w7xv91Vcs3MLG9auoVQAL2mjFEklWwqnR3Rc4iJEx08Go2O00kgZuaW58vFUgPA8tPGQ0kMqhZMWdS+LbeaJmRKa9BRWO01n2mNju4NIIO3KAc6MEpP/Oh1B6WZJNT3t/um5Y9o/vupxSAtTnZypc6InE9jHxjk/Pf74j3hmz96X+UrkeKsgJ803AbSxHD45x3/9u++ybeM+3nvzVbz7hksZqFYolgIQBlRSoEmKIsakQu+kbGNxvpVJFV1YneRGA6TnozwD1kWvUiUD3FZpLFOasgKsEshEu9nTt50YbYiE+DLZlNUI4SWPYhNiBETvWzQlYQOQjDs22vkoycStKY2wTY/+0+U23bEnjy1xs5GiiObKCn/3d19kfmHhFb8eOd7cyEnzTQRjLc8fm+LE1Bw/3neID//kTWzdtIZqpYjyZGdCpdGud124mUVp2CWtwEg3lVLECqE0QhmkNiht3FJeeSilk9ZMDyUVlvSx7BlFH9fbneRMjcVikgjXRbqdnshUtmQyjahDV2GnJzrt+Iwa4+RXZ+Q+U/2pTcWfyf5ZUFIRa+M0o9byoyee4OHH8rG8OV4cOWm+CdEKIx7+8SEOHp/lpqu38RO37GLz+lEKBZ9iIcBa6+ayxwapkmU6SY954mhkhEEYA9ogPZv0sXtuPrqMkV6AUgajPFdM6nIP6kjObZYKsJn2MmEtEls4I0hX9r2zd3pJMK2Ed8g0uZ9wYv9UCpX2FKX0mlXjrcAmkyatcP3lxlgMhvryCl/76lfzinmOl4ScNN+ksDjT429878c89OPneed1l/MTN1/JurEhikU/masOGoNM8p2WhDQtWDRWeCA0UscgnURHSR/peXhaY7yESH3fFY+kdFVz67w+07xkKpCHlAYNFoPRBis06RLfzYdPDiCLErtMOGxv1Ekie3L6y3SbhDoTsswMSrJ2p85y3SIwWvPsM0/z1FNPvbYvSI43DXLSfJPDWsvcYp2v3/84P953hPfdcjXXX7WV4UHXzy4VeDKRsUswNk4E6xKsxorEpCPpXzdCY2KJ9mKUjlBe4Jb40kMohZda0VmVEZYblZsaLLsOJYFEKIuxBmM73qGQmidD99J8NboNg7MiU0+kKrPjEInoHpH2zEtMQphRq8mDDz7IQp7LzPESkZPmWwSuWHSav/nyA/zgR89x864dXH/VdkaGq5RKRQLfQ2qTLKMt4OazCzxXJEJgYoMRzjFJJLPVtdHIOEYpH6l8rOcjrEYJhecFrtfdgkZ3EafNfgohUNKNFHbXJbeftdvn7OgeyiakckPSMgejZFkv04F1Th5ljcEYzdFDh/nGt76FOZeJcY4cq5CT5lsMsTY8e3iSQydnefSZQ9z6zmu4fNsG+ioFigWPguclpJlGhTFWJT6WpI5ABqRGGI0yMUZqrIpRKsYajTAeRqTz2h1ZKuW7qrp75I7IvKelsSM57/7FWdz16jU7sqMOCadWdrLLGYlsVIYrSiXTkNDG0A5DvvL1r7OwuPSanOscb07kpPkWRTuM2XPwJPuPTnHVjg381Huu49Kt69CBoZBOyJQSbS0mjlFSOdKxGtBgYoSJncmF1Bip0MpDxiF4Pkr5LhJVTjzveQaUW7qnhh7GdoivG0J0rbTTvGia20xbKY113nXdG4u0+NNxL3KpzGSpnlwXG0OoQw4ePMj9330gH2WR47yQk+ZbHFGsefK5oxw8PsPOret45/U7uXL7OspFDz/w8IuBW5qbxPlCODclVwA3SGOwwmCEh5QxyBh8jfI0UmuUCpBKoXWM9H2UdS7ySIG0npvbfo6leLdgvptbjTFYmbRqpjcnkauU3a2TScQp0wmZ7kG01rTbbb76ta9yKh/Lm+M8sdo14QXx+7//+5m0JL3s3Lkzu73VanHHHXcwMjJCtVrlox/9KNPT0z2PcezYMW6//XbK5TJjY2P81m/9Vt62doFhgeV6k0eeOcj/84V7+Ou/u49nDp6k3oxoNcOkzdCgddxxh7cGa5wA3ugIHbeJoxY6DtFRSBi2CFtN2u0GUdwmjiPiMKTdbhOGITpOHsvYVVIjshxqtmpf1TiURp7pdaknp0wjyuQW5+4kk8jTXWetI83FxQW+973vn/ncOXK8CM470rzyyiu55557Og/gdR7iN3/zN7nzzjv5whe+wMDAAJ/61Kf4yEc+wg9+8APAvVlvv/12JiYm+OEPf8jk5CS/9Eu/hO/7/PEf//GrcDg5XilqjTYP/vggzzx/kusu38Z7b76KLetGKRQV0nddQK6TCDc6WKpkEey6c5QQ2MhiY+nGCpsIo0OUClBegFI+KB8TW7QC6cnEE1QihAKV2NolbGhtOjVdu+W8VSBACdxAOYG7X0KKbmKGBCvRKXEKiTYglSCOY6J2i/vvvou5uXwsb47zx3mTpud5TExMnHH90tISf/3Xf83//J//k/e+970AfO5zn+Pyyy/noYce4pZbbuGuu+5i79693HPPPYyPj3PttdfyR3/0R/yrf/Wv+P3f/32CIHjlR5TjFcNaWK63eODxvTx76AQ3XrWd3W+7lPE1VUrFAlLqpLsIrHVVdik8rIU4aoPUjvCMwmjn02lkTKwjlCzg+Rrl+a7ybiRaKmeKrAwSv9PDnnp1ZgbDFiHT+eQCTVbu6UShyS9OVC8TB6dOhV3HMYvzC9xz9z2EYW4ynOP8cV7Lc4ADBw6wbt06tm3bxsc//nGOHTsGwOOPP04URdx6663Ztjt37mTTpk08+OCDADz44INcffXVjI+PZ9vcdtttLC8vs2fPnnM+Z7vdZnl5ueeS47WHtTA9v8yd33uCP/5/vsR/+/IPeOrZ4yyvtGm3NFFk0NpmMiGtY6yJMXEbE4fouIWOWuiwSRy1iNsNwnadsFUnbDeI2g2idos4ahGFbXdpt4jDEBNrrHZDdSXpqF2FawOSiZwoqZgnF2MN2jjdpyucd+c2wcQxRmseeOABntn73AU7rzkubpxXpHnzzTfzN3/zN1x22WVMTk7yB3/wB7zrXe/imWeeYWpqiiAIGBwc7LnP+Pg4U1Mu2T41NdVDmOnt6W3nwmc+8xn+4A/+4Hx2NcerCGuh3gz57uPP8eP9x7lyxwbefeMVbN0wRiGQzsfTt2foL92cIo2xCmtiED6I0A180wHKK+J5Pta4IpKQEqkCpPRRHsmsNje6GAAhXF98T2zphOtCiiTP6nw+TWLckbZxGmPQcczM9BRf/upX8op5jpeN8yLND33oQ9nvu3bt4uabb2bz5s387//9vymVSq/6zqX4nd/5HT796U9nfy8vL7Nx48bX7PlynB3WwsJyne//aB9P7TvKdZdv5drLt3DplnHKxYBCwXcO7jLRYSIdcQqNRYKIQXquRdPEWK0xkYfynDBeKR/rWaxKPC6lRsUx0vMdocokmQnOfb7LLzMVrotEKuVI1JmTKOVjtSEO29z17W9x+PDhC3sic1zUeEWSo8HBQS699FKef/553v/+9xOGIYuLiz3R5vT0dJYDnZiY4JFHHul5jLS6frY8aYpCoUChUHglu5rjVcZKvcV3H3+Ox/ceZvO6Ud7xtku5ZPM4w0MVPE8m7ZTO9FhIkyylBYgI5ftooTCxRipFHClHml4R5fkoz0mWjPIwxnMieuWjfN9xppTJILl0ansSeSZVdNeLnmQ7hZvOqeOQY0eP8OWvfpUwH8ub4xXgvHOa3ajVahw8eJC1a9dy/fXX4/s+9957b3b7vn37OHbsGLt37wZg9+7dPP3008zMzGTb3H333fT393PFFVe8kl3JcQFgraXWaLHn+RP8zZcf4L/87QN895F9zMzVaTRDWu0Yk47A0AZhNMQRUatB3G5iohYmbDu5UtgkajcIu/KeUbNO1K4TtVtOwtRuEbbbxGGEjmKscTnPVIMppUIqD4tyozmSXvMwDEFrvvf973H8xIkLfdpyXOQ4r0jzX/7Lf8mHP/xhNm/ezKlTp/i93/s9lFJ87GMfY2BggF/5lV/h05/+NMPDw/T39/Nrv/Zr7N69m1tuuQWAD3zgA1xxxRX84i/+In/6p3/K1NQUv/u7v8sdd9yRR5IXOcJIs+/IKQ6fnGHsoWe4adclXLplLetGqpQLCoklKAYEfoDVGoxFG7BSI5z2yLVLmgitI6QKnVu88fGUdn3xRmOVh1QaoXxkMkHSWpEYIyskAis7ozS01hgdMz09yd333pfrMnO8YpwXaZ44cYKPfexjzM3NsWbNGt75znfy0EMPsWbNGgD+/M//HCklH/3oR2m329x22238xV/8RXZ/pRRf//rX+eQnP8nu3bupVCp84hOf4A//8A9f3aPKccEQRjEnphc4cfcjDPSVuXLbOq67bCM7NoziN5soJSmWSviBjzAWrSzCaqyOsbGXLc+Njp3hsdVYZVFaY2IP7bv8p+enzkgWYwTK85BSIL1kClHSGWSNJo5DvvyVr+QD03K8KhD2IvzqXV5eZmBg4ELvRo6XAAEEvsfG8SHevmsrm9cOMdxfQirwfZ9SqYxfKCV5SukclJQbKyyVcnlMWcoKRVIFKL+A9AK8wAnmZSKal56fFI08kAopFVHY4rln9/Av/uW/ZH4+t3/L8cJYWlqiv7//BbfJe89zvKawQDuKef7ELEen5hgb7mfHxnEu37qWtQM+0p6mUq3QPziIXwgQvo/WAqF8jFZYU8AqN1rYegFSurZNGRQwVuNbi3IVIHQk8GUBHcf4vhPaT506wV/91V+xkBNmjlcJOWnmeN0QxYaTM4ucmlnkwaeeZ93oANdcso7LtwqE8ihEPngK3w/wPB/l+URxSCwDlAowXoiQbVRQQukYY12HkLEWY2KUV6EZNygUC0TtFktLc3zuc5/j4Uce5aJbTuV4wyJfnue4oPCUZHSwyrb1I1yzcyMTI31Uim7qZSHwqZSryKCElT5KeW60cFBEeQWEHyC8AkGhRCEogiyivDLWGmZmZ/nCF/+OO7/5LcIob5fM8dLwUpbnOWnmeMMg8BXb1q/hko2jbJoYYrDs01cuUukfxCsU8YIAIX08r4AKiqhC0U3F9AIKQQkhi7Qjy3P7D/CFL3+Fp/c+m1fLc5wXctLMcVFCSkG5EDDYV2LbOkego8P9DA8NUqlU8bwCfqGE8gPnaCQ9mq2IQ8cm+eGjT3Lg0GGWa7ULfRg5LkLkhaAcFyWMsdSabWrNNidmFlFSMNhXZmSgj3IxoFopUy6V0cbQaEcs1RpMzc6xsLTMxRcC5LjYkJNmjjc8tLHMLdWZW6oDielw4uJ5ES6UclzkyEkzx0UHa1OPzRw5Xn+8ot7zHDly5HirISfNHDly5DgP5KSZI0eOHOeBnDRz5MiR4zyQk2aOHDlynAdy0syRI0eO80BOmjly5MhxHshJM0eOHDnOAzlp5siRI8d5ICfNHDly5DgP5KSZI0eOHOeBnDRz5MiR4zyQk2aOHDlynAdy0syRI0eO80BOmjly5MhxHshJM0eOHDnOAzlp5siRI8d5ICfNHDly5DgP5KSZI0eOHOeBnDRz5MiR4zyQk2aOHDlynAdy0syRI0eO80BOmjly5MhxHshJM0eOHDnOAzlp5siRI8d5ICfNHDly5DgP5KSZI0eOHOeBnDRz5MiR4zyQk2aOHDlynAdy0syRI0eO80BOmjly5MhxHshJM0eOHDnOAzlp5siRI8d54KIkTWvthd6FHDlyvAnxUrjloiTNubm5C70LOXLkeBNiZWXlRbfxXof9eNUxPDwMwLFjxxgYGLjAe/P6Y3l5mY0bN3L8+HH6+/sv9O5cELzVz0F+/K/u8VtrWVlZYd26dS+67UVJmlK6AHlgYOAt+YZJ0d/f/5Y+fsjPQX78r97xv9QA7KJcnufIkSPHhUJOmjly5MhxHrgoSbNQKPB7v/d7FAqFC70rFwRv9eOH/Bzkx3/hjl/YXL+TI0eOHC8ZF2WkmSNHjhwXCjlp5siRI8d5ICfNHDly5DgP5KSZI0eOHOeBnDRz5MiR4zxwUZLmZz/7WbZs2UKxWOTmm2/mkUceudC79Krgu9/9Lh/+8IdZt24dQgi+/OUv99xureX/+r/+L9auXUupVOLWW2/lwIEDPdvMz8/z8Y9/nP7+fgYHB/mVX/kVarXa63gULw+f+cxnuPHGG+nr62NsbIyf+7mfY9++fT3btFot7rjjDkZGRqhWq3z0ox9lenq6Z5tjx45x++23Uy6XGRsb47d+67eI4/j1PJSXjb/8y79k165dWZfL7t27+eY3v5nd/mY//m78yZ/8CUIIfuM3fiO77g1z/PYiw+c//3kbBIH9r//1v9o9e/bYf/pP/6kdHBy009PTF3rXXjG+8Y1v2H/zb/6N/eIXv2gB+6Uvfann9j/5kz+xAwMD9stf/rJ96qmn7M/8zM/YrVu32mazmW3zwQ9+0F5zzTX2oYcest/73vfsjh077Mc+9rHX+UjOH7fddpv93Oc+Z5955hn75JNP2p/6qZ+ymzZtsrVaLdvmV3/1V+3GjRvtvffeax977DF7yy232Le//e3Z7XEc26uuusreeuut9oknnrDf+MY37OjoqP2d3/mdC3FI542vfvWr9s4777T79++3+/bts//6X/9r6/u+feaZZ6y1b/7jT/HII4/YLVu22F27dtlf//Vfz65/oxz/RUeaN910k73jjjuyv7XWdt26dfYzn/nMBdyrVx+rSdMYYycmJuy/+3f/LrtucXHRFgoF+7/+1/+y1lq7d+9eC9hHH3002+ab3/ymFULYkydPvm77/mpgZmbGAvaBBx6w1rpj9X3ffuELX8i2efbZZy1gH3zwQWut+9KRUtqpqalsm7/8y7+0/f39tt1uv74H8CphaGjI/pf/8l/eMse/srJiL7nkEnv33Xfb97znPRlpvpGO/6JanodhyOOPP86tt96aXSel5NZbb+XBBx+8gHv22uPw4cNMTU31HPvAwAA333xzduwPPvggg4OD3HDDDdk2t956K1JKHn744dd9n18JlpaWgI6j1eOPP04URT3Hv3PnTjZt2tRz/FdffTXj4+PZNrfddhvLy8vs2bPnddz7Vw6tNZ///Oep1+vs3r37LXP8d9xxB7fffnvPccIb6/W/qFyOTp8+jda656QAjI+P89xzz12gvXp9MDU1BXDWY09vm5qaYmxsrOd2z/MYHh7OtrkYYIzhN37jN3jHO97BVVddBbhjC4KAwcHBnm1XH//Zzk9628WAp59+mt27d9NqtahWq3zpS1/iiiuu4Mknn3zTH//nP/95fvSjH/Hoo4+ecdsb6fW/qEgzx1sDd9xxB8888wzf//73L/SuvO647LLLePLJJ1laWuJv//Zv+cQnPsEDDzxwoXfrNcfx48f59V//de6++26KxeKF3p0XxEW1PB8dHUUpdUbFbHp6momJiQu0V68P0uN7oWOfmJhgZmam5/Y4jpmfn79ozs+nPvUpvv71r/Od73yHDRs2ZNdPTEwQhiGLi4s9268+/rOdn/S2iwFBELBjxw6uv/56PvOZz3DNNdfw7//9v3/TH//jjz/OzMwM1113HZ7n4XkeDzzwAP/hP/wHPM9jfHz8DXP8FxVpBkHA9ddfz7333ptdZ4zh3nvvZffu3Rdwz157bN26lYmJiZ5jX15e5uGHH86Offfu3SwuLvL4449n29x3330YY7j55ptf930+H1hr+dSnPsWXvvQl7rvvPrZu3dpz+/XXX4/v+z3Hv2/fPo4dO9Zz/E8//XTPF8fdd99Nf38/V1xxxetzIK8yjDG02+03/fG/733v4+mnn+bJJ5/MLjfccAMf//jHs9/fMMf/qpWUXid8/vOft4VCwf7N3/yN3bt3r/1n/+yf2cHBwZ6K2cWKlZUV+8QTT9gnnnjCAvbP/uzP7BNPPGGPHj1qrXWSo8HBQfuVr3zF/vjHP7Y/+7M/e1bJ0dve9jb78MMP2+9///v2kksuuSgkR5/85CftwMCAvf/+++3k5GR2aTQa2Ta/+qu/ajdt2mTvu+8++9hjj9ndu3fb3bt3Z7enkpMPfOAD9sknn7Tf+ta37Jo1ay4ayc1v//Zv2wceeMAePnzY/vjHP7a//du/bYUQ9q677rLWvvmPfzW6q+fWvnGO/6IjTWut/Y//8T/aTZs22SAI7E033WQfeuihC71Lrwq+853vWOCMyyc+8QlrrZMd/dt/+2/t+Pi4LRQK9n3ve5/dt29fz2PMzc3Zj33sY7Zardr+/n77y7/8y3ZlZeUCHM354WzHDdjPfe5z2TbNZtP+83/+z+3Q0JAtl8v253/+5+3k5GTP4xw5csR+6EMfsqVSyY6Ojtp/8S/+hY2i6HU+mpeHf/JP/ondvHmzDYLArlmzxr7vfe/LCNPaN//xr8Zq0nyjHH/up5kjR44c54GLKqeZI0eOHBcaOWnmyJEjx3kgJ80cOXLkOA/kpJkjR44c54GcNHPkyJHjPJCTZo4cOXKcB3LSzJEjR47zQE6aOXLkyHEeyEkzR44cOc4DOWnmyJEjx3kgJ80cOXLkOA/8/wE8V8EYJYWf5gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#RIGHT\n",
    "predict(model, 't2.png')\n",
    "predict(model, 't1.png')\n",
    "predict(model, 't55.png') \n",
    "predict(model, 't22.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0de419a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#predict(model, 't2.png')\n",
    "#predict(model, 't1.png')\n",
    "predict(model, 't5.png') #\n",
    "predict(model, 't55.png')\n",
    "\n",
    "\n",
    "\n",
    "predict(model, 't555.png') #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
